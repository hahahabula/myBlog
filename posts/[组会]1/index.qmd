---
title: "近期汇报"
date: 2025/03/06
categories: [组会, 搜广推]
format:
    revealjs:
        smaller: true
        scrollable: true
        theme: custom
---

# LS-PLM模型

------------------------------------------------------------------------

![LS-PLM模型示意图](images/paste-1.png){width="400"}

LS-PLM的模型采用了分而治之的策略，其有以下优点：

-   可获取非线性关系
-   可扩展性，适应大规模数据
-   稀疏性

可以看作是一个加入了注意力机制的三层神经网络。

-   输入层是特征向量
-   中间层是由m个神经元组成的隐层（m为分片数）
-   最后一层是由一个单一神经元组成的输出层。

::: aside
\[1\] GAI K, ZHU X, LI H, 等. Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction\[A/OL\]. arXiv, 2017\[2025-03-05\]. http://arxiv.org/abs/1704.05194. DOI:10.48550/arXiv.1704.05194.
:::

------------------------------------------------------------------------

## 模型表达式

已知有如下数据集 $\{x_t,y_t\}|_{t=1}^n,y_t\in {0,1},x_t\in\mathbb{R}^d$，则LS-PLM模型的一般形式为：

$$
p(y=1|x) = g\left(\sum_{j=1}^{m} \sigma(u_j^T x) \eta(w_j^T x)\right)
$$

记模型参数为 $\Theta=\{u_1,\ldots,u_m,w_1,\ldots,w_m\}\in R^{d\times 2m}$，当聚类函数选取Softmax函数时，拟合使用LR模型，则有如下模型：

$$
p(y = 1|x) = \sum_{i=1}^{m} \frac{\exp(u_i^T x)}{\sum_{j=1}^{m} \exp(u_j^T x)} \cdot \frac{1}{1 + \exp(-w_i^T x)}
$$ {#eq-soft}

损失函数采用交叉熵损失函数:

$$
\text{loss}(\Theta) = -\sum_{t=1}^{n} \left[ y_t \log \left( p(y_t=1|x_t, \Theta) \right) + (1 - y_t) \log \left( p(y_t=0|x_t, \Theta) \right) \right]
$$

为了模型的泛化能力和参数的稀疏性（便于实际部署），故有如下优化问题：

$$
\arg\min_{\Theta} f(\Theta) = \text{loss}(\Theta) + \lambda \|\Theta\|_{2,1} + \beta \|\Theta\|_1
$$ {#eq-problem}

$L_{2,1}$ 范数定义如下：

$$
\|A\|_{2,1} = \sum_{i=1}^m \sqrt{\sum_{j=1}^n A_{ij}^2}
$$

-   $L_{2,1}$ 范数诱导了行的稀疏性，让不重要的特征前参数为0
-   $L_1$ 范数进一步诱导那些靠近0的参数变为0

------------------------------------------------------------------------

### $L_1,L_{2,1}$ 范数

由下图易得 $L_1,L_{2,1}$ 范数均非光滑

::: panel-tabset
#### $\sqrt{x^2+y^2}$

```{python}
#| echo: false
import numpy as np
import plotly.graph_objects as go

# 定义函数
def f(x, y):
    return np.sqrt(x**2 + y**2)

# 生成数据
x = np.linspace(-10, 10, 100)
y = np.linspace(-10, 10, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# 创建可交互 3D 图
fig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y, colorscale='viridis')])

# 设置轴标签
fig.update_layout(
    scene=dict(
        xaxis_title="X 轴",
        yaxis_title="Y 轴",
        zaxis_title="f(x, y)"
    )
)

fig.show()
```

#### $|x|+|y|$

```{python}
#| echo: false
# 定义函数
def f(x, y):
    return np.abs(x) + np.abs(y)

# 生成数据
x = np.linspace(-10, 10, 100)
y = np.linspace(-10, 10, 100)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

# 创建可交互 3D 图
fig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y, colorscale='viridis')])

# 设置轴标签
fig.update_layout(
    scene=dict(
        xaxis_title="X 轴",
        yaxis_title="Y 轴",
        zaxis_title="f(x, y)"
    )
)

fig.show()
```
:::

------------------------------------------------------------------------

## 目标函数的优化

由于正则项的非光滑和Softmax函数捕捉非线性关系的能力导致目标函数`非光滑且非凸`，因此常用的传统办法如梯度下降算法、EM算法便难以使用了。

这篇论文对于解决 @eq-problem 方法是用使得方向导数最小的方向替代梯度来进行优化，先声明如下公式：

-   右偏导

$$
\partial^+_{ij} f(\Theta) = \lim_{\alpha \to 0^+} \frac{f(\Theta + \alpha e_{ij}) - f(\Theta)}{\alpha} 
$$

-   方向导数

$$
f'(\Theta; d) = \lim_{\alpha \to 0^+} \frac{f(\Theta + \alpha d) - f(\Theta)}{\alpha}
$$ {#eq-fangxiang}

如果方向 $d$ s.t. $f'(\Theta;d)<0$，那么 $d$ 被认为为下降方向。

-   正交投影函数

$$
\pi_{ij}(\Theta; \Omega) = \begin{cases} 
\Theta_{ij} & , \quad \text{sign}(\Theta_{ij}) = \text{sign}(\Omega_{ij}) \\
0 & , \quad \text{otherwise}
\end{cases}
$$

------------------------------------------------------------------------

### 寻找下降方向

::: {#thm-piandao}
### @eq-fangxiang 方向导数处处存在

@eq-problem 的方向导数为：

$$
f'(\Theta;d)=\nabla \text{loss}(\Theta)^Td+\sum_{||\Theta_{i\cdot}||_{2,1}\neq 0}\lambda\frac{\Theta'_{i\cdot}d_{i\cdot}}{||\Theta_{i\cdot}||_{2,1}}+\sum_{||\Theta_{i\cdot}||_{2,1}= 0}\lambda||d_{i\cdot}||_{2,1}+\sum_{||\Theta_{ij}||\neq 0}\beta\text{sign}(\Theta_{ij})d_{ij}+\sum_{||\Theta_{ij}||= 0}\beta|d_{ij}|
$$
:::

![方向导数的证明](images/paste-1.jpeg){fig-align="center" width="550"}

------------------------------------------------------------------------

当得到方向导数后便可寻找d s.t. $f'(\Theta,d)$ 最小作为下降方向，也即是有如下优化问题：（约束条件是为了限制d的长度）

$$
\min_{d}f'(\Theta;d)\qquad \text{s.t.} ||d||^2\leq C
$$

上式是一个不等式约束问题，为了求解可利用拉格朗日函数转换为它的对偶问题：

$$
\begin{aligned}
&\max_{\mu}\min_{d}L(d,\mu)\\
\text{s.t.}& \mu \ge 0\\
&L(d,\mu)=f'(\Theta;d)+\mu(||d||^2-C)
\end{aligned}
$$

易得拉格朗日函数 $L(d,\mu)$ 是 $d$ 的凸函数，因此原始问题的解与对偶问题的解是等价的。

令拉格朗日函数的偏导为0，则有：

$$
s=-\nabla \text{loss}(\Theta)_{ij}-\lambda \frac{\Theta_{ij}}{||\Theta_{i\cdot}||_{2,1}}
$$

-   当 $\Theta_{ij}\neq 0$

$$
2\mu d_{ij}=s-\beta\text{sign}(\Theta_{ij})
$$

------------------------------------------------------------------------

-   当 $\Theta_{ij}=0$ 且 $||\Theta_{i\cdot}||_{2,1}\neq 0$

$$
2\mu d_{ij}=\max\{|s| - \beta, 0\} \text{sign}(s)
$$

![第二种情况下$d_{ij}$的计算](images/paste-2.jpeg){width="750"}

------------------------------------------------------------------------

-   当 $||\Theta_{i\cdot}||_{2,1}= 0$

$$
2\mu d_{ij}=\frac{\max\{\|v\|_{2,1} - \lambda, 0\}}{\|v\|_{2,1}} v
$$

$$
v = \max\{|-\nabla loss(\Theta)_{ij}| - \beta, 0\} \text{sign}(-\nabla loss(\Theta)_{ij})
$$

![$d_{ij}$的第3种情况证明](images/paste-3.jpeg){width="700"}

------------------------------------------------------------------------

由于我们在意的只是下降方向不在意 $d_{ij}$ 的大小，而 $\mu$ 可看作是一个长度的正缩放因子并不影响方向，故不再需要将 $\mu$ 求解出来，直接舍去 $2\mu$ 即可。

综上便找到了 $d_{ij}$ 使得方向导数 $f'(\Theta;d)$ 最小：

$$
d_{ij} = 
\begin{cases} 
s - \beta \text{sign}(\Theta_{ij}), & \Theta_{ij} \neq 0 \\
\max\{|s| - \beta, 0\} \text{sign}(s), & \Theta_{ij} = 0, \|\Theta_{i,\cdot}\|_{2,1} \neq 0 \\
\frac{\max\{\|v\|_{2,1} - \lambda, 0\}}{\|v\|_{2,1}} v, & \|\Theta_{i,\cdot}\|_{2,1} = 0
\end{cases}
$$

该论文在此基础上融合 `LBFGS`(一种拟牛顿算法) 方法最终确定下降方向 $p_k$。

#### LBFGS

拟牛顿法如BFGS法更新近似海瑟阵的步骤如下：

![BFGS算法步骤](images/paste-2.png){fig-align="center" width="540"}

------------------------------------------------------------------------

由BFGS公式可得，$H=B^{-1}$ 的更新公式如下

$$
\begin{aligned}
B_{k+1}^{-1} = &(I - \frac{s_k y_k^T}{y_k^T s_k}) B_k^{-1} (I - \frac{y_k s_k^T}{y_k^T s_k}) + \frac{s_k s_k^T}{y_k^T s_k}\\
=&V_kH_kV_k^T+\rho_ks_ks_k^T\\
\rho_k=&\frac{1}{y_k^Ts_k},V_k=(I-\rho_ks_ky_k^T),H_k=B_k^{-1}
\end{aligned}
$$

LBFGS则为了节省内存不存储上一次迭代产生的 $H_k$，而仅存储从 $k$ 到 $k-m+1$ 步的 $s_i$ 和 $y_i\quad (i=k,\ldots,k-m+1)$，从 $H_{k-m+1}=H^{(0)}$ 结合历史信息 $s_i$ 和 $y_i$ 往 $H_{k+1}$ 进行递推，递推公式如下：

$$
\begin{aligned}
H_{k+1} = &
(V_k V_{k-1} \cdots V_{k-m+1}) H_{k-m+1} (V_k V_{k-1} \cdots V_{k-m+1})^T \\
&+ (V_k V_{k-1} \cdots V_{k-m+2}) \rho_{k-m+1} s_{k-m+1} (V_k V_{k-1} \cdots V_{k-m+2})^T \\
&+ \cdots \\
&+ (V_k) \rho_{k-1} s_{k-1} s_{k-1}^T (V_k)^T \\
&+ \rho_k s_k s_k^T
\end{aligned}
$$ {#eq-remianH}

此时直接令 $H_{k-m+1}=H^{(0)}=\frac{s_k^Ty_k}{y_k^Ty_k}I$，舍去 $k-m-1$ 之前的所有历史信息。通过 @eq-remianH 便可得到目前的 $H_{k+1}$。

------------------------------------------------------------------------

::::: columns
::: column
![LBFGS的双重循环](images/paste-4.jpeg){fig-align="left" width="400"}
:::

::: column
左侧神奇的双重循环算法便可实现 @eq-remianH 中 $H_{k+1}$ 的计算，其中的r便是 $H_{k+1}g_{k+1}$。

![验证双重循环算法的正确性](images/paste-5.jpeg){fig-align="left" width="500"}
:::
:::::

------------------------------------------------------------------------

下降方向将主要由 $d_{ij}$ 决定，辅以 $H_kd^{(k)}$ 确定：

![下降方向的决定方式](images/paste-3.png){width="800"}

上述确定下降方向的方法是当 $H_k$ 正定时且由LBFGS方法确定的下降方向和 $d_{ij}$ 方向一致时，采用LBGFS（个人感觉是因为LBFGS是要求目标函数的梯度是存在的，而论文中梯度是由 $d_{ij}$ 进行替代，当两者相悖时优先 $d_{ij}$）；当 $H_k$ 非正定时则直接采用 $d_{ij}$ 确定方向。

步长则由回溯线搜索获取合适的步长 $\alpha$ ，比如采用 `Armijo条件`，其原理如下：

$$
f(x+td)\leq f(x) + ct\nabla f^T(x)p
$$

上式中 $\nabla^T f(x)$ 由 $d^T$ 替代，$c$ 为小常数，步长由大到小缩小，若下降条件不足，则按比例缩小步长 $t\to\beta t$，$\beta$ 常取0.5。

![回溯线搜索示意图](images/paste-6.jpeg){width="200"}

------------------------------------------------------------------------

### 参数更新

$$
\Theta^{(k+1)} = \pi(\Theta^{(k)} + \alpha p_k; \xi^{(k)})
$$

该参数的更新个人感觉像是避免振荡，当参数要从正变为负时应当首先变为0再变负。

$$
\xi^{(k)}_{ij} = 
\begin{cases} 
\text{sign}(\Theta^{(k)}_{ij}), & \Theta^{(k)}_{ij} 
\neq 0 \\
\text{sign}(d^{(k)}_{ij}), & \Theta^{(k)}_{ij} = 0 
\end{cases}
$$

### 算法综合

综合以上算法得到 @eq-problem 的优化算法如下：

![@eq-problem 的优化算法](images/paste-4.png){fig-align="left" width="550"}

## 工业实现

![](images/paste-5.png){fig-align="center" width="400"}

**1）图A)物理拓扑结构的优点**

1.  上述图A)中的物理拓扑结构可以最大化CPU计算能力的利用率：传统的一个计算节点只会由服务节点或者工作节点，不会同时存在服务节点和工作节点，当服务节点不进行计算密集型任务时就会动态分配CPU资源给工作节点，而不像传统计算节点那样闲置CPU资源。
2.  最大化内存利用率：和上面类似，当两种节点同属在一个计算节点中便可共享内存，提高内存利用率。

**2）图B)数据并行和模型并行的方法**

1.  将所有的数据集分成n份，每个工作节点仅存储一份。
2.  服务器节点存储模型的参数，但各服务器存储的参数互斥即一个参数只能存储在一个服务节点中。
3.  计算步骤：
    -   首先各工作节点通过自己的部分数据集训练出一个局部模型，当需要模型目前参数值时便与服务节点通信，此时工作节点会计算各自的损失和下降方向 $p$，而后各工作节点会将这些信息Push给服务节点。
    -   服务节点经过汇总上述信息更新参数，并将更新后的参数值广播到各工作节点。

```{mermaid}
graph LR
A([工作节点局部计算]) --> B[服务器节点下降方向、损失聚合]
B --> C([服务器节点参数更新])
C --> D(服务器节点向工作节点进行参数同步)
```

------------------------------------------------------------------------

## 共同特征技巧

![](images/paste-6.png){width="500"}

@eq-soft 中计算最复杂的部分是 $\mu_i^Tx$ 和 $w_i^Tx$，而共同技巧则将特征向量进行拆分为共同特征和非共同特征（类似`因子分析`中的将特征分为公共因子和特殊因子）：

$$
\begin{aligned}
μ _ {i} x =& μ _ {i,c} x _ {c} + μ _ {i,nc} x _ {nc}\\
w _ {i} x =& w _ {i,c} x _ {c} + w _ {i,nc} x _ {nc}\\
\end{aligned}
$$

则在工业实现中运用共同特征技巧需要满足如下方面：

-   确保拥有相同公共特征的样本处于同一个工作节点内
-   公共特征仅存储一次
-   对相同特征前的系数仅更新一次方向

## 效果

::: panel-tabset
### LS-PLM 模型在不同分片数中的效果

![](images/paste-8.png){width="700"}

### 正则项的效果

![](images/paste-9.png)

### 共同特征技巧的效果

![](images/paste-7.png)
:::

# 数据结构和算法

这一部分主要是刷力扣题学习到的一些编码方式或算法

------------------------------------------------------------------------

## 哈希表

主要用于查找，在理想情况下，查找的时间复杂度是 $O(1)$ 的。

### 哈希函数的实现方法

在理想状态下，哈希函数会将数据均匀散列分布在哈希表中。主要有直接定址法、除留余数法等。

::: panel-tabset
### 直接定址法

当数据的范围比较集中是，优先考虑使用定址法。其主要实现方法类似一种键值对，键为数据的值，值为数据出现的次数。

该方法的缺陷是当数据比较分散且范围较大时，就造成空间浪费。

![](images/paste-11.png){fig-align="center" width="400"}

### 除留余数法

其核心思想是将数据对某个数（一般是表长）进行求余运算，将运算结果作为索引值，即表示该数据存放的下标位置。假设某哈希表的大小为M，则哈希函数为 $\text{hash}=x \% M$

此时应当注意：

-   为了尽量减少哈希冲突出现的次数，建议M取不太接近2的整数次幂的一个素数。
-   除留余数法中，为了方便数据转化为索引值，要求哈希表的数据元素支持转换为整数。

![](images/paste-13.png){fig-align="center" width="400"}
:::

------------------------------------------------------------------------

### 哈希冲突

当一个数据通过哈希函数产生了与另一个数据相同的哈希值，那么也就意味着它们需要存储在同一个位置，显然无法满足。此时的情况就叫做哈希冲突（哈希碰撞）。我们可以通过以下两种方法进行解决：

::: panel-tabset
### 开放定址法（闭散列）

开放定址法是按照某种策略将冲突的数据放置在表中的另一个空位当中。常见的策略有三种：线性探测、二次探测和双重探测。

-   线性探测：线性探测指的是从冲突位置开始，依次向后一个一个查找，直到找到空位，再将冲突元素放置在当前空位。 当查找到表尾还没有找到空位，那么从表头位置开始重新查找。（容易造成表中的冲突数据聚集，且会占用其他元素的原本位置）
-   二次探测：二次探测对线性探测进行了优化。它的查找方式是依次左右按照二次方跳跃式探测。例如先探测冲突位置的右边 $1^{2}$ 个位置，然后左边 $1^{2}$ 个位置，右边 $2^{2}$ 个位置，左边 $2^{2}$ 个位置......依次类推。（二次探测有效地减少了数据聚集问题）

### 链接址法（开散列）

其改变了传统的哈希表元素存储策略，在链地址法当中，所有数据不再直接存储在哈希表中，而是将哈希表的每一个元素设置为指针，用以维护一个链表。此时哈希表中的每一个元素称作`哈希桶`，其中存储的元素互相之间就是哈希冲突的。

链接地址在发生冲突时，就不会占用其他元素的原本位置，相比开放地址法效率较高。

![](images/paste-14.png){fig-align="center" width="459"}
:::

### 装填因子

记 $\alpha=$ 表中元素的个数/散列表的长度，当装填因子过大时，哈希表的空间利用率就越高，发生哈希冲突的概率就越高，哈希表的总体性能就越低。在实际运用过程中，我们将装填因子作为哈希表是否需要扩容的标准。

-   采用开发定址法处理冲突时，该阈值通常位于0.7-0.8之间；
-   使用链地址法时，阈值则通常为1。

------------------------------------------------------------------------

## 康托编码

康托展开式全排列与自然数的双射，常用于空间压缩，本质上是计算当前排列在所有由小到大全排列中的顺序，因此可逆。

### 排列 $\to$ 自然数

求`34152`在全排列中的位置

1.  第一位小于3的所有数有：$2\times 4!$
2.  第2位小于4的所有数：$2\times 3!$(3由于被第一位占了所以是 $2\times$)
3.  第三位小于1的所有数：无
4.  第4位小于5的所有数：$1\times 1!$
5.  第5位小于2的所有数：$0$

则其编号为 $2 \times 4! + 2 \times 3! + 0 \times 2! + 1 \times 1! + 0 \times 0! = 61$

### 自然数 $\to$ 排列

编号X=61

1.  第1位：$\frac{61}{4} = 2 \cdots \cdots 13$，说明比第1位小的有2个，ans\[0\]=3

2.  第2位：$\frac{13}{3} = 2 \cdots \cdots 1$，说明比第2位小的有2个，a\[1\]=4

3.  第3位：$\frac{1}{2} = 0 \cdots \cdots 1$，说明比第3位小的有0个，ans\[2\]=1

4.  第4位：$\frac{1}{1} = 1 \cdots \cdots 0$，说明比第4位小的有1个，ans\[3\]=5

5.  第5位：剩下2，ans\[4\]=2

# 接下来要干的

------------------------------------------------------------------------

**1）论文**

-   [ ] 统计计算课给的有关生物领域的两篇论文[^1] [^2]
-   [ ] 看推荐系统模型原文

[^1]: YOU R, QU W, MAMITSUKA H, 等. DeepMHCII: a novel binding core-aware deep interaction model for accurate MHC-II peptide binding affinity prediction\[J/OL\]. Bioinformatics, 2022, 38(Supplement_1): i220-i228. DOI:10.1093/bioinformatics/btac225.

[^2]: JENSEN K K, ANDREATTA M, MARCATILI P, 等. Improved methods for predicting peptide binding affinity to MHC class II molecules\[J/OL\]. Immunology, 2018, 154(3): 394-406. DOI:10.1111/imm.12889.

**2）比赛**

-   [ ] 统计建模大赛

**3）实习准备**

-   [ ] 刷力扣的题
-   [ ] 《深度学习推荐系统》
-   [ ] 《神经网络和深度学习》
-   [ ] 一些网课啥的

**4）小项目**

-   [ ] Kaggle上的一个降雨预测的题（感觉可以用LS-PLM模型挺合适的）[^3]
- [ ] 为实习简历准备的项目（还在思考）

[^3]: https://www.kaggle.com/competitions/playground-series-s5e3