---
title: "S2.7 LS-PLM模型"
date: 2025/02/20
description: "介绍曾经阿里巴巴的主流推荐模型——LS-PLM模型"
categories: [搜广推, 《深度学习推荐系统》]
image: "./images/logo.png"
---

LS-PLM模型是“大规模分段线性模型”(Large Scale Piece-wise Linear Model)，其结构与三层神经网络极度类似。

# LS-PLM模型的主要结构

LS-PLM可看作对逻辑回归的自然推广，其思路是在做逻辑回归前先进行聚类。其数学公式如下：

$$
f(x)=\sum_{i=1}^m\pi_i(x)\cdot\theta_i(x)=\sum_{i=1}^m\frac{e_{\mu_ix}}{\sum_{j=1}^me^{\mu_j\cdot x}}\cdot\frac{1}{1+e^{-w_ix}}
$$

上式中假设聚类函数 $\pi$ 为softmax函数，而后用LR模型计算样本在分片中具体的CTR，然后将二者相乘后求和。分片数 $m$ ：可以较好第平衡模型的拟合和推广能力，m越大，模型拟合能力越强，但模型复杂度也随之上升，m的经验值为12。该模型的优点如下：

-   端对端的非线性学习能力
-   模型的稀疏性强（建模时引入L1和L2，1范数）

# L1和L2范数的区别

1.  L1范数指在损失函数中引入 $\lambda||w||_1$ 惩罚项
2.  L2范数指在损失函数中引入 $\lambda||w||^2_2$ 惩罚项

故L1-loss为:

$$
L_1(w)=L(w)+\lambda ||w||_1
$$

L2-loss为：

$$
L_2(w)=L(w)+\lambda ||w||^2_2
$$

假定真实模型为 $y=5x$，利用python产生如下模拟数据：

```{python}
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt
%config InlineBackend.figure_format = 'svg'

np.random.seed(42)
x = np.linspace(0, 10, 100)
y = [2*i + np.random.normal(0, 9) for i in x]
fig, ax = plt.subplots(figsize=(8,6))
ax.scatter(x, y, label="生成的虚拟数据")
ax.plot(x, 2*x, label="真实模型", c="r")
plt.legend()
plt.show()
```

以下动画展示了L1，L2范数下的原损失函数，正则化函数和L1，L2损失函数之间的关系。

```{python}
#| echo: false
import plotly.express as px
import plotly.graph_objects as go


def Loss(w: float) -> float:
    return np.sum((y - w*x)**2) / len(y)

def L1(w: float, lam: float) -> float:
    return lam * abs(w)

def L2(w: float, lam: float) -> float:
    return lam * w**2

# 创建数据
w = np.linspace(-5, 5, 1000)
lam = np.linspace(0, 198, 100)

y_Loss = np.array([Loss(i) for i in w])
y_Loss = np.tile(y_Loss, (100, 1))
y_L1 = np.array([[L1(j, i) for j in w] for i in lam])
y_L2 = np.array([[L2(j, i) for j in w] for i in lam])
y_L1Loss = y_Loss + y_L1
y_L2Loss = y_Loss + y_L2

# 找到每个 lambda 对应的最小值
min_values1 = [np.min(y_L1Loss[i, :]) for i in range(len(lam))]
min_w_values1 = [w[np.argmin(y_L1Loss[i, :])] for i in range(len(lam))]
min_values2 = [np.min(y_L2Loss[i, :]) for i in range(len(lam))]
min_w_values2 = [w[np.argmin(y_L2Loss[i, :])] for i in range(len(lam))]

# 创建动画帧
frames = [go.Frame(data=[go.Scatter(x=w, y=y_L1[i,:], mode='lines', name='L1正则化函数'),
                         go.Scatter(x=w, y=y_L1Loss[i,:], mode='lines', name='L1损失函数'),
                         go.Scatter(x=w, y=y_L2[i,:], mode='lines', name='L2正则化函数'),
                         go.Scatter(x=w, y=y_L2Loss[i,:], mode='lines', name='L2损失函数'),
                         go.Scatter(x=w, y=y_Loss[i,:], mode='lines', name='原损失函数'),
                         go.Scatter(x=[min_w_values1[i]], y=[min_values1[i]], mode='markers', marker=dict(color='red', size=10), name='L1损失函数最小值'),
                         go.Scatter(x=[min_w_values2[i]], y=[min_values2[i]], mode='markers', marker=dict(color='green', size=10), name='L2损失函数最小值')],
                   name=f'lambda={lam[i]:.2f}') for i in range(len(lam))]

# 创建初始图形
fig = go.Figure(
    data=[go.Scatter(x=w, y=y_L1[0,:], mode='lines', name='L1正则化函数'),
          go.Scatter(x=w, y=y_L1Loss[0,:], mode='lines', name='L1损失函数'),
          go.Scatter(x=w, y=y_L2[0,:], mode='lines', name='L2正则化函数'),
          go.Scatter(x=w, y=y_L2Loss[0,:], mode='lines', name='L2损失函数'),
          go.Scatter(x=w, y=y_Loss[0,:], mode='lines', name='原损失函数'),
          go.Scatter(x=[min_w_values1[0]], y=[min_values1[0]], mode='markers', marker=dict(color='red', size=10), name='L1损失函数最小值'),
          go.Scatter(x=[min_w_values2[0]], y=[min_values2[0]], mode='markers', marker=dict(color='green', size=10), name='L2损失函数最小值')],
    layout=go.Layout(
        xaxis=dict(range=[-6, 6], autorange=False),
        yaxis=dict(range=[0, 500], autorange=False),
        title='L1,L2正则化损失函数的动态变化',
        updatemenus=[dict(
            type="buttons",
            buttons=[dict(label="开始",
                          method="animate",
                          args=[None, {"frame": {"duration": 0, "redraw": True}, "fromcurrent": True, "mode": "immediate"}]),
                          dict(label="暂停", method="animate", args=[[None], dict(frame=dict(duration=0, redraw=False), mode='immediate', transition=dict(duration=0))])]
        )],
        sliders=[dict(
        steps=[dict(method="animate", args=[[f.name], dict(mode="immediate", frame=dict(duration=0, redraw=True), transition=dict(duration=0))], label=f"λ={lam[i]:.2f}") for i, f in enumerate(frames)],
        currentvalue={"prefix": "正则化参数 λ: ", "visible": True, "xanchor": "center"},
        pad={"b": 10},
        len=0.9,
        x=0.1,
        y=0.01)]
    ),
    frames=frames
)

# 显示图形
fig.show()
```

以上动画揭露了以下信息：

1. 当 $\lambda$ 大于130时L1损失函数的最小值就一直处在y轴上，而L2损失函数的最小值却没有。（L1正则化系数稀疏的表现）
2. L1/L2损失函数的最小值并不在正则化函数与原损失函数的交点或切点处( $\lambda=32,12$)。

从贝叶斯统计的角度上讨论，正则化项可看作贝叶斯推断中的先验分布，而L1正则化对应的是拉普拉斯分布，L2对应的是正态分布，它们的密度函数如下：

$$
\text{Laplace distribution: } f(x|\mu, b) = \frac{1}{2b} \exp\left(-\frac{|x - \mu|}{b}\right)
$$

$$
\text{Normal distribution: } f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) 
$$

它们的密度函数图像如下图：
```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import laplace, norm

x = np.linspace(-10, 10, 1000)
laplace_pdf = laplace.pdf(x, loc=0, scale=1)
normal_pdf = norm.pdf(x, loc=0, scale=1)

fig, ax = plt.subplots(figsize=(8, 6))
ax.plot(x, laplace_pdf, label="拉普拉斯分布", color='blue')
ax.plot(x, normal_pdf, label="正态分布", color='red')
ax.legend()
plt.title("拉普拉斯分布和正态分布对比(位置参数：0，尺度参数：1)")
plt.xlabel("x")
plt.ylabel("密度")
plt.show()
```

由图可得，拉普拉斯分布在 $x=0$ 处尖峰，相较于正态分布，拉普拉斯分布的随机变量更可能为0，因此，L1正则化后的参数更加稀疏。

# LS-PLM模型与深度学习的联系

LS-PLM模型可以看作一个加入了注意力机制的三层神经网络模型，其中输入层是样本的特征向量，中间层是由m个神经元组成的隐层，其中m是分片的个数，对于一个CTR预估问题，LS-PLM的最后一层自然是由单一神经元组成的输出层。

::: {.callout-note}
## 注意力机制

在隐层和输出层之间存在的，神经元之间的权重是由分片函数得出的注意力得分来确定的，即是样本属于哪个分片的概率就是其注意力得分。
:::


