---
title: "Logistic回归"
date: 2025/02/23
categories: [机器学习]
description: "介绍机器学习中有关Logistic回归与最大熵模型"
---

Logistic回归是统计学习中的经典分类方法，而最大熵是概率模型学习的一个准则，将其推广到分类问题中便得到了最大熵模型(MEM, maximum entropy model)。

# Logistic回归模型

Logistic回归模型是一种对数线性模型。

## Logistic分布

Logistic分布的定义如下：

1.  分布 $$
    F(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}
    $$

2.  密度 $$
    f(x)=F'(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(e^{-(x-\mu)/\gamma})^2}
    $$

式中，$\mu$ 为位置参数，$\gamma>0$ 为形状参数。

```{python}
#| echo: false
import numpy as np
def f(x: float, gam: float, mu: float) -> float:
    return np.exp(-(x-mu)/gam)/(1+np.exp(-(x-mu)/gam))**2/gam

def F(x: float, gam: float, mu: float) -> float:
    return 1 / (1 + np.exp(-(x - mu) / gam))

x = np.linspace(-10, 10, 1000)
f_value = [f(i, 1, 0) for i in x]
F_value = [F(i, 1, 0) for i in x]

import matplotlib.pyplot as plt
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

ax1.plot(x, f_value)
ax1.set_title('Logistic密度函数')
ax2.plot(x, F_value)
ax2.set_title('Logistic分布函数')
plt.show()
```

## 二项逻辑回归模型

二项逻辑回归模型是一种分类模型，由 $P(Y|X)$ 表示，其形式如下式：

$$
\begin{aligned}
P(Y=1|x)=\frac{\exp(w\cdot x)}{1+\exp(w\cdot x)}\\
P(Y=0|x)=\frac{1}{1+\exp(w\cdot x)}
\end{aligned}
$$

::: callout-note
## 事件的几率(odd)

事件的几率是指事件发生的概率于该事件不发生概率的比值，对数几率（Logit函数）是指 $\log(\frac{p}{1-p})$
:::

则输出 $Y=1$ 的对数几率为:

$$
\log\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\cdot x
$$

为输入x的线性函数，即输出 $Y=1$ 的对数几率是由输入 $x$ 的线性函数表示的模型，这便是Logistic回归模型。

### 模型参数估计

Logistic回归模型可采用极大似然估计模型参数，假设有 $P(Y=1|x)=\pi(x),P(Y=0|x)=1-\pi(x)$

![Logistic回归模型参数的极大似然估计](images/paste-1.jpeg){width="381"}

对于求解一对数似然函数为目标函数的最优化问题，Logistic回归学习常用的方法是梯度下降法和拟牛顿法。

### 多项Logistic回归

假设离散型随机变量 $Y$ 的取值集合是 $\{1, 2, \ldots, K\}$，那么多项Logistic回归模型是：

$$
\begin{aligned}
P(Y=k|x)&=\frac{\exp(w_k\cdot x)}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}&k=1,2,\ldots, K-1\\
P(Y=K|x)&=\frac{1}{1+\sum_{k=1}^{K-1}\exp(w_k\cdot x)}
\end{aligned}
$$

# 最大熵模型

最大熵模型(maximum entropy model)由最大熵原理推导实现。

## 最大熵原理

在满足约束条件的模型集合中选取熵最大的模型，离散随机变量 $X$ 的熵定义如下：
$$
H(X)=-\sum_{x}P(x)\log P(x)
$$

熵满足等式 $0\leq H(X)\leq \log|X|$，当且仅当 $X$ 的分布服从均匀分布时，熵最大。

最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是“等可能的”，最大熵原理通过熵的最大化来表示等可能性。最大熵原理给出了一个选择最优模型的准则。

## 最大熵模型定义

将最大熵原理运用到分类时便得到最大熵模型。给定一个训练数据集 $T=\{(x_1,y_1),\ldots,(x_N,y_N)\}$，假定联合分布 $P(X,Y)$、边缘分布 $P(X)$ 的经验分布为：
$$
\begin{aligned}
&\tilde{P}(X=x,Y=y)=\frac{v(X=x,Y=y)}{N}\\
&\tilde{P}(X=x)=\frac{v(X=x)}{N}
\end{aligned}
$$

上式中的 $v(\cdot)$ 为事件发生的频率，记特征函数为 $f(x,y)=I(x与y满足一个事实)$