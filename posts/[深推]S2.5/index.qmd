---
title: "S2.5从FM到FMM-自动特征交叉的解决方案"
description: "介绍深度学习推荐系统有关自动特征解决的方案"
author: "Hahabula"
date: 2025-01-25
date-modified: 2025-01-26
categories:
    - 搜广推
    - 《深度学习推荐系统》
---

# 辛普森悖论

在对样本集合进行分组研究时，在分组比较中都占优势的一方，在总评中有时反而是失势的一方——辛普森悖论，有如下的例子：

::: {layout-ncol="2"}
| 视频 | 点击 | 曝光 | 点击率 |
|------|------|------|--------|
| A    | 8    | 530  | 1.51%  |
| B    | 51   | 1520 | 3.36%  |

: 男性用户

| 视频 | 点击 | 曝光 | 点击率 |
|------|------|------|--------|
| A    | 201  | 2510 | 8.01%  |
| B    | 92   | 1010 | 9.11%  |

: 女性用户
:::

| 视频 | 点击 | 总曝光 | 点击率 |
|------|------|--------|--------|
| A    | 209  | 3040   | 6.88%  |
| B    | 143  | 2530   | 5.65%  |

: 数据汇总（忽略性别这个维度）

奇怪的现象：A的点击率在男女中都少于B，但汇总后A的点击率却高于B 原因：分组实验是使用“性别”+“视频id”的组合特征计算点击率，而汇总实验则是实验“视频id”这一单一特征计算点击率，<font color="#ff0000">汇总使用对高维特征进行了合并，损失了大量有效的信息而无法正确刻画数据模式</font>。 逻辑回归模型只对单一特征做简单加权，不具备进行特征交叉生成高维组合特征的能力，<font color="#ff0000">因此表达能力弱，可能得出像“辛普森悖论”类似的错误结论</font>。

------------------------------------------------------------------------

# POLY2模型-特征交叉的开始

POLY2的数学形式

$$
POLY2(w,x)=\sum_{j_1=1}^{n-1}\sum_{j_2=j_1+1}^nw_{h(j_1,j_2)}x_{j_1}x_{j_2}
$$

-   该模型对所有特征进行两两交叉($x_{j_1}$和$x_{j_2}$)并对所有的特征组合赋予权重$w_{h(j_1,j_2)}$，通过暴力组合特征的方式在一定程度上解决了特征组合的问题
-   实际意义：综合地将两个特征一同考虑
-   缺陷：
    1)  在处理互联网数据时，经常采用one-hot编码的方法处理类别型数据，致使特征向量极度稀疏，POLY2进行无选择的特征交叉-原本就非常稀疏的特征向量更加稀疏，导致大部分交叉特征的权重缺乏有效的数据进行训练，<font color="#ff0000">无法收敛</font>。
    2)  权重参数的数量由$n$直接上升到$n^2$，极大地增加了训练复杂度。

one-hot编码

:   将类别特征转换为向量的一种编码方式：

![独热编码示意图](img/one-hot.png){width="350"}

::: callout-note
## 独热编码

有多少种取值向量就有几个维度，这是造成互联网模型的输入特征向量稀疏的主要原因。
:::

POLY 2 模型本质上是逻辑回归模型的修正, 其完整数学形式如下：

$$
f_{POLY2}(w,x)=\frac{1}{1+e^{-(W^Tx+POLY2(w,x))}}
$$

------------------------------------------------------------------------

# FM模型-隐向量特征交叉

## 数学表达式

$$
FM(w,x)=\sum_{j_1=1}^{n-1}\sum_{j_2=j_1+1}^n(w_{j_1},w_{j_2})x_{j_1}x_{j_2},(\cdot,\cdot)\text{表示内积}
$$

-   FM 模型为每个模型学习了一个隐权重向量（latent vector）
-   FM 模型引入隐向量的做法与矩阵分解隐向量的思想类似，但 FM 模型将单纯的用户、物品隐向量拓展到了所有特征上。
-   FM 节省了训练开销，将权重参数的复杂度从 $n^2$ 级别下降到了 $nk$, $k$ 为隐向量维度 $n>>k$

## 优点

举例来说，在某商品推荐的场景下，样本有两个特征，分别是频道（channel）和品牌（brand），某训练样本的特征组合是（ESPN，Adidas）。

-   [疏解模型对数据稀疏性的要求]{style="color: #98FF98"} 在 POLY 2 中，只有当 ESPN 和 Adidas 同时出现在一个训练样本中时，模型才能学到这个组合特征对应的权重；而在 FM 中，ESPN 的隐向量也可以通过（ESPN，Gucci）样本进行更新，Adidas 的隐向量也可以通过（NBC，Adidas）样本进行更新，这大幅降低了模型对数据稀疏性的要求。
-   [提高模型的泛化能力]{style="color: #98FF98"} 甚至对于一个从未出现过的特征组合（NBC，Gucci），由于模型之前已经分别学习过 NBC 和 Gucci 的隐向量，具备了计算该特征组合权重的能力，这是 POLY 2 无法实现的。相比 POLY 2，FM 虽然丢失了某些具体特征组合的精确记忆能力，但是泛化能力大大提高。
-   相比于以后的深度学习，FM 模型更易进行线上部署和服务

------------------------------------------------------------------------

# FFM模型-引入特征域的概念

引入特征域感知(field-aware)

## FFM数学表示

$$
y=W^TX+\sum_{j_1=1}^{n-1}\sum_{j_2=j_1+1}^n(w_{j_1,f_2},w_{j_2,f_1})x_{j_1}x_{j_2}=W^TX+FFM(W_1,X)
$$

## FFM隐变量的讨论

当获得如下一个训练样本：

| Publisher(P) | Advertiser(A) | Gender(G) |
|:------------:|:-------------:|:---------:|
|     EPSN     |     NIKE      |   Male    |

: 样本 {#tbl-sample}

在 FM 中特征 ESPN、NIKE 和 Male 都有对应的隐向量 $w_{ESPN},w_{NIKE},w_{Male}$ 那么ESPN特征与NIKE特征、ESPN特征与Male特征做交叉的权重应该是 $(w_{ESPN}, w_{NIKE}), (w_{ESPN},w_{Male})$ 。 在 FFM 中，ESPN 与 NIKE、ESPN 与 Male 交叉特殊的权重分别是 $(w_{ESPN,A}, w_{NIKE,P})$ 和 $(w_{ESPN,G}, w_{Male,P})$ $f$ 表示特征域的个数，也即是分类型数据的个数。

## FFM模型的评价

-   FFM 模型训练中，需要学习 n 个特征在 $f$ 个域上的 k 维隐向量，参数个数共 $n\times k\times (f-1)$ 个，FFM 的复杂度为 $kn^2$
-   FFM 引入了特征域，为模型引入了更多有价值的信息，表达能力更强，但计算复杂度更高。

------------------------------------------------------------------------

# POLY2到FFM模型的演化过程

在公式中认为特征数为 $n$, 特征域的个数为 $f$，隐向量维数取 $k=2$，同时仅讨论上述样本 @tbl-sample：

::: callout-tip
## 为啥不是表示为变量和变量之间交叉？

因为在独热编码的情况下分类型变量成为一个向量，其拥有特征个数的维数，因此其系数也同时具有特征个数的维数，故变量的每个取值前都会具有一个系数。
:::

![POLY2模型(对于其而言将有$\frac {n(n-1)} 2=3$个参数，参数的数量级为$O(n^2)$)](img/POLY2模型.png){width="287"}

![FM模型(对于其而言有$nk=6$个参数，参数数量级为$O(nk)$)](img/FM模型.png){width="313" height="227"}

::: callout-tip
## 此时为啥维度都为2？

之所以为2是因为隐向量的维度$k$在之前设置为了2，设置为其他数也可。
:::

![FFM模型(对于其而言有$nk(f-1)=12$个参数)](img/FFM模型系数示意图.png){width="412"}

> FM可推广到三特征交叉，或者更高维度，但由于组合爆炸问题的限制，其难以在实际工程中实现。