{
  "hash": "e90229cd87b273dff450de1f060ab9dc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"GBDT模型\"\ndescription: \"介绍GBDT模型\"\ndate: 2025-02-07\ndate-modified: 2025-02-07\ncategories: [机器学习, 算法]\n---\n\n\nGBDT (Gradient Boosting Decision Tree), 梯度提升树。它是一种基于决策树的集成算法。\n\n-   基本结构：决策树组成的树林\n\n-   学习方式：梯度提升\n\n通过构造一组弱的学习（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效结合。\n\n# GBDT详解\n\n接下来将详细介绍GBDT模型。\n\n## GBDT的原理\n\n-   所有弱分类器的结构相加等于预测值\n-   每次都以当前预测值为基准，下一个弱分类器去拟合误差函数对预测值的误差\n-   GBDT 的弱分类器使用的是树模型\n\n## 前向分布算法 {#sec-前向分布算法}\n\n许多加法模型都有如下形式\n\n$$\nf(x)=\\sum_{i=1}^M\\beta_m b(x,\\gamma_m)\n$$\n\n$\\beta_m$ 是系数, $b(x,\\gamma_m)$ 是基函数并带有一组参数 $\\gamma_m$。 这类模型的参数的求解大都采用极小化如下损失函数：\n\n$$\n\\sum_{i=1}^nL(y_i,\\sum_{m=1}^M\\beta_mb(x,\\gamma_m))\n$$\n\n但是当样本足够多，问题较为复杂时，如果直接求解，则要估计的参数将会有 $M+\\sum_{m=1}^M\\dim(\\gamma_m)$ 个，求解十分耗费算力，于是前向分布算法(Forward stagewise additive modeling)被提出了。 前向分布算法将原来的问题转换为——一步一步的估计基函数项，当在估计某一基函数项 $\\beta_mb(x,\\gamma_m)$ 时，其之前的 $\\beta_ib(x,\\gamma_i),i=1,2,\\ldots,m-1$ 将作为定值，不再被改变，故该算法的第m步即是取解决如下下问题：\n\n$$\n\\begin{aligned}\n\\min& \\sum_{i=1}^nL(y_i,f_m(x))\\\\\nf_m(x)&=f_{m-1}(x)+\\beta_mb(x,\\gamma_m)\n\\end{aligned}\n$$\n\n其中 $f_{m-1}(x)$ 已知。 当我们假定采用平方损失函数时，我们有：\n\n$$\n\\begin{aligned}\nLoss&=\\sum_{i=1}^n\\frac 12[y_i-(f_{m-1}(x)+\\beta_mb(x,\\gamma_m))]^2\\\\\n&=\\sum_{i=1}^n\\frac 12[(y_i-f_{m-1}(x))-\\beta_mb(x,\\gamma_m)]^2\\\\\n&=\\sum_{i=1}^n\\frac 12(r_{mi}-\\beta_mb(x,\\gamma_m))^2\n\\end{aligned}\n$$\n\n$r_{mi}=y_i-f_{m-1}(x_i)$ 是在第m-1 步拟合后模型的残差，从损失函数得第m步的本质在于利用第m个基函数对上一步所得残差进行拟合，但这种理解是建立在损失函数为平方损失的情况下。\n\n-   前向的意义：\"前向\"指的是逐步进行的过程，即算法是从零开始，逐步向解的方向构建和优化。每一步添加一个新的基函数或模型参数，以逐渐逼近最终的目标。与之相对的概念是“后向”（Backward），后向算法通常从一个复杂的模型开始，然后逐步移除不必要的成分，而前向算法则是从简单的模型开始逐步加成。\n\n-   分布的含义：\"分布\"或\"阶段性\"（Stagewise）意味着算法在每一步迭代时，只增加一个新的基函数或参数，而不会对之前的基函数或参数进行重新调整。也就是说，每次迭代仅调整新增的模型成分，已添加的部分在整个过程中保持不变。这种特性使得算法较为保守，每次变化较小，且有利于控制模型的复杂度和避免过拟合。\n\n------------------------------------------------------------------------\n\n# 参数估计\n\n假设有一个加法模型：\n\n$$\nF(\\mathbf x;\\{\\beta_m,\\mathbf a_m\\}_1^M)=\\sum_{i=1}^M\\beta_mh(\\mathbf x;\\mathbf a_m)\n$$\n\n$h(\\mathbf x;\\mathbf a_m)$ 是一个参数比较简单的基函数；$\\mathbf a=\\{a_1,a_2,\\ldots\\}$\n\n## 数值优化\n\n需要估计的参数为：\n\n$$\n\\mathbf P^*=\\arg\\min_{\\mathbf P}\\Phi(\\mathbf P)\n$$\n\n$$\n\\Phi(\\mathbf P)=E_{y,\\mathbf x}L(y,F(\\mathbf x;\\mathbf P))\n$$\n\n最优的模型为： \n\n$$\nF^*(\\mathbf x)=F(\\mathbf x;\\mathbf P^*)\n$$\n\n参数的组成：\n\n$$\n\\mathbf P^*=\\sum_{m=0}^M\\mathbf p_m\n$$\n\n对于参数构成的理解：即所有参数的列向量，每一次拟合模型都相当于往 $\\mathbf P$ 的某些列上加值/更新值。\n\n## 最速下降法(Steepest-descent)\n\n由前面讨论可得，有如下优化问题：\n\n$$\n\\min_{\\mathbf P}\\Phi(\\mathbf P)\n$$\n\n则由梯度下降法得到的梯度为：\n\n$$\n\\mathbf g_m=\\nabla\\Phi(\\mathbf P)|_{\\mathbf P=\\mathbf P_{m-1}}=[\\frac{\\partial\\Phi(\\mathbf P)}{\\partial P_j}|_{\\mathbf P =\\mathbf P_{m-1} }]^T\n$$\n\n由于 $\\mathbf P_{m-1}=\\sum_{i=0}^{m-1}\\mathbf p_i$, 则得到如下迭代公式：\n\n$$\n\\mathbf P_m=\\mathbf P_{m-1}+\\mathbf p_m\n$$\n\n而由最速下降法得:\n\n$$\n\\mathbf p_m=-\\rho_m\\mathbf g_m\n$$\n\n由一维线搜索得步长 $\\rho_m$ 有下式决定： 由前面讨论可得，有如下优化问题：\n\n$$\n\\min_{\\mathbf P}\\Phi(\\mathbf P)\n$$\n\n则由梯度下降法得到的梯度为：\n\n$$\n\\mathbf g_m=\\nabla\\Phi(\\mathbf P)|_{\\mathbf P=\\mathbf P_{m-1}}=[\\frac{\\partial\\Phi(\\mathbf P)}{\\partial P_j}|_{\\mathbf P =\\mathbf P_{m-1} }]^T\n$$\n\n由于 $\\mathbf P_{m-1}=\\sum_{i=0}^{m-1}\\mathbf p_i$, 则得到如下迭代公式：\n\n$$\n\\mathbf P_m=\\mathbf P_{m-1}+\\mathbf p_m\n$$\n\n而由最速下降法得:\n\n$$\n\\mathbf p_m=-\\rho_m\\mathbf g_m\n$$\n\n由一维线搜索得步长 $\\rho_m$ 有下式决定：\n\n$$\n\\rho_m=\\arg\\min_\\rho\\Phi(\\mathbf P_{m-1}-\\rho\\mathbf g_m)\n$$\n\n## 函数空间上的数值优化\n\n假定某函数由多个基函数相加而成，即加法模型：\n\n$$\nF^*(x)=\\sum_{m=0}^Mf_m(x)\n$$\n\n有如下优化问题：\n\n$$\n\\min\\Phi(F(x))=E_y[L(y,F(x))|x]\n$$\n\n$F_m(x)$ 是迭代的类似于 $x_k=x_{k-1}+\\alpha_kd_k$ 则由最速下降法得：\n\n$$\nf_m(\\mathbf x)=-\\rho_m\\mathbf g_m(\\mathbf x)\n$$\n\n$$\n\\mathbf g_m(\\mathbf x)=E_y[\\frac{\\partial L(y,F(\\mathbf x))}{\\partial F(\\mathbf x)}|\\mathbf x]_{\\mathbf F(x)=\\mathbf F_{m-1}(\\mathbf x)}\n$$\n\n步长 $\\rho_m$ 由下列精确一维线搜索决定：\n\n$$\n\\rho_m = \\arg\\min_\\rho E_{y,\\mathbf x}L(y,F_{m-1}(\\mathbf x)-\\rho\\mathbf g_m(\\mathbf x))\n$$\n\n## 有限维数据——梯度提升算法\n\n从参数的角度而言，已知样本数据 $\\{y_i,\\mathbf x_i\\}$，模型为加法模型，则有如下参数优化问题：\n\n$$\n\\{\\beta_m^*,\\mathbf a_m^*\\}_1^M=\\arg\\min_{\\{\\beta_m,\\mathbf a_m\\}_1^M}\\sum_{i=1}^NL(y_i,\\sum_{m=1}^M\\beta_mh(\\mathbf x_i;\\mathbf a_m))\n$$ {#eq-参数优化限制条件}\n\n要直接求解上述参数优化问题将会比较棘手，因为需要在一个优化问题中求解 $M\\cdot(1+\\dim (\\mathbf a_m))$ 个参数，参数个数庞大，我们考虑使用 @sec-前向分布算法 中的前向分布算法，其将一个参数优化问题转换为一个函数优化问题。 利用前向分布算法后，此时需要解决函数优化问题，即对该优化问题的每一步有：\n\n$$\nF^*=\\arg\\min_{F}\\sum_{i=1}^NL(y_i,F(\\mathbf x_i))\n$$\n\n$$\nF_m(\\mathbf x)=F_{m-1}(\\mathbf x)+\\beta_m h(\\mathbf x;\\mathbf a_m)\n$$\n\n由最速下降法得:\n\n$$\nh(\\mathbf x_i;\\mathbf a_m)=-\\mathbf g_m(\\mathbf x_i)=-[\\frac{\\partial L(y_i,F(\\mathbf x_i))}{\\partial F(\\mathbf x_i)}]_{F(\\mathbf x)=F_{m-1}(\\mathbf x)}\n$$\n\n但是 $h(\\mathbf x;\\mathbf a_m)$ 是一个在 $\\mathcal D_{\\mathbf x}$ 上均有定义的函数，而 $\\mathbf g_m(\\mathbf x_i)$ 仅在 $\\{\\mathbf x_1,\\ldots,x_N\\}$ 处有定义，故上式并不能直接将 $h(\\mathbf x;\\mathbf a_m)$ 直接确定下来，需要采用其他方式让 $h(\\mathbf x;\\mathbf a_m)$ 去贴合负梯度方向。 一种想法是在 $h(\\mathbf x;\\mathbf a_m)$ 函数族中找到一个 $h^*(\\mathbf x;\\mathbf a_m)$ 使得 $\\mathcal h_m=\\{h(\\mathcal x_i;\\mathcal a_m)\\}_1^N$ 与 $-\\mathcal g_m$ 最平行，则有如下优化问题：\n\n$$\n\\mathbf a_m=\\arg\\min_a\\sum_{i=1}^N[-\\mathbf g_m(\\mathbf x_i)-\\alpha h(\\mathbf x_i;\\mathbf a)]^2\n$$\n\n::: callout-tip\n## 寻找负梯度方向说明\n\n-   此处的思想是回归的思想，但是没有截距项因为我们要的就是尽量平行。两向量平行的数学定义是 $-\\mathbf g_m=\\alpha \\mathbf h_m$\n\n-   $\\alpha$ 也是该优化问题所要求解的\n:::\n\n一旦确定了 $h(\\mathbf x;\\mathbf a_m)$ 后，我们便可利用精确一维线搜索去求解步长 $\\beta_m$:\n\n$$\n\\beta_m=\\arg\\min_{\\beta}\\sum_{i=1}^NL(y_i,F_{m-1}(\\mathbf x_i+\\beta h(\\mathbf x_i;\\mathbf a_m)))\n$$\n\n确定了 $\\beta_m,\\mathbf a_m$ 后便可得到：\n\n$$\nF_m(\\mathbf x)=F_{m-1}(\\mathbf x)+\\beta_m h(\\mathbf x;\\mathbf a_m)\n$$\n\n在现实数据中并不是直接去找 @eq-参数优化限制条件 下的 $h(\\mathbf x;\\mathbf a_m)$ ，而是将 $h(\\mathbf x;\\mathbf a_m)$ 去拟合伪响应 $\\{\\tilde{y}_i=-\\mathbf g_m(\\mathbf x_i)\\}_i^N$, 这使得函数优化问题转化为了最小二乘函数优化问题。\n\n\n```{pseudocode}\n\\begin{algorithm}\n\\caption{梯度提升算法}\n\\begin{algorithmic}\n\\State 初始化模型 $F_0(\\mathbf{x}) = \\arg\\min_{\\rho} \\sum_{i=1}^N L(y_i, \\rho)$\n\\For{$m = 1$ 到 $M$}\n    \\State 计算伪残差：$\\tilde y_i = -\\left[\\frac{\\partial L(y_i, F(\\mathbf{x}_i))}{\\partial F(\\mathbf{x}_i)}\\right]_{F=F_{m-1}}$\n    \\State 拟合一个基学习器 $h_m(\\mathbf{x};\\mathbf a_m)$ 来拟合伪残差 $\\tilde y_i:\\mathbf a_m=\\arg\\min_{\\mathbf a}\\sum_{i=1}^N[-\\mathbf g_m(\\mathbf x_i)-\\alpha h(\\mathbf x_i;\\mathbf a)]^2$\n    \\State 计算最佳步长：$\\beta_m=\\arg\\min_{\\beta}\\sum_{i=1}^NL(y_i,F_{m-1}(\\mathbf x_i+\\beta h(\\mathbf x_i;\\mathbf a_m)))$\n    \\State 更新模型：$F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\beta_m h_m(\\mathbf{x};\\mathbf a_m)$\n\\EndFor\n\\State \\Return 最终模型 $F_M(\\mathbf{x})$\n\\end{algorithmic}\n\\end{algorithm}\n```\n\n",
    "supporting": [
      "GBDT_files"
    ],
    "filters": [],
    "includes": {}
  }
}