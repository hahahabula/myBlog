{
  "hash": "11d3cdd828c075701747b069fd50e88e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Logistic回归\"\ndate: 2025/02/23\ncategories: [机器学习]\ndescription: \"介绍机器学习中有关Logistic回归与最大熵模型\"\n---\n\n\nLogistic回归是统计学习中的经典分类方法，而最大熵是概率模型学习的一个准则，将其推广到分类问题中便得到了最大熵模型(MEM, maximum entropy model)。\n\n# Logistic回归模型\n\nLogistic回归模型是一种对数线性模型。\n\n## Logistic分布\n\nLogistic分布的定义如下：\n\n1.  分布 $$\n    F(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}\n    $$\n\n2.  密度 $$\n    f(x)=F'(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(e^{-(x-\\mu)/\\gamma})^2}\n    $$\n\n式中，$\\mu$ 为位置参数，$\\gamma>0$ 为形状参数。\n\n::: {#8828f5c7 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.svg){}\n:::\n:::\n\n\n## 二项逻辑回归模型\n\n二项逻辑回归模型是一种分类模型，由 $P(Y|X)$ 表示，其形式如下式：\n\n$$\n\\begin{aligned}\nP(Y=1|x)=\\frac{\\exp(w\\cdot x)}{1+\\exp(w\\cdot x)}\\\\\nP(Y=0|x)=\\frac{1}{1+\\exp(w\\cdot x)}\n\\end{aligned}\n$$\n\n::: callout-note\n## 事件的几率(odd)\n\n事件的几率是指事件发生的概率于该事件不发生概率的比值，对数几率（Logit函数）是指 $\\log(\\frac{p}{1-p})$\n:::\n\n则输出 $Y=1$ 的对数几率为:\n\n$$\n\\log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x\n$$\n\n为输入x的线性函数，即输出 $Y=1$ 的对数几率是由输入 $x$ 的线性函数表示的模型，这便是Logistic回归模型。\n\n### 模型参数估计\n\nLogistic回归模型可采用极大似然估计模型参数，假设有 $P(Y=1|x)=\\pi(x),P(Y=0|x)=1-\\pi(x)$\n\n![Logistic回归模型参数的极大似然估计](images/paste-1.jpeg){width=\"381\"}\n\n对于求解一对数似然函数为目标函数的最优化问题，Logistic回归学习常用的方法是梯度下降法和拟牛顿法。\n\n### 多项Logistic回归\n\n假设离散型随机变量 $Y$ 的取值集合是 $\\{1, 2, \\ldots, K\\}$，那么多项Logistic回归模型是：\n\n$$\n\\begin{aligned}\nP(Y=k|x)&=\\frac{\\exp(w_k\\cdot x)}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\cdot x)}&k=1,2,\\ldots, K-1\\\\\nP(Y=K|x)&=\\frac{1}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\cdot x)}\n\\end{aligned}\n$$\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}