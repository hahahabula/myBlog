{
  "hash": "c5f11de01cd513bac1f6981b6becbb5b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"机器学习的简单实例：线性回归\"\ndate: 2025/02/16\ncategories: [深度学习, 《神经网络和深度学习》]\ndescription: \"介绍有关线性回归模型在不同学习准则和优化算法下的不同情况\"\n---\n\n\n# 线性模型\n\n$$\nf(x;w,b)=w^Tx+b\n$$\n\n上式中的权重向量 $w\\in\\mathbb{R}^d$ 和偏置 $b\\in \\mathbb{R}$，是可学习的参数，其可写成如下形式：\n\n$$\nf(x;\\tilde{w})=\\tilde{w}^T\\cdot\\tilde{x}\n$$\n\n# 参数学习\n\n已知一组包含 $N$ 个训练样本的训练集 $\\mathcal{D}=\\{(x^{(n)},y^{(n)}\\}^D_{n=1}$，下文将探讨如何学习有关最优的线性回归的模型参数 $w$。\n\n## 经验风险最小化（最小二乘法）\n\n损失函数采用平方损失函数，按照经验风险最小化准则，训练集上的经验风险定义为：\n\n![最小二乘计算过程](images/paste-1.png){width=\"495\"}\n\n::: callout-tip\n## 最小二乘法得出的最优参数的限制\n\n由最优参数 $w^*$ 的形式可得：要求 $XX^T$ 满秩。当 $XX^T$ 不可逆时，可采用下面两种方法来估计参数：\n\n-   先使用主成分分析等方法来预处理数据，消除不同特征之间的相关性，然后再使用最小二乘法来估计参数（减少特征的维数）\n-   通过梯度下降法来估计参数，先初始化 $w=0$，然后通过下面公式进行迭代：\n\n$$\nw\\leftarrow w+\\alpha X(y-X^Tw)\n$$\n:::\n\n::: {#1426b4c5 .cell execution_count=1}\n``` {.python .cell-code}\n# 生成模拟数据\nimport numpy as np\n# 设置随机种子，确保结果可重复\nnp.random.seed(42)\n# 参数设置\nnum_points = 100  # 数据点数量\na = 2  # 斜率\nb = 5  # 截距\nsigma = 3  # 噪声的标准差\n# 生成X值（随机或均匀分布）\nX = np.linspace(1, 10, num_points)  # 在1到10之间均匀生成num_points个点\nepsilon = np.random.normal(0, sigma, num_points)  # 生成正态分布噪声\nY = a * X + b + epsilon\nY = Y.reshape(-1, 1)\n\n# 利用经验风险最小化计算权重\nX_t = np.vstack((X, [1 for _ in range(num_points)]))\nw = np.linalg.inv(X_t @ X_t.T) @ X_t @ Y\nprint(f\"经验风险最小化后的权重系数：\\n{w}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n经验风险最小化后的权重系数：\n[[2.04597756]\n [4.43558388]]\n```\n:::\n:::\n\n\n当采用小批量随机梯度下降法来估计参数，结果如下：\n\n::: {#367e7a1e .cell execution_count=2}\n``` {.python .cell-code}\nimport tensorflow as tf\ntf.random.set_seed(42)\n# 1. 定义模型\nclass LinearRegressionModel:\n    def __init__(self):  # <1>\n        # 初始化权重\n        self.weights = tf.Variable(initial_value=tf.random.normal([2, 1]), name=\"权重\", dtype=tf.float32)  # <2>\n    def __call__(self, x):\n        return tf.matmul(x, self.weights)  # <3>\n# 2. 定义损失函数（均方误差）\ndef mean_squared_error(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_true - y_pred))  # <4>\n# 3. 小批量随机梯度下降算法\ndef train(model, X, y, learning_rate=0.01, batch_size=10, epochs=100):\n    # 创建优化器\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n    # 划分小批量\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    dataset = dataset.shuffle(buffer_size=X.shape[0]).batch(batch_size)\n\n    for epoch in range(epochs):\n        for batch_X, batch_y in dataset:\n            with tf.GradientTape() as tape:\n                # 计算预测值\n                y_pred = model(batch_X)\n                # 计算损失\n                loss = mean_squared_error(batch_y, y_pred)\n             # 计算梯度\n            gradients = tape.gradient(loss, model.weights)\n            # 更新参数\n            optimizer.apply_gradients([(gradients, model.weights)])\n    return model.weights.numpy()\n# 训练模型\nmodel = LinearRegressionModel()\nweights = train(model, tf.constant(X_t.T, dtype=tf.float32), tf.constant(Y, dtype=tf.float32), learning_rate=0.01, batch_size=10, epochs=1000)\nprint(f\"采用小批量随机梯度下降法训练后的权重: \\n{weights}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n采用小批量随机梯度下降法训练后的权重: \n[[2.0707269]\n [4.4438233]]\n```\n:::\n:::\n\n\n1.  初始化模型参数\n2.  tf.random.normal(\\[2, 1\\])中的\\[2,1\\]指的是参数的维度\n3.  构造模型的形式\n4.  构造损失函数\n\n::::: grid\n::: g-col-4\n\n```{mermaid}\n%%| fig-cap: \"利用tensorflow实现机器学习模型的步骤图\"\nclassDiagram\n    class 构建模型{\n        +初始化模型参数(__init__)\n        +表示模型(__call__)\n    }\n    class 学习准则{\n        +函数名(y_true, y_pred)\n    }\n    class 优化算法{\n        +定义优化算法(train)\n    }\n构建模型 --> 学习准则\n学习准则 --> 优化算法\n```\n\n:::\n\n::: g-col-8\n\n::: {#c3cc16ff .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![结果对比图](index_files/figure-html/cell-4-output-1.svg){}\n:::\n:::\n\n\n由上图结果可得，两种算法的效率类似，且都在靠近0的部分有较大偏离。\n:::\n:::::\n\n## 结构风险最小化（岭回归）\n\n结构风险最小化等价于岭回归，可以用解决最小二乘回归法对 $XX'$ 满秩的要求。岭回归的想法是在 $XX'$ 中加入 $\\lambda I$ 一项使得 $XX'+\\lambda I$ 满秩，而结构风险最小化是追求下式目标函数最小( $\\lambda>0$ 为正则化系数：\n\n$$\nR(w)=\\frac 1 2||y-X^Tw||^2+\\frac 12 \\lambda ||w||^2\n$$\n\n由下图可得二者结果等价。\n\n![结构风险最小化等价于岭回归](images/paste-1.jpeg){width=\"602\"}\n\n## 最大似然估计\n\n::: callout-tip\n## 机器学习任务的分类\n\n1.  样本的特征向量 $x$ 和标签 $y$ 之间存在未知的函数关系 $y=h(x)$\n2.  条件概率 $p(y|x)$ 服从某个未知分布\n:::\n\n之前介绍的最小二乘法是属于第一类，现在从建模条件概率 $p(y|x)$ 的角度进行参数估计。\n\n假设 $y\\sim N(\\tilde{w}^Tx, \\sigma^2)$，参数 $\\tilde{w}$ 在训练集 $\\mathcal{D}$ 上的似然函数为：\n\n$$\n\\begin{aligned}\np(y|\\tilde{X};w,\\sigma)&= \\prod_{n=1}^Np(y^{(n)}|x^{(n)};\\tilde{w},\\sigma)\\\\\n&=\\prod_{n=1}^N N(\\tilde{w}^Tx^{(n)},\\sigma^2)\n\\end{aligned}\n$$\n\n由最大似然估计原理可得如下图：\n\n![最大似然估计的结果](images/paste-2.jpeg){width=\"587\"}\n\n易得最大似然估计的解和最小二乘法的解相同。\n\n## 最大后验估计\n\n假设参数 $\\tilde{w}$ 为一个随机向量，并服从一个先验分布 $p(\\tilde{w};v)=N(\\tilde{w};0,v^2I)$， $v^2$ 为每一维上的方差。\n\n由贝叶斯估计原理得如下图：\n\n![最大后验估计](images/paste-3.jpeg)\n\n```c++\n#include <iostream>  // <1>\n\n```\n1. 急急急\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}