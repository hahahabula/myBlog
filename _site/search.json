[
  {
    "objectID": "posts/[深推]S2.5/index.html",
    "href": "posts/[深推]S2.5/index.html",
    "title": "S2.5从FM到FMM-自动特征交叉的解决方案",
    "section": "",
    "text": "在对样本集合进行分组研究时，在分组比较中都占优势的一方，在总评中有时反而是失势的一方——辛普森悖论，有如下的例子：\n\n\n\n\n\n\n\n男性用户\n\n\n视频\n点击\n曝光\n点击率\n\n\n\n\nA\n8\n530\n1.51%\n\n\nB\n51\n1520\n3.36%\n\n\n\n\n\n\n女性用户\n\n\n视频\n点击\n曝光\n点击率\n\n\n\n\nA\n201\n2510\n8.01%\n\n\nB\n92\n1010\n9.11%\n\n\n\n\n\n\n\n数据汇总（忽略性别这个维度）\n\n\n视频\n点击\n总曝光\n点击率\n\n\n\n\nA\n209\n3040\n6.88%\n\n\nB\n143\n2530\n5.65%\n\n\n\n奇怪的现象：A的点击率在男女中都少于B，但汇总后A的点击率却高于B 原因：分组实验是使用“性别”+“视频id”的组合特征计算点击率，而汇总实验则是实验“视频id”这一单一特征计算点击率，汇总使用对高维特征进行了合并，损失了大量有效的信息而无法正确刻画数据模式。 逻辑回归模型只对单一特征做简单加权，不具备进行特征交叉生成高维组合特征的能力，因此表达能力弱，可能得出像“辛普森悖论”类似的错误结论。"
  },
  {
    "objectID": "posts/[深推]S2.5/index.html#数学表达式",
    "href": "posts/[深推]S2.5/index.html#数学表达式",
    "title": "S2.5从FM到FMM-自动特征交叉的解决方案",
    "section": "3.1 数学表达式",
    "text": "3.1 数学表达式\n\\[\nFM(w,x)=\\sum_{j_1=1}^{n-1}\\sum_{j_2=j_1+1}^n(w_{j_1},w_{j_2})x_{j_1}x_{j_2},(\\cdot,\\cdot)\\text{表示内积}\n\\]\n\nFM 模型为每个模型学习了一个隐权重向量（latent vector）\nFM 模型引入隐向量的做法与矩阵分解隐向量的思想类似，但 FM 模型将单纯的用户、物品隐向量拓展到了所有特征上。\nFM 节省了训练开销，将权重参数的复杂度从 \\(n^2\\) 级别下降到了 \\(nk\\), \\(k\\) 为隐向量维度 \\(n&gt;&gt;k\\)"
  },
  {
    "objectID": "posts/[深推]S2.5/index.html#优点",
    "href": "posts/[深推]S2.5/index.html#优点",
    "title": "S2.5从FM到FMM-自动特征交叉的解决方案",
    "section": "3.2 优点",
    "text": "3.2 优点\n举例来说，在某商品推荐的场景下，样本有两个特征，分别是频道（channel）和品牌（brand），某训练样本的特征组合是（ESPN，Adidas）。\n\n疏解模型对数据稀疏性的要求 在 POLY 2 中，只有当 ESPN 和 Adidas 同时出现在一个训练样本中时，模型才能学到这个组合特征对应的权重；而在 FM 中，ESPN 的隐向量也可以通过（ESPN，Gucci）样本进行更新，Adidas 的隐向量也可以通过（NBC，Adidas）样本进行更新，这大幅降低了模型对数据稀疏性的要求。\n提高模型的泛化能力 甚至对于一个从未出现过的特征组合（NBC，Gucci），由于模型之前已经分别学习过 NBC 和 Gucci 的隐向量，具备了计算该特征组合权重的能力，这是 POLY 2 无法实现的。相比 POLY 2，FM 虽然丢失了某些具体特征组合的精确记忆能力，但是泛化能力大大提高。\n相比于以后的深度学习，FM 模型更易进行线上部署和服务"
  },
  {
    "objectID": "posts/[深推]S2.5/index.html#ffm数学表示",
    "href": "posts/[深推]S2.5/index.html#ffm数学表示",
    "title": "S2.5从FM到FMM-自动特征交叉的解决方案",
    "section": "4.1 FFM数学表示",
    "text": "4.1 FFM数学表示\n\\[\ny=W^TX+\\sum_{j_1=1}^{n-1}\\sum_{j_2=j_1+1}^n(w_{j_1,f_2},w_{j_2,f_1})x_{j_1}x_{j_2}=W^TX+FFM(W_1,X)\n\\]"
  },
  {
    "objectID": "posts/[深推]S2.5/index.html#ffm隐变量的讨论",
    "href": "posts/[深推]S2.5/index.html#ffm隐变量的讨论",
    "title": "S2.5从FM到FMM-自动特征交叉的解决方案",
    "section": "4.2 FFM隐变量的讨论",
    "text": "4.2 FFM隐变量的讨论\n当获得如下一个训练样本：\n\n\n\nTable 1: 样本\n\n\n\n\n\nPublisher(P)\nAdvertiser(A)\nGender(G)\n\n\n\n\nEPSN\nNIKE\nMale\n\n\n\n\n\n\n在 FM 中特征 ESPN、NIKE 和 Male 都有对应的隐向量 \\(w_{ESPN},w_{NIKE},w_{Male}\\) 那么ESPN特征与NIKE特征、ESPN特征与Male特征做交叉的权重应该是 \\((w_{ESPN}, w_{NIKE}), (w_{ESPN},w_{Male})\\) 。 在 FFM 中，ESPN 与 NIKE、ESPN 与 Male 交叉特殊的权重分别是 \\((w_{ESPN,A}, w_{NIKE,P})\\) 和 \\((w_{ESPN,G}, w_{Male,P})\\) \\(f\\) 表示特征域的个数，也即是分类型数据的个数。"
  },
  {
    "objectID": "posts/[深推]S2.5/index.html#ffm模型的评价",
    "href": "posts/[深推]S2.5/index.html#ffm模型的评价",
    "title": "S2.5从FM到FMM-自动特征交叉的解决方案",
    "section": "4.3 FFM模型的评价",
    "text": "4.3 FFM模型的评价\n\nFFM 模型训练中，需要学习 n 个特征在 \\(f\\) 个域上的 k 维隐向量，参数个数共 \\(n\\times k\\times (f-1)\\) 个，FFM 的复杂度为 \\(kn^2\\)\nFFM 引入了特征域，为模型引入了更多有价值的信息，表达能力更强，但计算复杂度更高。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "关于",
    "section": "",
    "text": "About this blog\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hahabula_blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDay12\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的分发糖果\n\n\n\n\n\nMar 10, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay11\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的分发糖果\n\n\n\n\n\nMar 9, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay10\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的爬楼梯\n\n\n\n\n\nMar 8, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay9\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的加一\n\n\n\n\n\nMar 7, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\n近期汇报\n\n\n\n\n\n\n组会\n\n\n搜广推\n\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nC++模板参数、字符串的处理\n\n\n\n\n\n\nC++\n\n\n\n介绍有关C++模板参数和字符串的处理\n\n\n\n\n\nMar 3, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay8\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的排序序列、有效数独、接雨水、旋转图像、加一\n\n\n\n\n\nMar 3, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\n降雨二元预测\n\n\n\n\n\n\n项目\n\n\n深度学习\n\n\n\nKaggle上的竞赛\n\n\n\n\n\nMar 2, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay7\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的最长连续序列和四数之和\n\n\n\n\n\nMar 1, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay6\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的最长连续序列和两数之和\n\n\n\n\n\nFeb 28, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay5\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的最长连续序列和两数之和\n\n\n\n\n\nFeb 27, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS4.离散特征处理\n\n\n\n\n\n\n搜广推\n\n\n王树森\n\n\n\n介绍有关离散特征的处理方法，如建立字典、向量化方法等\n\n\n\n\n\nFeb 27, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay4\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关线性表中数组部分的哈希表\n\n\n\n\n\nFeb 26, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay3\n\n\n\n\n\n\n刷题\n\n\n算法\n\n\n\n介绍有关\n\n\n\n\n\nFeb 25, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay2\n\n\n\n\n\n\n算法\n\n\n刷题\n\n\n\n线性表中有关数组的搜索旋转排序数组、\n\n\n\n\n\nFeb 24, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS3.协同过滤算法(召回)\n\n\n\n\n\n\n搜广推\n\n\n王树森\n\n\n\n介绍有召回部分的协同过滤算法，如ItemCF,UserCF,Swing\n\n\n\n\n\nFeb 24, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic回归\n\n\n\n\n\n\n机器学习\n\n\n\n介绍机器学习中有关Logistic回归与最大熵模型\n\n\n\n\n\nFeb 23, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nSoftmax函数\n\n\n\n\n\n\n机器学习\n\n\n\n介绍Softmax函数\n\n\n\n\n\nFeb 23, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nDay1\n\n\n\n\n\n\n算法\n\n\n刷题\n\n\n\n线性表中删除有序数组中的重复项、删除有序数组中的重复项Ⅱ\n\n\n\n\n\nFeb 23, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS2.7 LS-PLM模型\n\n\n\n\n\n\n搜广推\n\n\n《深度学习推荐系统》\n\n\n\n介绍曾经阿里巴巴的主流推荐模型——LS-PLM模型\n\n\n\n\n\nFeb 20, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nC++存储类\n\n\n\n\n\n\nC++\n\n\n\n介绍C++中有关存储类的知识以及运算符\n\n\n\n\n\nFeb 18, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nC++循环\n\n\n\n\n\n\nC++\n\n\n\n介绍有关C++循环语句的部分\n\n\n\n\n\nFeb 18, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS1.3 设计算法\n\n\n\n\n\n\n算法\n\n\n《算法导论》\n\n\n\n介绍有关分治法的有关内容\n\n\n\n\n\nFeb 18, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\n机器学习的简单实例：线性回归\n\n\n\n\n\n\n深度学习\n\n\n《神经网络和深度学习》\n\n\n\n介绍有关线性回归模型在不同学习准则和优化算法下的不同情况\n\n\n\n\n\nFeb 16, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS1.推荐系统简介\n\n\n\n\n\n\n搜广推\n\n\n王树森\n\n\n\n王树森老师视频中介绍有关推荐系统的消费侧指标、实验流程和链路的内容\n\n\n\n\n\nFeb 13, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS2.A/B测试\n\n\n\n\n\n\n搜广推\n\n\n王树森\n\n\n\n介绍有关A/B测试、随机分桶、分层实验、Holdout机制等内容\n\n\n\n\n\nFeb 13, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS1.2 算法分析\n\n\n\n\n\n\n算法\n\n\n《算法导论》\n\n\n\n以插入排序算法为例介绍如何计算算法的运行时间\n\n\n\n\n\nFeb 12, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nGBDT模型\n\n\n\n\n\n\n机器学习\n\n\n搜广推\n\n\n\n介绍GBDT模型\n\n\n\n\n\nFeb 7, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS2.6 GBDT+LR–特征工程模型化的开端\n\n\n\n\n\n\n搜广推\n\n\n《深度学习推荐系统》\n\n\n\n解释由Facebook提出的GBDT+LR模型\n\n\n\n\n\nFeb 7, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS1.1 插入排序\n\n\n\n\n\n\n算法\n\n\n《算法导论》\n\n\n\n《算法导论》第二章算法基础中的有关循环不变式、伪代码、插入排序的解释\n\n\n\n\n\nFeb 6, 2025\n\n\nHahabula\n\n\n\n\n\n\n\n\n\n\n\n\nS2.5从FM到FMM-自动特征交叉的解决方案\n\n\n\n\n\n\n搜广推\n\n\n《深度学习推荐系统》\n\n\n\n介绍深度学习推荐系统有关自动特征解决的方案\n\n\n\n\n\nJan 25, 2025\n\n\nHahabula\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "posts/[深推]S2.6/index.html",
    "href": "posts/[深推]S2.6/index.html",
    "title": "S2.6 GBDT+LR–特征工程模型化的开端",
    "section": "",
    "text": "FFM模型采用了引入特征域的方式增强了模型的特征交叉能力，但无论如何，FFM模型只能做二阶的特征交叉。如果继续提高特征交叉的维度，会不可避免第产生组合爆炸和计算复杂度过高的问题，因此Facebook提出了以下基于GBDT+LR组合模型的解决方案。\n\n1 GBDT+LR组合模型的结构\nGBDT+LR组合模型是由两个独立训练的部分构成，其结构图如下：\n\n\n\n\n\n\n\nGBDT+LR组合模型的结构图\n\n\n构建该模型首先得先构建一个GBDT（梯度提升树）而后在利用LR模型预测点击率。下文首先介绍利用GBDT进行特征交叉。\n\n\n2 GBDT模型\n详细的模型介绍见机器学习中的GBDT模型，在GBDT+LR模型中，特征交叉的示意图如下图所示：\n\n\n\n\n\n\n\nGBDT用于特征交叉示意图\n\n\n\n\n3 GBDT+LR模型的意义\n\nGBDT+LR大大推进了特征工程模型化这一重要趋势。在其出现以前，特征工程的两种解决方法是\na) 进行人工的或半人工的特征组合和特征筛选（对算法工程师的经验和精力投入要求高）\nb) 通过改造目标函数，改进模型结构，增加特征交叉湘的方式增强特征组合能力（对模型设计能力要求较高）\nGBDT+LR组合模型的提出意味着特征工程可以完全交由一个独立的模型来完成，模型的输入可以是原始的特征向量，不必再特征工程上投入过多的人工筛选和模型设计的精力，实现真正的端到端(End to End)训练\n\n\n\n\n\n\n\n端到端训练\n\n\n\n端对端训练依赖于神经网络的反向传播算法，通过计算损失函数的梯度来更新网络的权重和偏置，从而最小化输出与真实值之间的差异。在整个训练过程中，模型被视为一个整体进行优化，而不是将任务分解为多个独立的子任务。即GBDT模型不需要单独训练而是通过预测CTR的损失来进行更新。\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html",
    "href": "posts/GBDT模型/GBDT.html",
    "title": "GBDT模型",
    "section": "",
    "text": "GBDT (Gradient Boosting Decision Tree), 梯度提升树。它是一种基于决策树的集成算法。\n通过构造一组弱的学习（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效结合。"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html#gbdt的原理",
    "href": "posts/GBDT模型/GBDT.html#gbdt的原理",
    "title": "GBDT模型",
    "section": "1.1 GBDT的原理",
    "text": "1.1 GBDT的原理\n\n所有弱分类器的结构相加等于预测值\n每次都以当前预测值为基准，下一个弱分类器去拟合误差函数对预测值的误差\nGBDT 的弱分类器使用的是树模型"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html#前向分布算法",
    "href": "posts/GBDT模型/GBDT.html#前向分布算法",
    "title": "GBDT模型",
    "section": "1.2 前向分布算法",
    "text": "1.2 前向分布算法\n许多加法模型都有如下形式\n\nf(x)=\\sum_{i=1}^M\\beta_m b(x,\\gamma_m)\n\n\\beta_m 是系数, b(x,\\gamma_m) 是基函数并带有一组参数 \\gamma_m。 这类模型的参数的求解大都采用极小化如下损失函数：\n\n\\sum_{i=1}^nL(y_i,\\sum_{m=1}^M\\beta_mb(x,\\gamma_m))\n\n但是当样本足够多，问题较为复杂时，如果直接求解，则要估计的参数将会有 M+\\sum_{m=1}^M\\dim(\\gamma_m) 个，求解十分耗费算力，于是前向分布算法(Forward stagewise additive modeling)被提出了。 前向分布算法将原来的问题转换为——一步一步的估计基函数项，当在估计某一基函数项 \\beta_mb(x,\\gamma_m) 时，其之前的 \\beta_ib(x,\\gamma_i),i=1,2,\\ldots,m-1 将作为定值，不再被改变，故该算法的第m步即是取解决如下下问题：\n\n\\begin{align*}\n\\min& \\sum_{i=1}^nL(y_i,f_m(x))\\\\\nf_m(x)&=f_{m-1}(x)+\\beta_mb(x,\\gamma_m)\n\\end{align*}\n\n其中 f_{m-1}(x) 已知。 当我们假定采用平方损失函数时，我们有：\n\n\\begin{align*}\nLoss&=\\sum_{i=1}^n\\frac 12[y_i-(f_{m-1}(x)+\\beta_mb(x,\\gamma_m))]^2\\\\\n&=\\sum_{i=1}^n\\frac 12[(y_i-f_{m-1}(x))-\\beta_mb(x,\\gamma_m)]^2\\\\\n&=\\sum_{i=1}^n\\frac 12(r_{mi}-\\beta_mb(x,\\gamma_m))^2\n\\end{align*}\n\nr_{mi}=y_i-f_{m-1}(x_i) 是在第m-1 步拟合后模型的残差，从损失函数得第m步的本质在于利用第m个基函数对上一步所得残差进行拟合，但这种理解是建立在损失函数为平方损失的情况下。\n\n前向的意义：“前向”指的是逐步进行的过程，即算法是从零开始，逐步向解的方向构建和优化。每一步添加一个新的基函数或模型参数，以逐渐逼近最终的目标。与之相对的概念是“后向”（Backward），后向算法通常从一个复杂的模型开始，然后逐步移除不必要的成分，而前向算法则是从简单的模型开始逐步加成。\n分布的含义：“分布”或”阶段性”（Stagewise）意味着算法在每一步迭代时，只增加一个新的基函数或参数，而不会对之前的基函数或参数进行重新调整。也就是说，每次迭代仅调整新增的模型成分，已添加的部分在整个过程中保持不变。这种特性使得算法较为保守，每次变化较小，且有利于控制模型的复杂度和避免过拟合。"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html#数值优化",
    "href": "posts/GBDT模型/GBDT.html#数值优化",
    "title": "GBDT模型",
    "section": "2.1 数值优化",
    "text": "2.1 数值优化\n需要估计的参数为：\n\\[\n\\mathbf P^*=\\arg\\min_{\\mathbf P}\\Phi(\\mathbf P)\n\\]\n\\[\n\\Phi(\\mathbf P)=E_{y,\\mathbf x}L(y,F(\\mathbf x;\\mathbf P))\n\\]\n最优的模型为：\n\\[\nF^*(\\mathbf x)=F(\\mathbf x;\\mathbf P^*)\n\\]\n参数的组成：\n\\[\n\\mathbf P^*=\\sum_{m=0}^M\\mathbf p_m\n\\]\n对于参数构成的理解：即所有参数的列向量，每一次拟合模型都相当于往 \\(\\mathbf P\\) 的某些列上加值/更新值。"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html#最速下降法steepest-descent",
    "href": "posts/GBDT模型/GBDT.html#最速下降法steepest-descent",
    "title": "GBDT模型",
    "section": "2.2 最速下降法(Steepest-descent)",
    "text": "2.2 最速下降法(Steepest-descent)\n由前面讨论可得，有如下优化问题：\n\\[\n\\min_{\\mathbf P}\\Phi(\\mathbf P)\n\\]\n则由梯度下降法得到的梯度为：\n\\[\n\\mathbf g_m=\\nabla\\Phi(\\mathbf P)|_{\\mathbf P=\\mathbf P_{m-1}}=[\\frac{\\partial\\Phi(\\mathbf P)}{\\partial P_j}|_{\\mathbf P =\\mathbf P_{m-1} }]^T\n\\]\n由于 \\(\\mathbf P_{m-1}=\\sum_{i=0}^{m-1}\\mathbf p_i\\), 则得到如下迭代公式：\n\\[\n\\mathbf P_m=\\mathbf P_{m-1}+\\mathbf p_m\n\\]\n而由最速下降法得:\n\\[\n\\mathbf p_m=-\\rho_m\\mathbf g_m\n\\]\n由一维线搜索得步长 \\(\\rho_m\\) 有下式决定： 由前面讨论可得，有如下优化问题：\n\\[\n\\min_{\\mathbf P}\\Phi(\\mathbf P)\n\\]\n则由梯度下降法得到的梯度为：\n\\[\n\\mathbf g_m=\\nabla\\Phi(\\mathbf P)|_{\\mathbf P=\\mathbf P_{m-1}}=[\\frac{\\partial\\Phi(\\mathbf P)}{\\partial P_j}|_{\\mathbf P =\\mathbf P_{m-1} }]^T\n\\]\n由于 \\(\\mathbf P_{m-1}=\\sum_{i=0}^{m-1}\\mathbf p_i\\), 则得到如下迭代公式：\n\\[\n\\mathbf P_m=\\mathbf P_{m-1}+\\mathbf p_m\n\\]\n而由最速下降法得:\n\\[\n\\mathbf p_m=-\\rho_m\\mathbf g_m\n\\]\n由一维线搜索得步长 \\(\\rho_m\\) 有下式决定：\n\\[\n\\rho_m=\\arg\\min_\\rho\\Phi(\\mathbf P_{m-1}-\\rho\\mathbf g_m)\n\\]"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html#sec-前向分布算法",
    "href": "posts/GBDT模型/GBDT.html#sec-前向分布算法",
    "title": "GBDT模型",
    "section": "1.2 前向分布算法",
    "text": "1.2 前向分布算法\n许多加法模型都有如下形式\n\\[\nf(x)=\\sum_{i=1}^M\\beta_m b(x,\\gamma_m)\n\\]\n\\(\\beta_m\\) 是系数, \\(b(x,\\gamma_m)\\) 是基函数并带有一组参数 \\(\\gamma_m\\)。 这类模型的参数的求解大都采用极小化如下损失函数：\n\\[\n\\sum_{i=1}^nL(y_i,\\sum_{m=1}^M\\beta_mb(x,\\gamma_m))\n\\]\n但是当样本足够多，问题较为复杂时，如果直接求解，则要估计的参数将会有 \\(M+\\sum_{m=1}^M\\dim(\\gamma_m)\\) 个，求解十分耗费算力，于是前向分布算法(Forward stagewise additive modeling)被提出了。 前向分布算法将原来的问题转换为——一步一步的估计基函数项，当在估计某一基函数项 \\(\\beta_mb(x,\\gamma_m)\\) 时，其之前的 \\(\\beta_ib(x,\\gamma_i),i=1,2,\\ldots,m-1\\) 将作为定值，不再被改变，故该算法的第m步即是取解决如下下问题：\n\\[\n\\begin{aligned}\n\\min& \\sum_{i=1}^nL(y_i,f_m(x))\\\\\nf_m(x)&=f_{m-1}(x)+\\beta_mb(x,\\gamma_m)\n\\end{aligned}\n\\]\n其中 \\(f_{m-1}(x)\\) 已知。 当我们假定采用平方损失函数时，我们有：\n\\[\n\\begin{aligned}\nLoss&=\\sum_{i=1}^n\\frac 12[y_i-(f_{m-1}(x)+\\beta_mb(x,\\gamma_m))]^2\\\\\n&=\\sum_{i=1}^n\\frac 12[(y_i-f_{m-1}(x))-\\beta_mb(x,\\gamma_m)]^2\\\\\n&=\\sum_{i=1}^n\\frac 12(r_{mi}-\\beta_mb(x,\\gamma_m))^2\n\\end{aligned}\n\\]\n\\(r_{mi}=y_i-f_{m-1}(x_i)\\) 是在第m-1 步拟合后模型的残差，从损失函数得第m步的本质在于利用第m个基函数对上一步所得残差进行拟合，但这种理解是建立在损失函数为平方损失的情况下。\n\n前向的意义：“前向”指的是逐步进行的过程，即算法是从零开始，逐步向解的方向构建和优化。每一步添加一个新的基函数或模型参数，以逐渐逼近最终的目标。与之相对的概念是“后向”（Backward），后向算法通常从一个复杂的模型开始，然后逐步移除不必要的成分，而前向算法则是从简单的模型开始逐步加成。\n分布的含义：“分布”或”阶段性”（Stagewise）意味着算法在每一步迭代时，只增加一个新的基函数或参数，而不会对之前的基函数或参数进行重新调整。也就是说，每次迭代仅调整新增的模型成分，已添加的部分在整个过程中保持不变。这种特性使得算法较为保守，每次变化较小，且有利于控制模型的复杂度和避免过拟合。"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html#函数空间上的数值优化",
    "href": "posts/GBDT模型/GBDT.html#函数空间上的数值优化",
    "title": "GBDT模型",
    "section": "2.3 函数空间上的数值优化",
    "text": "2.3 函数空间上的数值优化\n假定某函数由多个基函数相加而成，即加法模型：\n\\[\nF^*(x)=\\sum_{m=0}^Mf_m(x)\n\\]\n有如下优化问题：\n\\[\n\\min\\Phi(F(x))=E_y[L(y,F(x))|x]\n\\]\n\\(F_m(x)\\) 是迭代的类似于 \\(x_k=x_{k-1}+\\alpha_kd_k\\) 则由最速下降法得：\n\\[\nf_m(\\mathbf x)=-\\rho_m\\mathbf g_m(\\mathbf x)\n\\]\n\\[\n\\mathbf g_m(\\mathbf x)=E_y[\\frac{\\partial L(y,F(\\mathbf x))}{\\partial F(\\mathbf x)}|\\mathbf x]_{\\mathbf F(x)=\\mathbf F_{m-1}(\\mathbf x)}\n\\]\n步长 \\(\\rho_m\\) 由下列精确一维线搜索决定：\n\\[\n\\rho_m = \\arg\\min_\\rho E_{y,\\mathbf x}L(y,F_{m-1}(\\mathbf x)-\\rho\\mathbf g_m(\\mathbf x))\n\\]"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html#有限维数据梯度提升算法",
    "href": "posts/GBDT模型/GBDT.html#有限维数据梯度提升算法",
    "title": "GBDT模型",
    "section": "2.4 有限维数据——梯度提升算法",
    "text": "2.4 有限维数据——梯度提升算法\n从参数的角度而言，已知样本数据 \\(\\{y_i,\\mathbf x_i\\}\\)，模型为加法模型，则有如下参数优化问题：\n\\[\n\\{\\beta_m^*,\\mathbf a_m^*\\}_1^M=\\arg\\min_{\\{\\beta_m,\\mathbf a_m\\}_1^M}\\sum_{i=1}^NL(y_i,\\sum_{m=1}^M\\beta_mh(\\mathbf x_i;\\mathbf a_m))\n\\tag{1}\\]\n要直接求解上述参数优化问题将会比较棘手，因为需要在一个优化问题中求解 \\(M\\cdot(1+\\dim (\\mathbf a_m))\\) 个参数，参数个数庞大，我们考虑使用 Section 1.2 中的前向分布算法，其将一个参数优化问题转换为一个函数优化问题。 利用前向分布算法后，此时需要解决函数优化问题，即对该优化问题的每一步有：\n\\[\nF^*=\\arg\\min_{F}\\sum_{i=1}^NL(y_i,F(\\mathbf x_i))\n\\]\n\\[\nF_m(\\mathbf x)=F_{m-1}(\\mathbf x)+\\beta_m h(\\mathbf x;\\mathbf a_m)\n\\]\n由最速下降法得:\n\\[\nh(\\mathbf x_i;\\mathbf a_m)=-\\mathbf g_m(\\mathbf x_i)=-[\\frac{\\partial L(y_i,F(\\mathbf x_i))}{\\partial F(\\mathbf x_i)}]_{F(\\mathbf x)=F_{m-1}(\\mathbf x)}\n\\]\n但是 \\(h(\\mathbf x;\\mathbf a_m)\\) 是一个在 \\(\\mathcal D_{\\mathbf x}\\) 上均有定义的函数，而 \\(\\mathbf g_m(\\mathbf x_i)\\) 仅在 \\(\\{\\mathbf x_1,\\ldots,x_N\\}\\) 处有定义，故上式并不能直接将 \\(h(\\mathbf x;\\mathbf a_m)\\) 直接确定下来，需要采用其他方式让 \\(h(\\mathbf x;\\mathbf a_m)\\) 去贴合负梯度方向。 一种想法是在 \\(h(\\mathbf x;\\mathbf a_m)\\) 函数族中找到一个 \\(h^*(\\mathbf x;\\mathbf a_m)\\) 使得 \\(\\mathcal h_m=\\{h(\\mathcal x_i;\\mathcal a_m)\\}_1^N\\) 与 \\(-\\mathcal g_m\\) 最平行，则有如下优化问题：\n\\[\n\\mathbf a_m=\\arg\\min_a\\sum_{i=1}^N[-\\mathbf g_m(\\mathbf x_i)-\\alpha h(\\mathbf x_i;\\mathbf a)]^2\n\\]\n\n\n\n\n\n\n寻找负梯度方向说明\n\n\n\n\n此处的思想是回归的思想，但是没有截距项因为我们要的就是尽量平行。两向量平行的数学定义是 \\(-\\mathbf g_m=\\alpha \\mathbf h_m\\)\n\\(\\alpha\\) 也是该优化问题所要求解的\n\n\n\n一旦确定了 \\(h(\\mathbf x;\\mathbf a_m)\\) 后，我们便可利用精确一维线搜索去求解步长 \\(\\beta_m\\):\n\\[\n\\beta_m=\\arg\\min_{\\beta}\\sum_{i=1}^NL(y_i,F_{m-1}(\\mathbf x_i+\\beta h(\\mathbf x_i;\\mathbf a_m)))\n\\]\n确定了 \\(\\beta_m,\\mathbf a_m\\) 后便可得到：\n\\[\nF_m(\\mathbf x)=F_{m-1}(\\mathbf x)+\\beta_m h(\\mathbf x;\\mathbf a_m)\n\\]\n在现实数据中并不是直接去找 Equation 1 下的 \\(h(\\mathbf x;\\mathbf a_m)\\) ，而是将 \\(h(\\mathbf x;\\mathbf a_m)\\) 去拟合伪响应 \\(\\{\\tilde{y}_i=-\\mathbf g_m(\\mathbf x_i)\\}_i^N\\), 这使得函数优化问题转化为了最小二乘函数优化问题。\n\n\n\\begin{algorithm} \\caption{梯度提升算法} \\begin{algorithmic} \\State 初始化模型 $F_0(\\mathbf{x}) = \\arg\\min_{\\rho} \\sum_{i=1}^N L(y_i, \\rho)$ \\For{$m = 1$ 到 $M$} \\State 计算伪残差：$\\tilde y_i = -\\left[\\frac{\\partial L(y_i, F(\\mathbf{x}_i))}{\\partial F(\\mathbf{x}_i)}\\right]_{F=F_{m-1}}$ \\State 拟合一个基学习器 $h_m(\\mathbf{x};\\mathbf a_m)$ 来拟合伪残差 $\\tilde y_i:\\mathbf a_m=\\arg\\min_{\\mathbf a}\\sum_{i=1}^N[-\\mathbf g_m(\\mathbf x_i)-\\alpha h(\\mathbf x_i;\\mathbf a)]^2$ \\State 计算最佳步长：$\\beta_m=\\arg\\min_{\\beta}\\sum_{i=1}^NL(y_i,F_{m-1}(\\mathbf x_i+\\beta h(\\mathbf x_i;\\mathbf a_m)))$ \\State 更新模型：$F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\beta_m h_m(\\mathbf{x};\\mathbf a_m)$ \\EndFor \\State \\Return 最终模型 $F_M(\\mathbf{x})$ \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "posts/GBDT模型/GBDT.html#参数估计",
    "href": "posts/GBDT模型/GBDT.html#参数估计",
    "title": "GBDT模型",
    "section": "1.3 参数估计",
    "text": "1.3 参数估计\n假设有一个加法模型：\n\nF(\\mathbf x;\\{\\beta_m,\\mathbf a_m\\}_1^M)=\\sum_{i=1}^M\\beta_mh(\\mathbf x;\\mathbf a_m)\n\nh(\\mathbf x;\\mathbf a_m) 是一个参数比较简单的基函数；\\mathbf a=\\{a_1,a_2,\\ldots\\}"
  },
  {
    "objectID": "posts/[算法导论]S2.1/index.html",
    "href": "posts/[算法导论]S2.1/index.html",
    "title": "S1.1 插入排序",
    "section": "",
    "text": "后续的算法设计于分析都是在本章框架下进行的。"
  },
  {
    "objectID": "posts/[算法导论]S2.1/index.html#约定",
    "href": "posts/[算法导论]S2.1/index.html#约定",
    "title": "S1.1 插入排序",
    "section": "3.1 约定",
    "text": "3.1 约定\n\n//：表示注释\n缩进表示块结构\n数组元素通过数组名[下标]的形式进行访问，A[1..j]表示 \\(A[1]-A[j]\\)\n复合对象被组织成对象，其又有属性构成，即A.length：表示数组A的长度"
  },
  {
    "objectID": "posts/[算法导论]S2.1/index.html#语法latex",
    "href": "posts/[算法导论]S2.1/index.html#语法latex",
    "title": "S1.1 插入排序",
    "section": "3.2 语法(Latex)",
    "text": "3.2 语法(Latex)\n\n待整理"
  },
  {
    "objectID": "posts/[C++]S1/index.html",
    "href": "posts/[C++]S1/index.html",
    "title": "S1.C++简介",
    "section": "",
    "text": "C++是一种静态类型的中级语言，这意味这它是在编译时执行类型检查而不是在运行时执行类型检查。\n\n1 面向对象程序设计\nC++ 完全支持面向对象的程序设计，包括面向对象开发的四大特性：\n\n封装（Encapsulation）：封装是将数据和方法组合在一起，对外部隐藏实现细节，只公开对外提供的接口。这样可以提高安全性、可靠性和灵活性。\n继承（Inheritance）：继承是从已有类中派生出新类，新类具有已有类的属性和方法，并且可以扩展或修改这些属性和方法。这样可以提高代码的复用性和可扩展性。\n多态（Polymorphism）：多态是指同一种操作作用于不同的对象，可以有不同的解释和实现。它可以通过接口或继承实现，可以提高代码的灵活性和可读性。\n抽象（Abstraction）：抽象是从具体的实例中提取共同的特征，形成抽象类或接口，以便于代码的复用和扩展。抽象类和接口可以让程序员专注于高层次的设计和业务逻辑，而不必关注底层的实现细节。\n\n\n\n2 标准库\n标准的 C++ 由三个重要部分组成：\n\n核心语言，提供了所有构件块，包括变量、数据类型和常量，等等。\nC++ 标准库，提供了大量的函数，用于操作文件、字符串等。\n标准模板库（STL），提供了大量的方法，用于操作数据结构等。"
  },
  {
    "objectID": "posts/[C++]S2/index.html",
    "href": "posts/[C++]S2/index.html",
    "title": "S2.C++基本语法",
    "section": "",
    "text": "C++程序可以定义为对象的集合，类、对象、方法、即时变量的定义如下：\n\n对象： 类的实例化，例如具体的人张三、李四。\n类：描述对象虚报范围/状态的模板，例如人这个概念。\n方法：类中的一种行为，例如人跑步。\n即时变量： 每个对象都有其独特的即时变量。对象的状态是由这些即时变量的值创建的。（属性？）\n\n\n1 C++程序结构\n#include &lt;iostream&gt;  // &lt;1&gt;\nusing namespace std;  // &lt;2&gt;\n\nint main(){  // &lt;3&gt;\n    cout &lt;&lt; \"Hello World\";\n    return 0;\n}\n\nC++语言定义的头文件，包含了程序中必须的或有用的信息\n告诉编译器使用std命名空间\nint main()是主函数，程序从这里开始"
  },
  {
    "objectID": "posts/[算法导论]S2.2/index.html",
    "href": "posts/[算法导论]S2.2/index.html",
    "title": "S1.2 算法分析",
    "section": "",
    "text": "分析算法主要度量的是计算时间，在算法分析中假定一种通用的单处理计算模型-随机访问器(random-access machine, RAM)来作为实现的技术，算法可以通过计算机程序来实现。在RAM模型中，指令一条接一条地执行，没有并发操作。在设计算法是应当采取真实计算机时如何设计的，RAM就如何设计的准则，即RAM模型包括计算机中的常见指令："
  },
  {
    "objectID": "posts/[算法导论]S2.2/index.html#最好情况",
    "href": "posts/[算法导论]S2.2/index.html#最好情况",
    "title": "S1.2 算法分析",
    "section": "1.1 最好情况",
    "text": "1.1 最好情况\n当循环到第 \\(i\\) 个元素时，最好情况就是其前元素小于它，则无需进入while循环直接来到第 \\(i+1\\) 个元素，也即是有如下总运行时间：\n\\[\n\\begin{aligned}\nT(n)&= c_1 n+(c_2+c_4)(n-1)+c_5(n-1)+c_8(n-1)\\\\\n&=(c_1+c_2+c_4+c_5+c_8)n-(c_2+c_4+c_5+c_8)\n\\end{aligned}\n\\]\n也即最好情况的运行时间是 \\(n\\) 的线性函数。"
  },
  {
    "objectID": "posts/[算法导论]S2.2/index.html#最坏的情况",
    "href": "posts/[算法导论]S2.2/index.html#最坏的情况",
    "title": "S1.2 算法分析",
    "section": "1.2 最坏的情况",
    "text": "1.2 最坏的情况\n最坏的情况是值每循环到第 \\(i\\) 个元素时，其前子序列均大于它，它需要插入到1的位置，则有 \\(\\sum_{j=2}^nt_j=2+3+\\ldots+n=\\frac{(n-1)(n+2)}{2}\\)（第2个元素只需要交换一次但是 \\(t_j\\) 包括while的判断），则总运行时间为：\n\\[\n\\begin{aligned}\nT(n)&=c_1n+c_2(n-1)+c_4(n-1)+c_5(\\frac{n(n+1)}{2}-1)+c_6(\\frac{n(n-1)}{2})\\\\\n&+c_7(\\frac{n(n-1)}{2})+c_8(n-1)\\\\\n&=a_1n^2+a_2n+a_3\n\\end{aligned}\n\\]\n即最坏情况的运行时间是 \\(n\\) 的二次函数。"
  },
  {
    "objectID": "posts/[王推]S1/index.html",
    "href": "posts/[王推]S1/index.html",
    "title": "S1.推荐系统简介",
    "section": "",
    "text": "点击率:=点击次数/曝光次数\n点赞率:=点赞次数/点击次数\n收藏率:=收藏次数/点击次数\n转发率:=转发次数/点击次数\n阅读完成率:=滑动到底次数/点击次数 \\(\\times\\) f(笔记长度)\n\n\n北极星指标\n\n\n\n\n\n\n\n用户规模\n消费\n发布\n\n\n\n\n\n日活用户数(DAU)\n月活用户数(MAU)\n\n\n人均使用推荐的时长\n人均阅读笔记数量\n\n\n发布渗透率\n人均发布量"
  },
  {
    "objectID": "posts/[王推]S1/index.html#粗排精排",
    "href": "posts/[王推]S1/index.html#粗排精排",
    "title": "S1.推荐系统简介",
    "section": "3.1 粗排、精排",
    "text": "3.1 粗排、精排\n\n\n\n\n\nflowchart LR\n    a_1[用户特征] --&gt; b{神经网络}\n    a_2[物品特征] --&gt; b\n    a_3[统计特征] --&gt; b\n    b --&gt; |预测| d[点击率]\n    d --&gt; 排序分数"
  },
  {
    "objectID": "posts/[王推]S1/index.html#重排",
    "href": "posts/[王推]S1/index.html#重排",
    "title": "S1.推荐系统简介",
    "section": "3.2 重排",
    "text": "3.2 重排\n\n做多样性抽样(MMR,DDP)从几百篇中选出几十篇\n用规则打散相似笔记\n插入广告、运营推广内容并根据生产要求调整排序"
  },
  {
    "objectID": "posts/[王推]S2/index.html",
    "href": "posts/[王推]S2/index.html",
    "title": "S2.A/B测试",
    "section": "",
    "text": "召回实验实现了一种GNN召回通道，离线实验结果正向，下一步做线上小流量A/B测试，考察新的召回通道对线上指标的影响。模型中有一些参数，比如GNN的深度取值 \\({1,2,3}\\) 需要用A/B测试选取最优参数。"
  },
  {
    "objectID": "posts/[王推]S2/index.html#正交",
    "href": "posts/[王推]S2/index.html#正交",
    "title": "S2.A/B测试",
    "section": "2.1 正交",
    "text": "2.1 正交\n如果所有实验都正交，则可以同时做无数组实验。\n\n同类的策略天然互斥，且它们的效果会相互增强1+1&gt;2或相互抵消(1+1&lt;2)\n不同类型的策略通常不互相干扰1+1=2可作为互斥的两层"
  },
  {
    "objectID": "posts/[深习]线性回归/index.html",
    "href": "posts/[深习]线性回归/index.html",
    "title": "机器学习的简单实例：线性回归",
    "section": "",
    "text": "\\[\nf(x;w,b)=w^Tx+b\n\\]\n上式中的权重向量 \\(w\\in\\mathbb{R}^d\\) 和偏置 \\(b\\in \\mathbb{R}\\)，是可学习的参数，其可写成如下形式：\n\\[\nf(x;\\tilde{w})=\\tilde{w}^T\\cdot\\tilde{x}\n\\]"
  },
  {
    "objectID": "posts/[深习]线性回归/index.html#经验风险最小化最小二乘法",
    "href": "posts/[深习]线性回归/index.html#经验风险最小化最小二乘法",
    "title": "机器学习的简单实例：线性回归",
    "section": "2.1 经验风险最小化（最小二乘法）",
    "text": "2.1 经验风险最小化（最小二乘法）\n损失函数采用平方损失函数，按照经验风险最小化准则，训练集上的经验风险定义为：\n\n\n\n最小二乘计算过程\n\n\n\n\n\n\n\n\n最小二乘法得出的最优参数的限制\n\n\n\n由最优参数 \\(w^*\\) 的形式可得：要求 \\(XX^T\\) 满秩。当 \\(XX^T\\) 不可逆时，可采用下面两种方法来估计参数：\n\n先使用主成分分析等方法来预处理数据，消除不同特征之间的相关性，然后再使用最小二乘法来估计参数（减少特征的维数）\n通过梯度下降法来估计参数，先初始化 \\(w=0\\)，然后通过下面公式进行迭代：\n\n\\[\nw\\leftarrow w+\\alpha X(y-X^Tw)\n\\]\n\n\n\n# 生成模拟数据\nimport numpy as np\n# 设置随机种子，确保结果可重复\nnp.random.seed(42)\n# 参数设置\nnum_points = 100  # 数据点数量\na = 2  # 斜率\nb = 5  # 截距\nsigma = 3  # 噪声的标准差\n# 生成X值（随机或均匀分布）\nX = np.linspace(1, 10, num_points)  # 在1到10之间均匀生成num_points个点\nepsilon = np.random.normal(0, sigma, num_points)  # 生成正态分布噪声\nY = a * X + b + epsilon\nY = Y.reshape(-1, 1)\n\n# 利用经验风险最小化计算权重\nX_t = np.vstack((X, [1 for _ in range(num_points)]))\nw = np.linalg.inv(X_t @ X_t.T) @ X_t @ Y\nprint(f\"经验风险最小化后的权重系数：\\n{w}\")\n\n经验风险最小化后的权重系数：\n[[2.04597756]\n [4.43558388]]\n\n\n当采用小批量随机梯度下降法来估计参数，结果如下：\n\nimport tensorflow as tf\ntf.random.set_seed(42)\n# 1. 定义模型\nclass LinearRegressionModel:\n1    def __init__(self):\n        # 初始化权重\n2        self.weights = tf.Variable(initial_value=tf.random.normal([2, 1]), name=\"权重\", dtype=tf.float32)\n    def __call__(self, x):\n3        return tf.matmul(x, self.weights)\n# 2. 定义损失函数（均方误差）\ndef mean_squared_error(y_true, y_pred):\n4    return tf.reduce_mean(tf.square(y_true - y_pred))\n# 3. 小批量随机梯度下降算法\ndef train(model, X, y, learning_rate=0.01, batch_size=10, epochs=100):\n    # 创建优化器\n    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n    # 划分小批量\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    dataset = dataset.shuffle(buffer_size=X.shape[0]).batch(batch_size)\n\n    for epoch in range(epochs):\n        for batch_X, batch_y in dataset:\n            with tf.GradientTape() as tape:\n                # 计算预测值\n                y_pred = model(batch_X)\n                # 计算损失\n                loss = mean_squared_error(batch_y, y_pred)\n             # 计算梯度\n            gradients = tape.gradient(loss, model.weights)\n            # 更新参数\n            optimizer.apply_gradients([(gradients, model.weights)])\n    return model.weights.numpy()\n# 训练模型\nmodel = LinearRegressionModel()\nweights = train(model, tf.constant(X_t.T, dtype=tf.float32), tf.constant(Y, dtype=tf.float32), learning_rate=0.01, batch_size=10, epochs=1000)\nprint(f\"采用小批量随机梯度下降法训练后的权重: \\n{weights}\")\n\n\n1\n\n初始化模型参数\n\n2\n\ntf.random.normal([2, 1])中的[2,1]指的是参数的维度\n\n3\n\n构造模型的形式\n\n4\n\n构造损失函数\n\n\n\n\n采用小批量随机梯度下降法训练后的权重: \n[[2.0707269]\n [4.4438233]]\n\n\n\n\n\n\n\n\n\nclassDiagram\n    class 构建模型{\n        +初始化模型参数(__init__)\n        +表示模型(__call__)\n    }\n    class 学习准则{\n        +函数名(y_true, y_pred)\n    }\n    class 优化算法{\n        +定义优化算法(train)\n    }\n构建模型 --&gt; 学习准则\n学习准则 --&gt; 优化算法\n\n\n 利用tensorflow实现机器学习模型的步骤图 \n\n\n\n\n\n\n\n\n\n\n结果对比图\n\n\n\n\n由上图结果可得，两种算法的效率类似，且都在靠近0的部分有较大偏离。"
  },
  {
    "objectID": "posts/[深习]线性回归/index.html#结构风险最小化岭回归",
    "href": "posts/[深习]线性回归/index.html#结构风险最小化岭回归",
    "title": "机器学习的简单实例：线性回归",
    "section": "2.2 结构风险最小化（岭回归）",
    "text": "2.2 结构风险最小化（岭回归）\n结构风险最小化等价于岭回归，可以用解决最小二乘回归法对 \\(XX'\\) 满秩的要求。岭回归的想法是在 \\(XX'\\) 中加入 \\(\\lambda I\\) 一项使得 \\(XX'+\\lambda I\\) 满秩，而结构风险最小化是追求下式目标函数最小( \\(\\lambda&gt;0\\) 为正则化系数：\n\\[\nR(w)=\\frac 1 2||y-X^Tw||^2+\\frac 12 \\lambda ||w||^2\n\\]\n由下图可得二者结果等价。\n\n\n\n结构风险最小化等价于岭回归"
  },
  {
    "objectID": "posts/[深习]线性回归/index.html#最大似然估计",
    "href": "posts/[深习]线性回归/index.html#最大似然估计",
    "title": "机器学习的简单实例：线性回归",
    "section": "2.3 最大似然估计",
    "text": "2.3 最大似然估计\n\n\n\n\n\n\n机器学习任务的分类\n\n\n\n\n样本的特征向量 \\(x\\) 和标签 \\(y\\) 之间存在未知的函数关系 \\(y=h(x)\\)\n条件概率 \\(p(y|x)\\) 服从某个未知分布\n\n\n\n之前介绍的最小二乘法是属于第一类，现在从建模条件概率 \\(p(y|x)\\) 的角度进行参数估计。\n假设 \\(y\\sim N(\\tilde{w}^Tx, \\sigma^2)\\)，参数 \\(\\tilde{w}\\) 在训练集 \\(\\mathcal{D}\\) 上的似然函数为：\n\\[\n\\begin{aligned}\np(y|\\tilde{X};w,\\sigma)&= \\prod_{n=1}^Np(y^{(n)}|x^{(n)};\\tilde{w},\\sigma)\\\\\n&=\\prod_{n=1}^N N(\\tilde{w}^Tx^{(n)},\\sigma^2)\n\\end{aligned}\n\\]\n由最大似然估计原理可得如下图：\n\n\n\n最大似然估计的结果\n\n\n易得最大似然估计的解和最小二乘法的解相同。"
  },
  {
    "objectID": "posts/[深习]线性回归/index.html#最大后验估计",
    "href": "posts/[深习]线性回归/index.html#最大后验估计",
    "title": "机器学习的简单实例：线性回归",
    "section": "2.4 最大后验估计",
    "text": "2.4 最大后验估计\n假设参数 \\(\\tilde{w}\\) 为一个随机向量，并服从一个先验分布 \\(p(\\tilde{w};v)=N(\\tilde{w};0,v^2I)\\)， \\(v^2\\) 为每一维上的方差。\n由贝叶斯估计原理得如下图：\n\n\n\n最大后验估计\n\n\n1#include &lt;iostream&gt;\n\n1\n\n急急急"
  },
  {
    "objectID": "posts/[Cpp]存储类/index.html",
    "href": "posts/[Cpp]存储类/index.html",
    "title": "C++存储类",
    "section": "",
    "text": "存储类定义 C++ 程序中变量/函数的范围（可见性）和生命周期。\n\n1 auto存储类\nauto 关键字用于两种情况:\n\n声明变量时根据初始化表达式自动推断该变量的类型\n声明函数时函数返回值的占位符\n\nauto f=3.14;  // double\nauto s(\"hello\");  // const char*\n1auto s{\"hello\"};\n2auto z = new auto(9);\n// int*\nauto x1 = 5, x2 = 5.0, x3='r';  //错误，必须是初始化为同一类型\n\n1\n\nC++可用变量名(字符串)/变量名{字符串}创建字符变量\n\n2\n\n首先new auto(9)解析创建一个int类型的对象，并用9进行初始化，并用z作为int*类型的指针指向这个对象\n\n\n\n\n2 static存储类\nstatic 存储类指示编译器在程序的生命周期内保持局部变量的存在，而不需要在每次它进入和离开作用域时进行创建和销毁。\n\n作用于局部变量 因此，使用 static 修饰局部变量可以在函数调用之间保持局部变量的值。\n作用于全局变量 static 修饰符也可以应用于全局变量。当 static 修饰全局变量时，会使变量的作用域限制在声明它的文件内。\n作用于类数据成员 在 C++ 中，当 static 用在类数据成员上时，会导致仅有一个该成员的副本被类的所有对象共享。\n\n#include &lt;iostream&gt;\n// 函数声明 \nvoid func(void);\n \nstatic int count = 10; /* 全局变量 */\n \nint main()\n{\n2    while(count--)\n    {\n       func();\n    }\n    return 0;\n}\n// 函数定义\nvoid func( void )\n{\n1    static int i = 5;\n    i++;\n    std::cout &lt;&lt; \"变量 i 为 \" &lt;&lt; i ;\n    std::cout &lt;&lt; \" , 变量 count 为 \" &lt;&lt; count &lt;&lt; std::endl;\n}\n\n1\n\n正常而言i在每次迭代中都会重新赋值于5，但加上static属性后该语句不会再执行，且其值会在函数调用后保持\n\n2\n\n此while循环中的count–即是判断条件其先将count目前返回判断是否为true，而后–，故会执行10次循环\n\n\n\n\n3 extern存储类\nextern 存储类用于提供一个全局变量的引用，全局变量对所有的程序文件都是可见的。即利用extern关键字修饰变量时可不进行初始化或声明从其他文件中获取，而不需再此处声明。\n// main.cpp\n#include &lt;iostream&gt;\n \nint count ;\nextern void write_extern();\n \nint main()\n{\n   count = 5;\n   write_extern();\n}\n// support.cpp\n#include &lt;iostream&gt;\n \nextern int count;\n \nvoid write_extern(void)\n{\n   std::cout &lt;&lt; \"Count is \" &lt;&lt; count &lt;&lt; std::endl;\n}\n\n\n\n\n\n\n使用extern关键字编译注意事项\n\n\n\n使用g++ main.cpp support.cpp -o write进行编译并键入write来运行\n结果为Count is 5\n\n\n\n\n4 mutable存储类\nmutable 是一个关键字，用于修饰类的成员变量，使其能够在 const 成员函数中被修改。通常情况下，const 成员函数不能修改对象的状态，但如果某个成员变量被声明为 mutable，则可以在 const 函数中对其进行修改，其特点如下：\n\n允许修改：mutable 成员变量可以在 const 成员函数内被改变。\n设计目的：通常用于需要在不改变对象外部状态的情况下进行状态管理的场景，比如缓存、延迟计算等。\n\n\n\n5 杂项运算符\nsizeof(a);  // 将返回变量的大小\nx = a&gt;2 ? 1:2;  // 若a&gt;2则x的值为1反之为2\n&a;  // 返回变量a的地址\n*a;  // 指向变量\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[Cpp]循环/index.html",
    "href": "posts/[Cpp]循环/index.html",
    "title": "C++循环",
    "section": "",
    "text": "循环的类型由以下四种："
  },
  {
    "objectID": "posts/[Cpp]循环/index.html#break语句",
    "href": "posts/[Cpp]循环/index.html#break语句",
    "title": "C++循环",
    "section": "4.1 break语句",
    "text": "4.1 break语句\nC++ 中 break 语句有以下两种用法：\n\n当 break 语句出现在一个循环内时，循环会立即终止，且程序流将继续执行紧接着循环的下一条语句。\n它可用于终止 switch 语句中的一个 case。 如果您使用的是嵌套循环（即一个循环内嵌套另一个循环），break 语句会停止执行最内层的循环，然后开始执行该块之后的下一行代码。"
  },
  {
    "objectID": "posts/[Cpp]循环/index.html#continue语句",
    "href": "posts/[Cpp]循环/index.html#continue语句",
    "title": "C++循环",
    "section": "4.2 continue语句",
    "text": "4.2 continue语句\n\nC++ 中的 continue 语句有点像 break 语句。但它不是强迫终止，continue 会跳过当前循环中的代码，强迫开始下一次循环。\n对于 for 循环，continue 语句会导致执行循环增量和条件测试部分。对于 while 和 do…while 循环，continue 语句会导致程序控制回到条件测试上。"
  },
  {
    "objectID": "posts/[Cpp]循环/index.html#goto语句",
    "href": "posts/[Cpp]循环/index.html#goto语句",
    "title": "C++循环",
    "section": "4.3 goto语句",
    "text": "4.3 goto语句\n将控制转移到被标记的语句。但是不建议在程序中使用 goto 语句。goto 语句一个很好的作用是退出深嵌套例程。\nfor(...) {\n   for(...) {\n      while(...) {\n         if(...) goto stop;\n         .\n         .\n         .\n      }\n   }\n}\nstop:\ncout &lt;&lt; \"Error in program.\\n\";"
  },
  {
    "objectID": "posts/[算法导论]S2.3/index.html",
    "href": "posts/[算法导论]S2.3/index.html",
    "title": "S1.3 设计算法",
    "section": "",
    "text": "插入排序使用了增量方法，在本文章中考虑一种分治法。"
  },
  {
    "objectID": "posts/[算法导论]S2.3/index.html#合并",
    "href": "posts/[算法导论]S2.3/index.html#合并",
    "title": "S1.3 设计算法",
    "section": "2.1 合并",
    "text": "2.1 合并\n\n伪代码pythonC++\n\n\n\n\n\\begin{algorithm} \\caption{合并(A,p,q,r)} \\begin{algorithmic} \\State $n_1=q-p-1$ \\State $n_2=r-q$ \\State 声明两个数组L[1..$n_1$+1]和R[1..$n_2$+1] \\For{i=1\\To $n_1$} \\State L[i]=A[p+i-1]\\EndFor \\For{j=1\\To $n_2$} \\State R[j]=A[q+j]\\EndFor \\State L[$n_1+1$]=$\\infty$ \\State R[$n_2+1$]=$\\infty$ \\State i=1,j=1 \\State \\For{k=p\\To r} \\State \\If{L[i]$\\leq$ R[j]} \\State A[k]=L[i] \\State i=i+1 \\Else \\State A[k]=R[j] \\State j=j+1\\EndIf\\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\ndef merge(A: list, p: int, q: int, r: int) -&gt; list:\n    \"\"\"\n    A: 待排序的列表\n\n    p：从何处开始进行排序(左子序列的首部)\n    \n    q：两子序列分割点(右子序列的首部)\n    \n    r：待排序的两个子序列和原先列表最后一个元素的索引\n    \"\"\"\n    L = A[p:q]; R = A[q:r+1]  # 切分为两个数组\n    L.append(float(\"inf\")); R.append(float(\"inf\"))  # 添加哨兵值\n    i = 0; j = 0  # 初始化两子序列指针\n    for k in range(p, r+1):\n        if L[i] &lt;= R[j]:\n            A[k] = L[i]\n            i = i + 1\n        else:\n            A[k] = R[j]\n            j = j + 1\n    return A\n\nA = [1, 2, 3, 4, 6, 8, 5, 7, 9]\nA = merge(A, 3, 6, 8)\nprint(f\"排序后：{A}\")\n\n排序后：[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\n\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;limits&gt;\n\nusing namespace std;\n\nvoid merge(vector&lt;int&gt;& A, int p, int q, int r){\n    int n_1 = q-p, n_2 = r+1-q;\n    vector&lt;int&gt; L(n_1), R(n_2);\n    for (int i = 0; i&lt;n_1; i++){\n        L[i] = A[p+i];\n    }\n    for (int i = 0; i&lt;n_2; i++){\n        R[i] = A[q+i];\n    }\n    // 添加哨兵值\n    L.push_back(numeric_limits&lt;int&gt;::max());\n    R.push_back(numeric_limits&lt;int&gt;::max());\n    int i = 0, j = 0;\n    for (int k = p; k &lt; r+1; k++){\n        if(L[i]&lt;=R[j]){\n            A[k] = L[i];\n            i++;\n        }\n        else{\n            A[k] = R[j];\n            j++;\n        }\n    }\n}\n\nint main(){\n    vector&lt;int&gt; A = {1, 2, 3, 4, 6, 8, 5, 7, 9};\n    merge(A, 3, 6, A.size()-1);\n    cout &lt;&lt; \"排序后：\";\n    for (int i = 0; i&lt;A.size(); i++){\n        cout &lt;&lt; A[i] &lt;&lt; \" \";\n    }\n    return 0;\n}\n// 排序后：1 2 3 4 5 6 7 8 9 \n\n\n\n其运行过程如下：\n\n\n该合并算法需要 \\(\\Theta(n)\\) 的时间，每次基本操作是常量时间，而最多执行n个基本步骤。或者可以观察其伪代码，发现就只有一个循环需要运行 \\(\\Theta(n)\\) 时间，而循环内的操作都是需要常量时间。 (\\(n=r-p\\))\n\n\n\n\n\n\n合并算法正确性\n\n\n\n循环不变式：在k次迭代时，子数组A[p..k-1]按从小到大的顺序包含L[1..\\(n_1+1\\)]和R[1..\\(n_2+1\\)]中的 \\(k-p\\) 个最小元素。\n\n初始化：易得在第一次迭代前循环不变式成立（平凡）。\n保持：假设第k次循环 L[i]&lt; R[j]，那么此时 A[p,k-1]中将会加入L[i]，在下次循环前A[p,k]中包含的就是L[1..\\(n_1+1\\)]和R[1..\\(n_2+1\\)]中的 \\(k+1-p\\) 个最小元素。故保持了循环不变式的成立\n终止：当k=r+1时循环终止，A[p..k-1]包含的元素就是L[1..\\(n_1+1\\)]和R[1..\\(n_2+1\\)]中的 \\(k-p\\) 个最小元素，且L,R仅剩下哨兵值。\n\n综上算法成立。"
  },
  {
    "objectID": "posts/GBDT模型/index.html",
    "href": "posts/GBDT模型/index.html",
    "title": "GBDT模型",
    "section": "",
    "text": "GBDT (Gradient Boosting Decision Tree), 梯度提升树。它是一种基于决策树的集成算法。\n通过构造一组弱的学习（树），并把多颗决策树的结果累加起来作为最终的预测输出。该算法将决策树与集成思想进行了有效结合。"
  },
  {
    "objectID": "posts/GBDT模型/index.html#gbdt的原理",
    "href": "posts/GBDT模型/index.html#gbdt的原理",
    "title": "GBDT模型",
    "section": "1.1 GBDT的原理",
    "text": "1.1 GBDT的原理\n\n所有弱分类器的结构相加等于预测值\n每次都以当前预测值为基准，下一个弱分类器去拟合误差函数对预测值的误差\nGBDT 的弱分类器使用的是树模型"
  },
  {
    "objectID": "posts/GBDT模型/index.html#sec-前向分布算法",
    "href": "posts/GBDT模型/index.html#sec-前向分布算法",
    "title": "GBDT模型",
    "section": "1.2 前向分布算法",
    "text": "1.2 前向分布算法\n许多加法模型都有如下形式\n\\[\nf(x)=\\sum_{i=1}^M\\beta_m b(x,\\gamma_m)\n\\]\n\\(\\beta_m\\) 是系数, \\(b(x,\\gamma_m)\\) 是基函数并带有一组参数 \\(\\gamma_m\\)。 这类模型的参数的求解大都采用极小化如下损失函数：\n\\[\n\\sum_{i=1}^nL(y_i,\\sum_{m=1}^M\\beta_mb(x,\\gamma_m))\n\\]\n但是当样本足够多，问题较为复杂时，如果直接求解，则要估计的参数将会有 \\(M+\\sum_{m=1}^M\\dim(\\gamma_m)\\) 个，求解十分耗费算力，于是前向分布算法(Forward stagewise additive modeling)被提出了。 前向分布算法将原来的问题转换为——一步一步的估计基函数项，当在估计某一基函数项 \\(\\beta_mb(x,\\gamma_m)\\) 时，其之前的 \\(\\beta_ib(x,\\gamma_i),i=1,2,\\ldots,m-1\\) 将作为定值，不再被改变，故该算法的第m步即是取解决如下下问题：\n\\[\n\\begin{aligned}\n\\min& \\sum_{i=1}^nL(y_i,f_m(x))\\\\\nf_m(x)&=f_{m-1}(x)+\\beta_mb(x,\\gamma_m)\n\\end{aligned}\n\\]\n其中 \\(f_{m-1}(x)\\) 已知。 当我们假定采用平方损失函数时，我们有：\n\\[\n\\begin{aligned}\nLoss&=\\sum_{i=1}^n\\frac 12[y_i-(f_{m-1}(x)+\\beta_mb(x,\\gamma_m))]^2\\\\\n&=\\sum_{i=1}^n\\frac 12[(y_i-f_{m-1}(x))-\\beta_mb(x,\\gamma_m)]^2\\\\\n&=\\sum_{i=1}^n\\frac 12(r_{mi}-\\beta_mb(x,\\gamma_m))^2\n\\end{aligned}\n\\]\n\\(r_{mi}=y_i-f_{m-1}(x_i)\\) 是在第m-1 步拟合后模型的残差，从损失函数得第m步的本质在于利用第m个基函数对上一步所得残差进行拟合，但这种理解是建立在损失函数为平方损失的情况下。\n\n前向的意义：“前向”指的是逐步进行的过程，即算法是从零开始，逐步向解的方向构建和优化。每一步添加一个新的基函数或模型参数，以逐渐逼近最终的目标。与之相对的概念是“后向”（Backward），后向算法通常从一个复杂的模型开始，然后逐步移除不必要的成分，而前向算法则是从简单的模型开始逐步加成。\n分布的含义：“分布”或”阶段性”（Stagewise）意味着算法在每一步迭代时，只增加一个新的基函数或参数，而不会对之前的基函数或参数进行重新调整。也就是说，每次迭代仅调整新增的模型成分，已添加的部分在整个过程中保持不变。这种特性使得算法较为保守，每次变化较小，且有利于控制模型的复杂度和避免过拟合。"
  },
  {
    "objectID": "posts/GBDT模型/index.html#数值优化",
    "href": "posts/GBDT模型/index.html#数值优化",
    "title": "GBDT模型",
    "section": "2.1 数值优化",
    "text": "2.1 数值优化\n需要估计的参数为：\n\\[\n\\mathbf P^*=\\arg\\min_{\\mathbf P}\\Phi(\\mathbf P)\n\\]\n\\[\n\\Phi(\\mathbf P)=E_{y,\\mathbf x}L(y,F(\\mathbf x;\\mathbf P))\n\\]\n最优的模型为：\n\\[\nF^*(\\mathbf x)=F(\\mathbf x;\\mathbf P^*)\n\\]\n参数的组成：\n\\[\n\\mathbf P^*=\\sum_{m=0}^M\\mathbf p_m\n\\]\n对于参数构成的理解：即所有参数的列向量，每一次拟合模型都相当于往 \\(\\mathbf P\\) 的某些列上加值/更新值。"
  },
  {
    "objectID": "posts/GBDT模型/index.html#最速下降法steepest-descent",
    "href": "posts/GBDT模型/index.html#最速下降法steepest-descent",
    "title": "GBDT模型",
    "section": "2.2 最速下降法(Steepest-descent)",
    "text": "2.2 最速下降法(Steepest-descent)\n由前面讨论可得，有如下优化问题：\n\\[\n\\min_{\\mathbf P}\\Phi(\\mathbf P)\n\\]\n则由梯度下降法得到的梯度为：\n\\[\n\\mathbf g_m=\\nabla\\Phi(\\mathbf P)|_{\\mathbf P=\\mathbf P_{m-1}}=[\\frac{\\partial\\Phi(\\mathbf P)}{\\partial P_j}|_{\\mathbf P =\\mathbf P_{m-1} }]^T\n\\]\n由于 \\(\\mathbf P_{m-1}=\\sum_{i=0}^{m-1}\\mathbf p_i\\), 则得到如下迭代公式：\n\\[\n\\mathbf P_m=\\mathbf P_{m-1}+\\mathbf p_m\n\\]\n而由最速下降法得:\n\\[\n\\mathbf p_m=-\\rho_m\\mathbf g_m\n\\]\n由一维线搜索得步长 \\(\\rho_m\\) 有下式决定： 由前面讨论可得，有如下优化问题：\n\\[\n\\min_{\\mathbf P}\\Phi(\\mathbf P)\n\\]\n则由梯度下降法得到的梯度为：\n\\[\n\\mathbf g_m=\\nabla\\Phi(\\mathbf P)|_{\\mathbf P=\\mathbf P_{m-1}}=[\\frac{\\partial\\Phi(\\mathbf P)}{\\partial P_j}|_{\\mathbf P =\\mathbf P_{m-1} }]^T\n\\]\n由于 \\(\\mathbf P_{m-1}=\\sum_{i=0}^{m-1}\\mathbf p_i\\), 则得到如下迭代公式：\n\\[\n\\mathbf P_m=\\mathbf P_{m-1}+\\mathbf p_m\n\\]\n而由最速下降法得:\n\\[\n\\mathbf p_m=-\\rho_m\\mathbf g_m\n\\]\n由一维线搜索得步长 \\(\\rho_m\\) 有下式决定：\n\\[\n\\rho_m=\\arg\\min_\\rho\\Phi(\\mathbf P_{m-1}-\\rho\\mathbf g_m)\n\\]"
  },
  {
    "objectID": "posts/GBDT模型/index.html#函数空间上的数值优化",
    "href": "posts/GBDT模型/index.html#函数空间上的数值优化",
    "title": "GBDT模型",
    "section": "2.3 函数空间上的数值优化",
    "text": "2.3 函数空间上的数值优化\n假定某函数由多个基函数相加而成，即加法模型：\n\\[\nF^*(x)=\\sum_{m=0}^Mf_m(x)\n\\]\n有如下优化问题：\n\\[\n\\min\\Phi(F(x))=E_y[L(y,F(x))|x]\n\\]\n\\(F_m(x)\\) 是迭代的类似于 \\(x_k=x_{k-1}+\\alpha_kd_k\\) 则由最速下降法得：\n\\[\nf_m(\\mathbf x)=-\\rho_m\\mathbf g_m(\\mathbf x)\n\\]\n\\[\n\\mathbf g_m(\\mathbf x)=E_y[\\frac{\\partial L(y,F(\\mathbf x))}{\\partial F(\\mathbf x)}|\\mathbf x]_{\\mathbf F(x)=\\mathbf F_{m-1}(\\mathbf x)}\n\\]\n步长 \\(\\rho_m\\) 由下列精确一维线搜索决定：\n\\[\n\\rho_m = \\arg\\min_\\rho E_{y,\\mathbf x}L(y,F_{m-1}(\\mathbf x)-\\rho\\mathbf g_m(\\mathbf x))\n\\]"
  },
  {
    "objectID": "posts/GBDT模型/index.html#有限维数据梯度提升算法",
    "href": "posts/GBDT模型/index.html#有限维数据梯度提升算法",
    "title": "GBDT模型",
    "section": "2.4 有限维数据——梯度提升算法",
    "text": "2.4 有限维数据——梯度提升算法\n从参数的角度而言，已知样本数据 \\(\\{y_i,\\mathbf x_i\\}\\)，模型为加法模型，则有如下参数优化问题：\n\\[\n\\{\\beta_m^*,\\mathbf a_m^*\\}_1^M=\\arg\\min_{\\{\\beta_m,\\mathbf a_m\\}_1^M}\\sum_{i=1}^NL(y_i,\\sum_{m=1}^M\\beta_mh(\\mathbf x_i;\\mathbf a_m))\n\\tag{1}\\]\n要直接求解上述参数优化问题将会比较棘手，因为需要在一个优化问题中求解 \\(M\\cdot(1+\\dim (\\mathbf a_m))\\) 个参数，参数个数庞大，我们考虑使用 Section 1.2 中的前向分布算法，其将一个参数优化问题转换为一个函数优化问题。 利用前向分布算法后，此时需要解决函数优化问题，即对该优化问题的每一步有：\n\\[\nF^*=\\arg\\min_{F}\\sum_{i=1}^NL(y_i,F(\\mathbf x_i))\n\\]\n\\[\nF_m(\\mathbf x)=F_{m-1}(\\mathbf x)+\\beta_m h(\\mathbf x;\\mathbf a_m)\n\\]\n由最速下降法得:\n\\[\nh(\\mathbf x_i;\\mathbf a_m)=-\\mathbf g_m(\\mathbf x_i)=-[\\frac{\\partial L(y_i,F(\\mathbf x_i))}{\\partial F(\\mathbf x_i)}]_{F(\\mathbf x)=F_{m-1}(\\mathbf x)}\n\\]\n但是 \\(h(\\mathbf x;\\mathbf a_m)\\) 是一个在 \\(\\mathcal D_{\\mathbf x}\\) 上均有定义的函数，而 \\(\\mathbf g_m(\\mathbf x_i)\\) 仅在 \\(\\{\\mathbf x_1,\\ldots,x_N\\}\\) 处有定义，故上式并不能直接将 \\(h(\\mathbf x;\\mathbf a_m)\\) 直接确定下来，需要采用其他方式让 \\(h(\\mathbf x;\\mathbf a_m)\\) 去贴合负梯度方向。 一种想法是在 \\(h(\\mathbf x;\\mathbf a_m)\\) 函数族中找到一个 \\(h^*(\\mathbf x;\\mathbf a_m)\\) 使得 \\(\\mathcal h_m=\\{h(\\mathcal x_i;\\mathcal a_m)\\}_1^N\\) 与 \\(-\\mathcal g_m\\) 最平行，则有如下优化问题：\n\\[\n\\mathbf a_m=\\arg\\min_a\\sum_{i=1}^N[-\\mathbf g_m(\\mathbf x_i)-\\alpha h(\\mathbf x_i;\\mathbf a)]^2\n\\]\n\n\n\n\n\n\n寻找负梯度方向说明\n\n\n\n\n此处的思想是回归的思想，但是没有截距项因为我们要的就是尽量平行。两向量平行的数学定义是 \\(-\\mathbf g_m=\\alpha \\mathbf h_m\\)\n\\(\\alpha\\) 也是该优化问题所要求解的\n\n\n\n一旦确定了 \\(h(\\mathbf x;\\mathbf a_m)\\) 后，我们便可利用精确一维线搜索去求解步长 \\(\\beta_m\\):\n\\[\n\\beta_m=\\arg\\min_{\\beta}\\sum_{i=1}^NL(y_i,F_{m-1}(\\mathbf x_i+\\beta h(\\mathbf x_i;\\mathbf a_m)))\n\\]\n确定了 \\(\\beta_m,\\mathbf a_m\\) 后便可得到：\n\\[\nF_m(\\mathbf x)=F_{m-1}(\\mathbf x)+\\beta_m h(\\mathbf x;\\mathbf a_m)\n\\]\n在现实数据中并不是直接去找 Equation 1 下的 \\(h(\\mathbf x;\\mathbf a_m)\\) ，而是将 \\(h(\\mathbf x;\\mathbf a_m)\\) 去拟合伪响应 \\(\\{\\tilde{y}_i=-\\mathbf g_m(\\mathbf x_i)\\}_i^N\\), 这使得函数优化问题转化为了最小二乘函数优化问题。\n\n\n\\begin{algorithm} \\caption{梯度提升算法} \\begin{algorithmic} \\State 初始化模型 $F_0(\\mathbf{x}) = \\arg\\min_{\\rho} \\sum_{i=1}^N L(y_i, \\rho)$ \\For{$m = 1$ 到 $M$} \\State 计算伪残差：$\\tilde y_i = -\\left[\\frac{\\partial L(y_i, F(\\mathbf{x}_i))}{\\partial F(\\mathbf{x}_i)}\\right]_{F=F_{m-1}}$ \\State 拟合一个基学习器 $h_m(\\mathbf{x};\\mathbf a_m)$ 来拟合伪残差 $\\tilde y_i:\\mathbf a_m=\\arg\\min_{\\mathbf a}\\sum_{i=1}^N[-\\mathbf g_m(\\mathbf x_i)-\\alpha h(\\mathbf x_i;\\mathbf a)]^2$ \\State 计算最佳步长：$\\beta_m=\\arg\\min_{\\beta}\\sum_{i=1}^NL(y_i,F_{m-1}(\\mathbf x_i+\\beta h(\\mathbf x_i;\\mathbf a_m)))$ \\State 更新模型：$F_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\beta_m h_m(\\mathbf{x};\\mathbf a_m)$ \\EndFor \\State \\Return 最终模型 $F_M(\\mathbf{x})$ \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "posts/[王推]S2/index.html#推全实验",
    "href": "posts/[王推]S2/index.html#推全实验",
    "title": "S2.A/B测试",
    "section": "4.1 推全实验",
    "text": "4.1 推全实验\n\n\n\n推全实验示意图\n\n\n在验证新策略有效时便可进行推全，此时将会新增一层“新层”与其他层正交，用以验证在全样本的情况下小流量实验结果。"
  },
  {
    "objectID": "posts/[算法导论]S2.3/index.html#归并排序算法",
    "href": "posts/[算法导论]S2.3/index.html#归并排序算法",
    "title": "S1.3 设计算法",
    "section": "2.2 归并排序算法",
    "text": "2.2 归并排序算法\n\n伪代码pythonC++\n\n\n\n\n\\begin{algorithm} \\caption{MERGE-SORT(A,p,r)} \\begin{algorithmic} \\State \\If{p&lt;r} \\State $q=\\lfloor (p+r)/2 \\rfloor$\\EndIf \\State MERGE-SORT(A,p,q) \\State MERGE-SORT(A,q+1,r) \\State MERGE(A,p,q,r) \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\nimport math\ndef MergeSort(A: list, p: int, r: int):\n    if p&lt;r:\n        q = math.floor((p+r)/2)  # 对半切分\n        MergeSort(A, p, q)  # 切分左半边\n        MergeSort(A, q+1, r)  # 切分右半边\n        merge(A, p, q+1, r)\n\nA = [4, 2, 7, 1, 5, 6, 3, 8]\nMergeSort(A, 0, len(A)-1)\nprint(f\"排序后：{A}\")\n\n排序后：[1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\n\nvoid MergeSort(vector&lt;int&gt;& A, int p, int r){\n    if (p&lt;r) {\n        int q = static_cast&lt;int&gt;((p+r)/2);\n        MergeSort(A, p, q);\n        MergeSort(A, q+1, r);\n        merge(A, p, q+1, r);\n    }\n}\n\nint main(){\n    vector&lt;int&gt; A = {4, 2, 7, 1, 5, 6, 3, 8};\n    MergeSort(A, 0, A.size()-1);\n    cout &lt;&lt; \"排序后：\";\n    for (int i = 0; i&lt;A.size(); i++){\n        cout &lt;&lt; A[i] &lt;&lt; \" \";\n    }\n    return 0;\n}\n// 排序后：1 2 3 4 5 6 7 8\n\n\n\n其运行过程如下："
  },
  {
    "objectID": "posts/[算法导论]S2.3/index.html#分析分治算法",
    "href": "posts/[算法导论]S2.3/index.html#分析分治算法",
    "title": "S1.3 设计算法",
    "section": "2.3 分析分治算法",
    "text": "2.3 分析分治算法\n这种归并排序的时间可由下式表示：\n\\[\nT(n)=\\begin{cases}\n\\Theta(1)&n\\leq c\\\\\naT(n/b)+D(n)+C(n)&\\text{其他}\n\\end{cases}\n\\]\n上式当问题规模足够小时，仅需常量时间。而当问题规模 \\(n&gt;c\\) 时，原问题解决的时间由以下三部分构成：\n\n解决被分解后规模相当于原问题 \\(1/b\\) 的a个子问题所耗费的时间 \\(aT(n/b)\\)\n分解成子问题所需要的时间 \\(D(n)\\)\n合并子问题成原问题的解需要的时间 \\(C(n)\\)\n\n\n2.3.1 归并排序算法时间复杂度分析\n\n分割：易得分解子问题的时间易得为常数时间。\n解决：为了便于分析此处假定问题的规模为2的幂数倍，则规模为 \\(n\\) 的原问题将会被分割为2个规模为 \\(n/2\\) 的子问题，解决这些子问题的时间为 \\(2T(n/2)\\)。\n合并：在归并排序算法中合并子问题的时间由上述分析得为 \\(\\Theta(n)\\)\n\n综上有如下公式： \\[\nT(n)=\\begin{cases}\n\\Theta(1)&n\\leq 1\\\\\n2T(n/2)+\\Theta(n)&\\text{其他}\n\\end{cases}\n\\]\n\n\n\n归并排序算法时间分析\n\n\n即归并排序算法的时间为 \\(\\Theta(n\\log_2n)\\)"
  },
  {
    "objectID": "posts/[深推]S2.7/index.html",
    "href": "posts/[深推]S2.7/index.html",
    "title": "S2.7 LS-PLM模型",
    "section": "",
    "text": "LS-PLM模型是“大规模分段线性模型”(Large Scale Piece-wise Linear Model)，其结构与三层神经网络极度类似。\n\n1 LS-PLM模型的主要结构\nLS-PLM可看作对逻辑回归的自然推广，其思路是在做逻辑回归前先进行聚类。其数学公式如下：\n\nf(x)=\\sum_{i=1}^m\\pi_i(x)\\cdot\\theta_i(x)=\\sum_{i=1}^m\\frac{e_{\\mu_ix}}{\\sum_{j=1}^me^{\\mu_j\\cdot x}}\\cdot\\frac{1}{1+e^{-w_ix}}\n\n上式中假设聚类函数 \\pi 为softmax函数，而后用LR模型计算样本在分片中具体的CTR，然后将二者相乘后求和。分片数 m ：可以较好第平衡模型的拟合和推广能力，m越大，模型拟合能力越强，但模型复杂度也随之上升，m的经验值为12。该模型的优点如下：\n\n端对端的非线性学习能力\n模型的稀疏性强（建模时引入L1和L2，1范数）\n\n\n\n2 L1和L2范数的区别\n\nL1范数指在损失函数中引入 \\lambda||w||_1 惩罚项\nL2范数指在损失函数中引入 \\lambda||w||^2_2 惩罚项\n\n故L1-loss为:\n\nL_1(w)=L(w)+\\lambda ||w||_1\n\nL2-loss为：\n\nL_2(w)=L(w)+\\lambda ||w||^2_2\n\n假定真实模型为 y=5x，利用python产生如下模拟数据：\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'svg'\n\nnp.random.seed(42)\nx = np.linspace(0, 10, 100)\ny = [2*i + np.random.normal(0, 9) for i in x]\nfig, ax = plt.subplots(figsize=(8,6))\nax.scatter(x, y, label=\"生成的虚拟数据\")\nax.plot(x, 2*x, label=\"真实模型\", c=\"r\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n以下动画展示了L1，L2范数下的原损失函数，正则化函数和L1，L2损失函数之间的关系。\n\n\n                                                \n\n\n以上动画揭露了以下信息：\n\n当 \\lambda 大于130时L1损失函数的最小值就一直处在y轴上，而L2损失函数的最小值却没有。（L1正则化系数稀疏的表现）\nL1/L2损失函数的最小值并不在正则化函数与原损失函数的交点或切点处( \\lambda=32,12)。\n\n从贝叶斯统计的角度上讨论，正则化项可看作贝叶斯推断中的先验分布，而L1正则化对应的是拉普拉斯分布，L2对应的是正态分布，它们的密度函数如下：\n\n\\text{Laplace distribution: } f(x|\\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x - \\mu|}{b}\\right)\n\n\n\\text{Normal distribution: } f(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\n它们的密度函数图像如下图：\n\n\n\n\n\n\n\n\n\n由图可得，拉普拉斯分布在 x=0 处尖峰，相较于正态分布，拉普拉斯分布的随机变量更可能为0，因此，L1正则化后的参数更加稀疏。\n\n\n3 LS-PLM模型与深度学习的联系\nLS-PLM模型可以看作一个加入了注意力机制的三层神经网络模型，其中输入层是样本的特征向量，中间层是由m个神经元组成的隐层，其中m是分片的个数，对于一个CTR预估问题，LS-PLM的最后一层自然是由单一神经元组成的输出层。\n\n\n\n\n\n\n注意力机制\n\n\n\n在隐层和输出层之间存在的，神经元之间的权重是由分片函数得出的注意力得分来确定的，即是样本属于哪个分片的概率就是其注意力得分。\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Softmax函数/index.html",
    "href": "posts/Softmax函数/index.html",
    "title": "Softmax函数",
    "section": "",
    "text": "Softmax函数将K维实向量转化为拥有K个可能结果的概率分布列。它是Logistic回归的多维推广，常用作神经网络最后的激活函数，用以归一化神经网络的输出结果，形成概率分布。\n\n1 定义\nSoftmax函数的定义为： \\(\\sigma:\\mathcal{R}^k\\to(0,1)^K\\)，具体形式如下式：\n\\[\n\\sigma(z)_i=\\frac{e^{z_i}}{\\sum_{j=1}^Ke^{z_j}}\n\\]\n\n\n                                                \n\n\n由以上动画可得，当基底 \\(b\\in(0,1)\\) 时，\\(b\\) 越小分布越尖，而当 \\(b\\in(1,+\\infty)\\) 时， \\(b\\) 越大分布越尖。\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Logistic回归与最大熵模型/index.html",
    "href": "posts/Logistic回归与最大熵模型/index.html",
    "title": "Logistic回归",
    "section": "",
    "text": "Logistic回归是统计学习中的经典分类方法，而最大熵是概率模型学习的一个准则，将其推广到分类问题中便得到了最大熵模型(MEM, maximum entropy model)。"
  },
  {
    "objectID": "posts/Logistic回归与最大熵模型/index.html#logistic分布",
    "href": "posts/Logistic回归与最大熵模型/index.html#logistic分布",
    "title": "Logistic回归",
    "section": "1.1 Logistic分布",
    "text": "1.1 Logistic分布\nLogistic分布的定义如下：\n\n分布 \\[\nF(x)=P(X\\leq x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}\n\\]\n密度 \\[\nf(x)=F'(x)=\\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(e^{-(x-\\mu)/\\gamma})^2}\n\\]\n\n式中，\\(\\mu\\) 为位置参数，\\(\\gamma&gt;0\\) 为形状参数。"
  },
  {
    "objectID": "posts/Logistic回归与最大熵模型/index.html#二项逻辑回归模型",
    "href": "posts/Logistic回归与最大熵模型/index.html#二项逻辑回归模型",
    "title": "Logistic回归",
    "section": "1.2 二项逻辑回归模型",
    "text": "1.2 二项逻辑回归模型\n二项逻辑回归模型是一种分类模型，由 \\(P(Y|X)\\) 表示，其形式如下式：\n\\[\n\\begin{aligned}\nP(Y=1|x)=\\frac{\\exp(w\\cdot x)}{1+\\exp(w\\cdot x)}\\\\\nP(Y=0|x)=\\frac{1}{1+\\exp(w\\cdot x)}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n事件的几率(odd)\n\n\n\n事件的几率是指事件发生的概率于该事件不发生概率的比值，对数几率（Logit函数）是指 \\(\\log(\\frac{p}{1-p})\\)\n\n\n则输出 \\(Y=1\\) 的对数几率为:\n\\[\n\\log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x\n\\]\n为输入x的线性函数，即输出 \\(Y=1\\) 的对数几率是由输入 \\(x\\) 的线性函数表示的模型，这便是Logistic回归模型。\n\n1.2.1 模型参数估计\nLogistic回归模型可采用极大似然估计模型参数，假设有 \\(P(Y=1|x)=\\pi(x),P(Y=0|x)=1-\\pi(x)\\)\n\n\n\nLogistic回归模型参数的极大似然估计\n\n\n对于求解一对数似然函数为目标函数的最优化问题，Logistic回归学习常用的方法是梯度下降法和拟牛顿法。\n\n\n1.2.2 多项Logistic回归\n假设离散型随机变量 \\(Y\\) 的取值集合是 \\(\\{1, 2, \\ldots, K\\}\\)，那么多项Logistic回归模型是：\n\\[\n\\begin{aligned}\nP(Y=k|x)&=\\frac{\\exp(w_k\\cdot x)}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\cdot x)}&k=1,2,\\ldots, K-1\\\\\nP(Y=K|x)&=\\frac{1}{1+\\sum_{k=1}^{K-1}\\exp(w_k\\cdot x)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/[hot100]Day1/index.html",
    "href": "posts/[hot100]Day1/index.html",
    "title": "Day1",
    "section": "",
    "text": "1 删除有序数组中的重复项\n\n\n伪代码PythonC++\n\n\n\n\n\\begin{algorithm} \\caption{removeDuplicates(A)} \\begin{algorithmic} \\State slow=0 \\For{fast=1 \\To A.length-1} \\State \\If{A[fast]!=A[slow]} \\State A[slow+1] = A[fast] \\State slow += 1\\EndIf\\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\nfrom typing import List\n\ndef removeDuplicates(A: List[int]) -&gt; int:\n    if not A:\n        return 0\n    slow = 0\n    for fast in range(1, len(A)):\n        if A[fast] != A[slow]:\n            A[slow+1] = A[fast]\n            slow += 1\n    return slow+1\n\nA = [1, 1, 1, 2, 2, 4, 6, 7, 7, 7, 8, 8, 8, 8, 8, 8]\na = removeDuplicates(A)\nprint(f\"返回值:{a}\")\nprint(f\"A:{A[:a]}\")\n\n返回值:6\nA:[1, 2, 4, 6, 7, 8]\n\n\n\n\nint removeDuplicates(vector&lt;int&gt;& nums) {\n        if (nums.empty()) return 0;\n        int slow = 0;\n        for (int fast=1; fast&lt;nums.size(); fast++){\n            if (nums[fast]!=nums[slow]){\n                nums[++slow] = nums[fast];\n            }\n        }\n        return slow+1;\n    }\n\n\n\n\n\n\n\n\n\n解决方法及证明\n\n\n\n双指针fast, slow\n\nfast:用以跳过重复序列并遍历整个数组\nslow:用以指向最后合法元素位置\n\n\n\n初始化：当slow在 \\(0\\) 处, fast在1处\n保持：当出现首个与目前slow不同的元素时，slow向前移动并赋值为fast所在元素，此时slow及其以前的子集符合要求。\n结束：当fast达到数组尾部时，迭代停止，此时slow及其以前的子集符合要求。\n\n\n\n\n\n2 删除有序数组中的重复项Ⅱ\n\n\n伪代码PythonC++\n\n\n\n\n\\begin{algorithm} \\caption{removeDuplicates2(A)} \\begin{algorithmic} \\State \\IF{A.length&lt;=2} \\State \\Return A.length \\EndIf \\State slow=1 \\State \\For{fast=2\\To A.length} \\State \\If{A[fast]!=A[slow-1]} \\State slow += 1 \\State A[slow] = A[fast] \\EndIf \\EndFor \\State \\Return slow+1 \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\ndef removeDuplicates2(A: List[int]) -&gt; int:\n    if len(A) &lt;=2:\n        return len(A)\n    slow = 1\n    for fast in range(2, len(A)):\n        if A[fast]!=A[slow-1]:\n            slow += 1\n            A[slow] = A[fast]\n    return slow+1\n\n\n\nint removeDuplicates(vector&lt;int&gt;& nums) {\n        if (nums.size()&lt;=2){\n            return nums.size();\n        }\n        int slow = 1;\n        for (int fast=2; fast&lt;nums.size(); fast++){\n            if (nums[fast]!=nums[slow-1]){\n                nums[++slow]=nums[fast];\n            }\n        }\n        return slow+1;\n    }\n\n\n\n\n\n\n\n\n\n解决方法及证明\n\n\n\n双指针，fast和slow：\n\nfast：用以遍历全数组的同时跳出重复值\nslow：用以记录合法子集的最后一个索引\n\n\n循环不变式：slow及其以前元素组成的子集是满足要求，且其中的元素是原数组各数值出现的前两个而非其他位置的。\n\n初始化：初始的slow=1即数组的前两个元素组成的子集必然满足条件。\n保持：需分情况讨论，slow向前走的情况分为：1)首次添加一个元素;2)重复添加一个已经出现一次的元素。\n\n当首次添加时，slow及其以前的元素构成的子集一定满足条件（假设），此时slow-1为其他元素，在slow+1首次添加元素，且下次增加相同元素的时机是当fast来到原数组相同元素第二次出现的位置，而非其他位置，故保持循环不变式成立；\n当重复添加时，易得也满足，且下一次不会再重复添加第三次因为此时slow-1为该元素，不会导致slow的更新(A[fast]!=A[slow-1])，循环不变式成立。\n\n终止：当fast达到原数组的末尾时循环终止，此时不会再添加相同的元素和首个元素，循环不变式易得成立。\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[王推]S2/index.html#反转推全",
    "href": "posts/[王推]S2/index.html#反转推全",
    "title": "S2.A/B测试",
    "section": "4.2 反转推全",
    "text": "4.2 反转推全\n有些指标（点击、交互）立刻会得到新策略的反馈，而有的指标却存在滞后性需长期观测，此时可用反转实验再推全的新层中开一个旧策略的桶，长期观察实验指标。\n\n\n\n反转推全示意图"
  },
  {
    "objectID": "posts/[王推]S3/index.html",
    "href": "posts/[王推]S3/index.html",
    "title": "S3.协同过滤算法(召回)",
    "section": "",
    "text": "ItemCF召回的完整流程如下：\n\n\n\n\n建立“用户➡️物品”的索引\n\n记录每个用户点击、交互过的物品ID，给定任意用户ID，可以找到他最近感兴趣的物品列表。\n\n\n\n\n\n\n“用户➡️物品”的索引示意图\n\n\n\n\n建立“物品➡️物品”索引\n\n计算物品之间两两相似度，对于每个物品索引与它最相似的k个物品。物品相似独的计算由如下公式确定：\n\n\n\n\\[\n\\text{sim}(i_1,i_2)=\\frac{|w_1\\cap w_2|}{\\sqrt{|w_1|\\cdot|w_2|}}\n\\]\n上式 \\(w_1\\)为喜欢物品 \\(i_1\\) 的用户集 \\(w_1\\)，\\(w_2\\) 为喜欢物品 \\(i_2\\) 的用户集。\n\n\n\n“物品➡️物品”的索引示意图\n\n\n\n\n\n\n\n\n线上召回示意图\n\n\n\n给定用户ID，通过“用户➡️物品”的索引，找到用户近期感兴趣的物品列表(last-n)。\n对于last-n列表中的每个物品，通过“物品➡️物品”的索引，找到top-k相似的物品。\n对于取回的最多nk个相似物品，用以下公式预估用户对物品的兴趣分数。 \\[\nR_{u,i}=R_{u,j}\\cdot\\text{sim}(i,j)\n\\]\n\n上式中的 \\(j\\) 表示物品i是由j召回的，\\(R_{u,\\cdot}\\) 表示用户对某物品的兴趣分数。对于重复的物品，利用兴趣分相加进行去重。\n\n返回分数最高的x个物品，作为推荐结果召回通道的输出。\n\nItemCF采用索引避免了枚举所有的物品，但在离线计算时计算量大，与之相对的是线上计算量小。其主要使用用户行为定义物品相似度。"
  },
  {
    "objectID": "posts/[王推]S3/index.html#事先做离线计算",
    "href": "posts/[王推]S3/index.html#事先做离线计算",
    "title": "S3.协同过滤算法(召回)",
    "section": "",
    "text": "建立“用户➡️物品”的索引\n\n记录每个用户点击、交互过的物品ID，给定任意用户ID，可以找到他最近感兴趣的物品列表。\n\n\n\n\n\n\n“用户➡️物品”的索引示意图\n\n\n\n\n建立“物品➡️物品”索引\n\n计算物品之间两两相似度，对于每个物品索引与它最相似的k个物品。物品相似独的计算由如下公式确定：\n\n\n\n\\[\n\\text{sim}(i_1,i_2)=\\frac{|w_1\\cap w_2|}{\\sqrt{|w_1|\\cdot|w_2|}}\n\\]\n上式 \\(w_1\\)为喜欢物品 \\(i_1\\) 的用户集 \\(w_1\\)，\\(w_2\\) 为喜欢物品 \\(i_2\\) 的用户集。\n\n\n\n“物品➡️物品”的索引示意图"
  },
  {
    "objectID": "posts/[王推]S3/index.html#线上做召回",
    "href": "posts/[王推]S3/index.html#线上做召回",
    "title": "S3.协同过滤算法(召回)",
    "section": "",
    "text": "线上召回示意图\n\n\n\n给定用户ID，通过“用户➡️物品”的索引，找到用户近期感兴趣的物品列表(last-n)。\n对于last-n列表中的每个物品，通过“物品➡️物品”的索引，找到top-k相似的物品。\n对于取回的最多nk个相似物品，用以下公式预估用户对物品的兴趣分数。 \\[\nR_{u,i}=R_{u,j}\\cdot\\text{sim}(i,j)\n\\]\n\n上式中的 \\(j\\) 表示物品i是由j召回的，\\(R_{u,\\cdot}\\) 表示用户对某物品的兴趣分数。对于重复的物品，利用兴趣分相加进行去重。\n\n返回分数最高的x个物品，作为推荐结果召回通道的输出。\n\nItemCF采用索引避免了枚举所有的物品，但在离线计算时计算量大，与之相对的是线上计算量小。其主要使用用户行为定义物品相似度。"
  },
  {
    "objectID": "posts/[王推]S3/index.html#离线计算",
    "href": "posts/[王推]S3/index.html#离线计算",
    "title": "S3.协同过滤算法(召回)",
    "section": "3.1 离线计算",
    "text": "3.1 离线计算\n\n建立“用户➡️物品”的索引。（其中主要记录物品ID和用户对物品的兴趣分数）\n建立“用户➡️用户”的索引。（记录用户ID和用户相似度）\n\n示意图仅需将之前示意图颜色框中的物品换成用户。"
  },
  {
    "objectID": "posts/[王推]S3/index.html#线上召回",
    "href": "posts/[王推]S3/index.html#线上召回",
    "title": "S3.协同过滤算法(召回)",
    "section": "3.2 线上召回",
    "text": "3.2 线上召回\n\n给定用户ID，通过“用户➡️用户”的索引找到top-k相似用户。\n对于每个top-k相似用户，通过“用户➡️物品”的索引，找到用户近期感兴趣的物品列表(last-n)。\n对于召回的nk个相似用户，用公式预估用户对每个物品的兴趣分数，\n返回分数最高的100个物品，作为召回结果。"
  },
  {
    "objectID": "posts/[hot100]Day2/index.html",
    "href": "posts/[hot100]Day2/index.html",
    "title": "Day2",
    "section": "",
    "text": "1 搜索旋转排序数组\n\n\n\n\n\n\n\n分析\n\n\n\n\n要求时间复杂度为 \\(\\Theta(\\log(n))\\)，目前已知二分查找的复杂度符合\n旋转排序数组的特征是从某个位置切下后两边都为有序数组，而从其他位置切下时将会得到一个有序一个无序。\n\n\n\n\n伪代码PythonC++\n\n\n\n\n\\begin{algorithm} \\caption{searchRotated(A, target)} \\begin{algorithmic} \\State first = 0, last = A.length - 1 \\While{first &lt;= last} \\State mid = $\\lfloor first + (last - first)/2 \\rfloor$ \\Comment{计算中间索引} \\If{A[mid] == target} \\State \\Return mid \\Comment{如果中间值等于目标值，返回索引} \\EndIf \\If{A[first] &lt;= A[mid]} \\If{A[first] &lt;= target \\And target &lt; A[mid]} \\State last = mid - 1 \\Comment{目标值在左侧有序子数组中} \\Else \\State first = mid + 1 \\Comment{目标值在右侧子数组中} \\EndIf \\Else \\If{A[mid] &lt; target \\And target &lt;= A[last]} \\State first = mid + 1 \\Comment{目标值在右侧有序子数组中} \\Else \\State last = mid - 1 \\Comment{目标值在左侧子数组中} \\EndIf \\EndIf \\EndWhile \\State \\Return -1 \\Comment{未找到目标值，返回-1} \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\nfrom typing import List\ndef searchRotated(A: List[int], target: int) -&gt; int:\n    import math\n    first = 0\n    last = len(A) - 1\n    while first &lt;= last:\n        mid = math.floor(first+(last-first)/2)\n        if A[mid]==target:\n            return mid\n        if A[first]&lt;=A[mid]:\n            if A[first]&lt;=target and A[mid]&gt;target:\n                last = mid - 1\n            else:\n                first = mid + 1\n        else:\n            if A[mid]&lt;target and A[last]&gt;=target:\n                first = mid + 1\n            else:\n                last = mid - 1\n    return -1\n\n\n\nint searchRotated(vector&lt;int&gt;& nums, int target){\n    int first = 0, last = nums.size() - 1;\n    while (first&lt;=last){\n        int mid = first + (last - first) / 2;\n        if (nums[mid]==target){\n            return mid;\n        }        \n        if (nums[first]&lt;=nums[mid]){\n            if (nums[first]&lt;=target && nums[mid]&gt;target){\n                last = mid - 1;\n            }\n            else first = mid + 1;\n        }\n        else{\n            if (nums[mid]&lt;target && nums[last]&gt;=target){\n                first = mid + 1;\n            }\n            else last = mid - 1;\n        }\n    }\n    return -1;\n}\n\n\n\n\n\n2 搜索旋转排序数组Ⅱ\n\n\n\n\n\n\n\n分析\n\n\n\n该问题与上一个问题的区别在于元素可重复，元素可重复并没有改变当从数组中间切一刀时将会得到一个有序的子数组和一个无序的子数组的事实，但是会造成无法通过A[first]&lt;=A[mid]来判断是否为有序数列如[1,3,1,1,1]，因此需要修改。 - 若A[first]&lt;A[mid]，则必可说明A[first..mid]为有序数组。 - 若A[first]&gt;A[mid]，则必可说明A[first..mid]为无序数组，A[mid..last]为有序数组。 - 若A[first]=A[mid]，则可通过first=first+1的形式避免这种情况，由于mid处和first处值一致因此直接删除first处的值也无碍。\n\n\n\n伪代码Python\n\n\n\n\n\\begin{algorithm} \\caption{searchRotated2(nums, target)} \\begin{algorithmic} \\State $first \\gets 0$ \\State $last \\gets \\text{length}(nums) - 1$ \\While{$first \\leq last$} \\State $mid \\gets \\lfloor first + \\frac{(last - first)}{2} \\rfloor$ \\If{$nums[mid] = target$} \\State \\Return true \\EndIf \\If{$nums[first] &lt; nums[mid]$} \\If{$nums[first] \\leq target \\And target &lt; nums[mid]$} \\State $last \\gets mid - 1$ \\Else \\State $first \\gets mid + 1$ \\EndIf \\ElsIf{$nums[first] &gt; nums[mid]$} \\If{$nums[mid] &lt; target \\And nums[last] \\geq target$} \\State $first \\gets mid + 1$ \\Else \\State $last \\gets mid - 1$ \\EndIf \\Else \\State $first \\gets first + 1$ \\EndIf \\EndWhile \\State \\Return $false$ \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\nfrom typing import List\ndef searchRotated(A: List[int], target: int) -&gt; int:\n    import math\n    first = 0\n    last = len(A) - 1\n    while first &lt;= last:\n        mid = math.floor(first+(last-first)/2)\n        if A[mid]==target:\n            return True\n        if A[first]&lt;A[mid]:\n            if A[first]&lt;=target and A[mid]&gt;target:\n                last = mid - 1\n            else:\n                first = mid + 1\n        elif A[first]&gt;A[mid]:\n            if A[mid]&lt;target and A[last]&gt;=target:\n                first = mid + 1\n            else:\n                last = mid - 1\n        else:\n            first += 1\n    return False\n\nbool search(vector&lt;int&gt;& nums, int target) {\n        int first = 0, last = nums.size() - 1;\n        while (first&lt;=last){\n            int mid = first + (last - first) / 2;\n            if (nums[mid]==target){\n                return true;\n            }        \n            if (nums[first]&lt;nums[mid]){\n                if (nums[first]&lt;=target && nums[mid]&gt;target){\n                    last = mid - 1;\n                }\n                else first = mid + 1;\n            }\n            else if (nums[first]&gt;nums[mid]){\n                if (nums[mid]&lt;target && nums[last]&gt;=target){\n                    first = mid + 1;\n                }\n                else last = mid - 1;\n            }\n            else first++;\n        }\n        return false;\n    }\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[hot100]Day3/index.html",
    "href": "posts/[hot100]Day3/index.html",
    "title": "Day3",
    "section": "",
    "text": "1 寻找两正序数组中位数\n\n\n\n\n\n\n\n分析\n\n\n\n\n由于限制时间复杂度为 \\(O\\log(m+n)\\)，因此可以联想到二分或者递归。\n找中位数可转化为找第k大的值\n选取两序列 \\(\\lfloor k/2\\rfloor-1\\) 处的值进行比对，若A序列的小于B序列的则说明第k大的数不在A序列的左半边，可直接删去；若大于，则删除B左边的；若相等则A[\\(\\lfloor k/2\\rfloor-1\\)]或B[\\(\\lfloor k/2\\rfloor-1\\)]为第k大的数。\n若处于3.中不相等的情况时，则应当转为寻找第k-\\(\\lfloor k/2\\rfloor\\)大的数。\n若两数组中存在至少一个空集时，直接返回非空集的第k个元素。\n若k=1，则返回A,B中第一个元素较大的。\n\n\n\n\n伪代码PythonC++\n\n\n\n\n\\begin{algorithm} \\caption{findK(nums1,m,nums2,n,x)} \\begin{algorithmic} \\If{m&gt;n}\\Comment{保证前一个传入的数组是个数较少的} \\State \\Return findK(nums2,n,nums1,x) \\EndIf \\If{m==0}\\Comment{当存在空集合时另一个集合的第x个即为所求} \\State \\Return nums2[x-1] \\EndIf \\If{x==1}\\Comment{当x为1时仅需比较两数组的首位即可} \\State \\Return min(nums1[0], nums2[0]) \\EndIf \\State ia = min($\\lfloor x/2\\rfloor$,m), ib = x - ia \\Comment{保证比较前x个元素} \\If{nums1[ia-1]&lt;nums2[ib-1]} \\State \\Return findK(nums1[ia..], m-ia, nums2, n, x-ia) \\Elif{nums1[ia-1]&gt;nums2[ib-1]} \\State \\Return findK(nums1, m, nums2[ib..], n-ib, x-ib) \\Else \\State \\Return nums1[ia-1] \\EndIf \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\\begin{algorithm} \\caption{findMedian(nums1, nums2)} \\begin{algorithmic} \\State m=nums1.length,n=nums2.length \\If{m+n为奇数} \\State \\Return findK(nums1, m, nums2, n, (m+n+1)/2) \\Else \\State \\Return (findK(nums1, m, nums2, n, (m+n)/2)+findK(nums1, m, nums2, n, (m+n)/2+1))/2 \\EndIf \\end{algorithmic} \\end{algorithm}\n\n\n之所以此时的时间复杂度为 \\(O(\\log(m+n))\\) 是因为算法最多会运行 \\(\\Theta(\\log(x))\\) ，而x的上界为m+n，因此时间复杂度为 \\(O(\\log(m+n))\\)\n\n\n\nfrom typing import List\nclass Solution:\n    def findK(self, nums1: List[int], m: int, nums2: List[int], n: int, x: int) -&gt; float:\n        if m&gt;n:\n            return self.findK(nums2, n, nums1, m, x)\n        if m==0:\n            return nums2[x-1]\n        if x==1:\n            return min(nums1[0], nums2[0])\n        ia = min(m, int(x/2)); ib = x - ia\n        if nums1[ia-1] &lt; nums2[ib-1]:\n            return self.findK(nums1[ia:], m-ia, nums2, n, x-ia)\n        elif nums1[ia-1] &gt; nums2[ib-1]:\n            return self.findK(nums1, m, nums2[ib:], n-ib, x-ib)\n        else:\n            return nums1[ia-1]\n    def findMedianSortedArrays(self, nums1: List[int], nums2: List[int]) -&gt; float:\n        m = len(nums1); n = len(nums2)\n        if (m+n)%2 == 1:\n            return self.findK(nums1, m, nums2, n, int((m+n+1)/2))\n        else:\n            return (self.findK(nums1, m, nums2, n, int((m+n)/2)) + self.findK(nums1, m, nums2, n, int((m+n)/2+1)))/2\n\n\n\nclass Solution {\npublic:\n    double findMedianSortedArrays(vector&lt;int&gt;& nums1, vector&lt;int&gt;& nums2) {\n        int m = nums1.size(), n = nums2.size();\n        int total = m + n;\n        if (total % 2 != 0) return findK(nums1.begin(), m, nums2.begin(), n, total / 2 + 1);\n        else return (findK(nums1.begin(), m, nums2.begin(), n, total / 2) \n                    + findK(nums1.begin(), m, nums2.begin(), n, total / 2 + 1)) / 2.0;\n    }\nprivate:\n    static int findK(vector&lt;int&gt;::const_iterator nums1, int m, vector&lt;int&gt;::const_iterator nums2, int n, int x){\n        if (m &gt; n) return findK(nums2, n, nums1, m, x);\n        if (m == 0) return  *(nums2 + x - 1);\n        if (x == 1) return min(nums1[0], nums2[0]);\n        int ia = min(x/2, m), ib = x - ia; \n        if (*(nums1 + ia - 1) &lt; *(nums2 + ib -1)) return findK(nums1 + ia, m - ia, nums2, n, x - ia);\n        else if (*(nums1 + ia - 1) &gt; *(nums2 + ib -1)) return findK(nums1, m, nums2 + ib, n - ib, x - ib);\n        else return *(nums1 + ia -1);\n    }\n};\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[hot100]Day4/index.html",
    "href": "posts/[hot100]Day4/index.html",
    "title": "Day4",
    "section": "",
    "text": "今天涉及哈希表的有关内容。\n\n\n哈希表(Hash Table)也称散列表，是一种特殊的数据结构。其通过一定的规则将数据进行 “ 散乱排列 ” ， 并通过该规则进行快速查找。这个规则叫做 “ 哈希函数 ” ，其可以将数据转化为一个固定的索引值，后续将该数据根据索引值存储在数组的某个位置。\n\n\n在理想状态下，哈希函数会将数据均匀散列分布在哈希表中。主要有直接定址法、除留余数法等。\n1）直接定址法\n当数据的范围比较集中是，优先考虑使用定址法。其主要实现方法类似一种键值对，键为数据的值，值为数据出现的次数。\n该方法的缺陷是当数据比较分散且范围较大时，就造成空间浪费。\n2）除留余数法\n其核心思想是将数据对某个数（一般是表长）进行求余运算，将运算结果作为索引值，即表示该数据存放的下标位置。假设某哈希表的大小为M，则哈希函数为 \\(\\text{hash}=x \\% M\\)\n此时应当注意：\n\n为了尽量减少哈希冲突出现的次数，建议M取不太接近2的整数次幂的一个素数。\n除留余数法中，为了方便数据转化为索引值，要求哈希表的数据元素支持转换为整数。\n\n\n\n\n当一个数据通过哈希函数产生了与另一个数据相同的哈希值，那么也就意味着它们需要存储在同一个位置，显然无法满足。此时的情况就叫做哈希冲突（哈希碰撞）。我们可以通过以下两种方法进行解决：\n1）开放定址法（闭散列）\n开放定址法是按照某种策略将冲突的数据放置在表中的另一个空位当中。常见的策略有三种：线性探测、二次探测和双重探测。\n\n线性探测：线性探测指的是从冲突位置开始，依次向后一个一个查找，直到找到空位，再将冲突元素放置在当前空位。 当查找到表尾还没有找到空位，那么从表头位置开始重新查找。（容易造成表中的冲突数据聚集，且会占用其他元素的原本位置）\n二次探测：二次探测对线性探测进行了优化。它的查找方式是依次左右按照二次方跳跃式探测。例如先探测冲突位置的右边 \\(1^{2}\\) 个位置，然后左边 \\(1^{2}\\) 个位置，右边 \\(2^{2}\\) 个位置，左边 \\(2^{2}\\) 个位置……依次类推。（二次探测有效地减少了数据聚集问题）\n\n2）链接址法（开散列）\n其改变了传统的哈希表元素存储策略，在链地址法当中，所有数据不再直接存储在哈希表中，而是将哈希表的每一个元素设置为指针，用以维护一个链表。此时哈希表中的每一个元素称作哈希桶，其中存储的元素互相之间就是哈希冲突的。\n链接地址在发生冲突时，就不会占用其他元素的原本位置，相比开放地址法效率较高。\n\n\n\n记 \\(\\alpha=\\) 表中元素的个数/散列表的长度，当装填因子过大时，哈希表的空间利用率就越高，发生哈希冲突的概率就越高，哈希表的总体性能就越低。在实际运用过程中，我们将装填因子作为哈希表是否需要扩容的标准。\n\n采用开发定址法处理冲突时，该阈值通常位于0.7-0.8之间；\n使用链地址法时，阈值则通常为1。\n\n\n\n\n\n\n\nWarning\n\n\n\n懈怠了┗|｀O′|┛ 嗷~~"
  },
  {
    "objectID": "posts/[hot100]Day4/index.html#哈希表",
    "href": "posts/[hot100]Day4/index.html#哈希表",
    "title": "Day4",
    "section": "",
    "text": "哈希表(Hash Table)也称散列表，是一种特殊的数据结构。其通过一定的规则将数据进行 “ 散乱排列 ” ， 并通过该规则进行快速查找。这个规则叫做 “ 哈希函数 ” ，其可以将数据转化为一个固定的索引值，后续将该数据根据索引值存储在数组的某个位置。\n\n\n在理想状态下，哈希函数会将数据均匀散列分布在哈希表中。主要有直接定址法、除留余数法等。\n1）直接定址法\n当数据的范围比较集中是，优先考虑使用定址法。其主要实现方法类似一种键值对，键为数据的值，值为数据出现的次数。\n该方法的缺陷是当数据比较分散且范围较大时，就造成空间浪费。\n2）除留余数法\n其核心思想是将数据对某个数（一般是表长）进行求余运算，将运算结果作为索引值，即表示该数据存放的下标位置。假设某哈希表的大小为M，则哈希函数为 \\(\\text{hash}=x \\% M\\)\n此时应当注意：\n\n为了尽量减少哈希冲突出现的次数，建议M取不太接近2的整数次幂的一个素数。\n除留余数法中，为了方便数据转化为索引值，要求哈希表的数据元素支持转换为整数。\n\n\n\n\n当一个数据通过哈希函数产生了与另一个数据相同的哈希值，那么也就意味着它们需要存储在同一个位置，显然无法满足。此时的情况就叫做哈希冲突（哈希碰撞）。我们可以通过以下两种方法进行解决：\n1）开放定址法（闭散列）\n开放定址法是按照某种策略将冲突的数据放置在表中的另一个空位当中。常见的策略有三种：线性探测、二次探测和双重探测。\n\n线性探测：线性探测指的是从冲突位置开始，依次向后一个一个查找，直到找到空位，再将冲突元素放置在当前空位。 当查找到表尾还没有找到空位，那么从表头位置开始重新查找。（容易造成表中的冲突数据聚集，且会占用其他元素的原本位置）\n二次探测：二次探测对线性探测进行了优化。它的查找方式是依次左右按照二次方跳跃式探测。例如先探测冲突位置的右边 \\(1^{2}\\) 个位置，然后左边 \\(1^{2}\\) 个位置，右边 \\(2^{2}\\) 个位置，左边 \\(2^{2}\\) 个位置……依次类推。（二次探测有效地减少了数据聚集问题）\n\n2）链接址法（开散列）\n其改变了传统的哈希表元素存储策略，在链地址法当中，所有数据不再直接存储在哈希表中，而是将哈希表的每一个元素设置为指针，用以维护一个链表。此时哈希表中的每一个元素称作哈希桶，其中存储的元素互相之间就是哈希冲突的。\n链接地址在发生冲突时，就不会占用其他元素的原本位置，相比开放地址法效率较高。\n\n\n\n记 \\(\\alpha=\\) 表中元素的个数/散列表的长度，当装填因子过大时，哈希表的空间利用率就越高，发生哈希冲突的概率就越高，哈希表的总体性能就越低。在实际运用过程中，我们将装填因子作为哈希表是否需要扩容的标准。\n\n采用开发定址法处理冲突时，该阈值通常位于0.7-0.8之间；\n使用链地址法时，阈值则通常为1。\n\n\n\n\n\n\n\nWarning\n\n\n\n懈怠了┗|｀O′|┛ 嗷~~"
  },
  {
    "objectID": "posts/[hot100]Day5/index.html",
    "href": "posts/[hot100]Day5/index.html",
    "title": "Day5",
    "section": "",
    "text": "1 最长连续序列\n\n\n\n\n\n\n\n分析\n\n\n\n由于要求时间复杂度为 \\(O(n)\\)，同时该题的本质在于查询，哈希表的查询效率就是 \\(O(1)\\) 。因此可以采用哈希表进行查询同时遍历所有值\n\n\n\n伪代码C++Python\n\n\n\n\n\\begin{algorithm} \\caption{longestConsecutive(nums)} \\begin{algorithmic} \\State 哈希表 used \\For{i in nums} \\State used[i] = false \\Comment{初始化所有索引认为均未遍历} \\EndFor \\State longest = 0\\Comment{设置初始化最大连续长度} \\For{i in nums}\\Comment{开始遍历全部元素这也是时间复杂度为$O(n)$的原因} \\If{used[i]} \\Continue\\Comment{若被遍历过说明其已找到属于它的最大连续子数组，没有必要再遍历} \\EndIf \\State length = 1\\Comment{初始化最大连续长度} \\State used[i] = True\\Comment{标记该元素已经被遍历} \\State j = i + 1\\Comment{开始遍历其右侧的值直至不再连续} \\While{j in used.keys} \\State used[j] = true \\State length += 1\\Comment{找到了一个连续的} \\State j += 1 \\EndWhile \\State j = i - 1\\Comment{继续寻找i左侧的连续值} \\While{j in used.keys} \\State used[j] = true \\State length += 1 \\State j -= 1 \\EndWhile \\State longest = max(length, longest) \\EndFor \\Return longest \\end{algorithmic} \\end{algorithm}\n\n\n\n\nclass Solution {\npublic:\n    int longestConsecutive(vector&lt;int&gt;& nums) {\n        unordered_map&lt;int, bool&gt; used;  // 创建哈希表\n        for (auto i : nums) used[i] = false;\n        int longest = 0;\n        for (auto i : nums){  // 利用auto i : nums遍历nums中的值\n            if (used[i]) continue;\n            int length = 1;\n            used[i] = true;\n            for (int j = i + 1; used.find(j) != used.end(); ++j){\n                used[j] = true;\n                ++length;\n            }\n            for (int j = i - 1;used.find(j) != used.end(); --j){\n                used[j] = true;\n                ++length;\n            }\n            longest = max(longest, length);\n        }\n        return longest;\n    }\n};\n\n\nclass Solution:\n    def longestConsecutive(self, nums: List[int]) -&gt; int:\n        used = dict();\n        for i in nums:\n            used[i] = False\n        longest = 0\n        for i in nums:\n            if used[i]: continue\n            length = 1\n            used[i] = True\n            j = i + 1\n            while j in used:\n                used[j] = True\n                length += 1\n                j += 1\n            j = i - 1\n            while j in used:\n                used[j] = True\n                length += 1\n                j -= 1\n            longest = max(length, longest)\n        return longest\n        \n\n\n\n\n\n2 两数之和\n\n\n\n\n\n\n\n分析\n\n\n\n为了尽量使时间复杂度较低，现分析几种方法：\n\n暴力枚举法(\\(C_n^2\\to O(n^2)\\))\n先排序(\\(O(n\\log n)\\))后夹逼(\\(O(n)\\))，最终 \\(O(n\\log n)\\)\n哈希：用一个哈希表，存储每个数对应的下标，复杂度 \\(O(n)\\)\n\n\n\n\n伪代码PythonC++\n\n\n该题较为简单暂不书写伪代码😃\n\n\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:\n        used = dict()\n        for i in range(len(nums)): used[nums[i]] = i  # 此处便保证了仅有较后的索引会留下在返回值时无需再判断索引的大小关系\n        for i, num in enumerate(nums):\n            remain = target - num\n            if remain in used and used[remain] &gt; i:\n                return [i, used[remain]]\n\n\nclass Solution {\npublic:\n    vector&lt;int&gt; twoSum(vector&lt;int&gt;& nums, int target) {\n        unordered_map&lt;int,int&gt; mapping;\n        vector&lt;int&gt; result;\n        for (int i = 0; i &lt; nums.size(); i++) mapping[nums[i]] = i;\n        for (int i = 0; i &lt; nums.size(); i++){\n            int gap = target - nums[i];\n            if (mapping.find(gap) != mapping.end() && mapping[gap] &gt; i){\n                result.push_back(i);\n                result.push_back(mapping[gap]);\n            }\n        }\n        return result;\n\n    }\n};\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[王推]S4/index.html",
    "href": "posts/[王推]S4/index.html",
    "title": "S4.离散特征处理",
    "section": "",
    "text": "离散特征的取值是实数域上间断的，可以通过建立字典和向量化两步进行处理。"
  },
  {
    "objectID": "posts/[王推]S4/index.html#one-hot编码",
    "href": "posts/[王推]S4/index.html#one-hot编码",
    "title": "S4.离散特征处理",
    "section": "2.1 One-hot编码",
    "text": "2.1 One-hot编码\n用2维向量表示性别，有多少取值就有多少维度。如下例：\n\n\n\nOne-hot编码示例\n\n\n\n\n\n\n\n\n注意\n\n\n\n当类别数量过大时，不宜采用one-hot编码"
  },
  {
    "objectID": "posts/[王推]S4/index.html#embeding嵌入",
    "href": "posts/[王推]S4/index.html#embeding嵌入",
    "title": "S4.离散特征处理",
    "section": "2.2 Embeding嵌入",
    "text": "2.2 Embeding嵌入\n以国籍的Embeding嵌入为例：\n\n\n\n国籍Embeding编码方式\n\n\n参数数量为：向量维度 \\(\\times\\) 类别数量\n\n\n\n\n\n\nEmbeding与one-hot编码的联系\n\n\n\nEmbedding = 参数矩阵 \\(\\times\\) one-hot向量\n\n\n\nEmbeding与one-hot编码的联系图\n\n\n类别数量较大时用Embeding，如Word Embeding和ID Embeding"
  },
  {
    "objectID": "posts/[hot100]Day6/index.html",
    "href": "posts/[hot100]Day6/index.html",
    "title": "Day6",
    "section": "",
    "text": "1 三数之和\n\n\n\n\n\n\n\n分析\n\n\n\n\n\n\n\n伪代码PythonC++\n\n\n\n\n\\begin{algorithm} \\caption{threeSum(nums)} \\begin{algorithmic} \\State List[List[int]] result \\If{nums.length&lt;3} \\State \\Return result\\Comment{当数组长度不足3时不可能有满足情况的组合} \\EndIf \\State sort(nums)\\Comment{对nums进行排序} \\State target=0\\Comment{可进行扩展} \\For{i=0\\To nums.length-2}\\Comment{保证有i,j,k的空间} \\State j = i + 1 \\State k = nums.length - 1 \\If{i&gt;0 \\And nums[i]==nums[i-1]} \\Continue \\EndIf \\While{j&lt;k}\\Comment{遍历i某值下的所有情况} \\If{nums[i]+nums[j]+nums[k]&lt;target} \\State j = j + 1 \\While{nums[j]==nums[j-1]\\And j&lt;k} \\State j = j + 1 \\EndWhile \\Elif{nums[i]+nums[j]+nums[k]&lt;target} \\State k = k -1 \\While{nums[k]==nums[k+1]\\And j&lt;k} \\State k = k - 1 \\EndWhile \\Else \\State nums.append([nums[i],nums[j],nums[k]]) \\State j = j + 1 \\State k = k - 1 \\While{nums[j]==nums[j-1]\\And nums[k]==nums[k+1]\\And j&lt;k} \\State j = j + 1\\Comment{j,k不一起蹦的原因，当更新后落入这个while循环假，设j,k之间仅有一个元素，且这个元素对于j而言是新值，一旦二者同时变动j,k将会同时指向相同的元素而跳出循环，造成缺少含有这个的组合(1,11,15)[(10,11,15)j=10,k=15,i=1]} \\EndWhile \\EndIf \\EndWhile \\EndFor \\Return result \\end{algorithmic} \\end{algorithm}\n\n\n\n\nclass Solution:\n    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:\n        result = []\n        if len(nums) &lt; 3:\n            return result\n        target = 0\n        nums.sort()\n        for i in range(len(nums)-2):\n            if i &gt; 0 and nums[i] == nums[i-1]: continue\n            j = i + 1\n            k = len(nums) - 1\n            while j &lt; k:\n                if (nums[i] +  nums[j] + nums[k] &lt; target):\n                    j += 1\n                    while nums[j] == nums[j-1] and j &lt; k: j += 1\n                elif (nums[i] + nums[j] + nums[k] &gt; target):\n                    k -= 1\n                    while nums[k] == nums[k+1] and j &lt; k: k -= 1\n                else:\n                    result.append([nums[i], nums[j], nums[k]])\n                    j += 1\n                    k -= 1\n                    while (nums[j] == nums[j-1] and nums[k] == nums[k+1] and j &lt; k):\n                        j += 1\n        return result\n\n\nclass Solution {\npublic:\n    vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;& nums) {\n        vector&lt;vector&lt;int&gt;&gt; result;\n        if (nums.size() &lt; 3) return result;\n        sort(nums.begin(), nums.end());\n        auto last = nums.end();\n        const int target = 0;\n        for (auto i = nums.begin(); i &lt; last - 2; i++){\n            auto j = i + 1;\n            if (i &gt; nums.begin() && *i == *(i-1)) continue;\n            auto k = last - 1;\n            while (j &lt; k){\n                if (*i + *j + *k &lt; target){\n                    j++;\n                    while (*j == *(j-1) && j &lt; k) j++;\n                }\n                else if (*i + *j + *k &gt; target){\n                    k--;\n                    while (*k == *(k+1) && j &lt; k) k--;\n                }\n                else {\n                    result.push_back({*i, *j, *k});\n                    j++;\n                    k--;\n                    while (*j == *(j-1) && *k==*(k+1) && j &lt; k) j++;\n                }\n            }\n        }\n        return result;\n    }\n};\n\n\n\n\n\n2 最接近的三数之和\n\n\n\n\n\n\n\n分析\n\n\n\n该题较上题而言十分相似，也可以先排序后左右夹逼，但有以下区别：\n\n需要两个记录变量一个记录元素和，另一个记录绝对差\n指针的跳动规则也存在不同，当和超过target时k向左跳动，小于则j向右跳动\n\n\n\n\n伪代码PythonC++\n\n\n\n\n\\begin{algorithm} \\caption{ThreeSumClosest(nums, target)} \\begin{algorithmic} \\State \\textbf{输入:} 数组 nums，整数 target \\State \\textbf{输出:} 最接近 target 的三个整数之和 \\State 初始化 closest = $\\infty$ \\Comment{用于记录当前最接近的差值} \\State 初始化 sum\\_r = $\\infty$ \\Comment{用于记录当前最接近的三元组和} \\State Sort(nums) \\Comment{对数组进行排序，以便使用双指针技巧} \\For{$i = 0$ \\textbf{到} $len(nums) - 2$} \\If{$i &gt; 0$ \\textbf{且} $nums[i] == nums[i-1]$} \\State \\Continue \\Comment{跳过重复值，避免重复计算} \\EndIf \\State 初始化 $j = i + 1$ \\Comment{左指针} \\State 初始化 $k = len(nums) - 1$ \\Comment{右指针} \\While{$j &lt; k$} \\State 计算 $sum\\_i = nums[i] + nums[j] + nums[k]$ \\Comment{当前三元组的和} \\State 计算 $closest\\_i = |sum\\_i - target|$ \\Comment{当前三元组和与目标值的差值} \\If{$closest\\_i &lt; closest$} \\State 更新 $sum\\_r = sum\\_i$ \\Comment{更新最接近的三元组和} \\State 更新 $closest = closest\\_i$ \\Comment{更新最接近的差值} \\EndIf \\If{$sum\\_i &lt; target$} \\State $j = j + 1$ \\Comment{如果当前和小于目标值，移动左指针增加和} \\Else \\State $k = k - 1$ \\Comment{如果当前和大于目标值，移动右指针减小和} \\EndIf \\EndWhile \\EndFor \\State \\Return $sum\\_r$ \\Comment{返回最接近的三元组和} \\end{algorithmic} \\end{algorithm}\n\n\n\n\nclass Solution:\n    def threeSumClosest(self, nums: List[int], target: int) -&gt; int:\n        closest_r = float(\"inf\")\n        sum_r = float(\"inf\")\n        nums.sort()\n        for i in range(len(nums) - 2):\n            if i &gt; 0 and nums[i] == nums[i-1]: continue\n            j = i + 1\n            k = len(nums) - 1\n            while j &lt; k:\n                sum_i = nums[i] + nums[j] + nums[k]\n                closest_i =  abs(sum_i - target)\n                if closest_i &lt; closest_r:\n                    sum_r = sum_i\n                    closest_r = closest_i\n                if sum_i &lt; target: j += 1\n                else: k -= 1\n        return sum_r\n\n\nclass Solution {\npublic:\n    int threeSumClosest(vector&lt;int&gt;& nums, int target) {\n        int sum_r = INT_MAX;\n        int closest_r = INT_MAX;\n        sort(nums.begin(), nums.end());\n        auto last = nums.end();\n        for (auto i = nums.begin(); i &lt; last - 2; i++){\n            if (i &gt; nums.begin() && *i == *(i-1)) continue;\n            auto j = i + 1;\n            auto k = last - 1;\n            while (j &lt; k){\n                auto sum_i = *i + *j + *k;\n                auto closest_i = abs(sum_i - target);\n                if (closest_i &lt; closest_r){\n                    closest_r = closest_i;\n                    sum_r = sum_i;\n                }\n                if (sum_i &lt; target) j++;\n                else k--;\n            }\n        }\n        return sum_r;\n    }\n};\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[hot100]Day8/index.html",
    "href": "posts/[hot100]Day8/index.html",
    "title": "Day8",
    "section": "",
    "text": "今天主要涉及康托编码的原理、\n\n\n康托展开式全排列与自然数的双射，常用于空间压缩，本质上是计算当前排列在所有由小到大全排列中的顺序，因此可逆。\n\n\n求34152在全排列中的位置\n\n第一位小于3的所有数有：\\(2\\times 4!\\)\n第2位小于4的所有数：\\(2\\times 3!\\)(3由于被第一位占了所以是 \\(2\\times\\))\n第三位小于1的所有数：无\n第4位小于5的所有数：\\(1\\times 1!\\)\n第5位小于2的所有数：\\(0\\)\n\n则其编号为 \\(2 \\times 4! + 2 \\times 3! + 0 \\times 2! + 1 \\times 1! + 0 \\times 0! = 61\\)\n\n\n\n编号X=61\n\n第1位：\\(\\frac{61}{4} = 2 \\cdots \\cdots 13\\)，说明比第1位小的有2个，ans[0]=3\n第2位：\\(\\frac{13}{3} = 2 \\cdots \\cdots 1\\)，说明比第2位小的有2个，a[1]=4\n第3位：\\(\\frac{1}{2} = 0 \\cdots \\cdots 1\\)，说明比第3位小的有0个，ans[2]=1\n第4位：\\(\\frac{1}{1} = 1 \\cdots \\cdots 0\\)，说明比第4位小的有1个，ans[3]=5\n第5位：剩下2，ans[4]=2"
  },
  {
    "objectID": "posts/[hot100]Day8/index.html#康托编码",
    "href": "posts/[hot100]Day8/index.html#康托编码",
    "title": "Day8",
    "section": "",
    "text": "康托展开式全排列与自然数的双射，常用于空间压缩，本质上是计算当前排列在所有由小到大全排列中的顺序，因此可逆。\n\n\n求34152在全排列中的位置\n\n第一位小于3的所有数有：\\(2\\times 4!\\)\n第2位小于4的所有数：\\(2\\times 3!\\)(3由于被第一位占了所以是 \\(2\\times\\))\n第三位小于1的所有数：无\n第4位小于5的所有数：\\(1\\times 1!\\)\n第5位小于2的所有数：\\(0\\)\n\n则其编号为 \\(2 \\times 4! + 2 \\times 3! + 0 \\times 2! + 1 \\times 1! + 0 \\times 0! = 61\\)\n\n\n\n编号X=61\n\n第1位：\\(\\frac{61}{4} = 2 \\cdots \\cdots 13\\)，说明比第1位小的有2个，ans[0]=3\n第2位：\\(\\frac{13}{3} = 2 \\cdots \\cdots 1\\)，说明比第2位小的有2个，a[1]=4\n第3位：\\(\\frac{1}{2} = 0 \\cdots \\cdots 1\\)，说明比第3位小的有0个，ans[2]=1\n第4位：\\(\\frac{1}{1} = 1 \\cdots \\cdots 0\\)，说明比第4位小的有1个，ans[3]=5\n第5位：剩下2，ans[4]=2"
  },
  {
    "objectID": "posts/[hot100]Day7/index.html",
    "href": "posts/[hot100]Day7/index.html",
    "title": "Day7",
    "section": "",
    "text": "1 四数之和\n\n\n\n\n\n\n\n分析\n\n\n\n与三数之和的方法类似\n\n\n\n伪代码PythonC++\n\n\n由于与三数之和很类似故此处不写😎\n\n\nclass Solution:\n    def fourSum(self, nums: List[int], target: int) -&gt; List[List[int]]:\n        nums.sort()  # 对数组进行排序O(log(n))\n        result = []\n        for i in range(len(nums) - 3):\n            if i &gt; 0 and nums[i] == nums[i-1]: continue  # 保证不再遍历之前的值\n            for j in range(i + 1, len(nums) - 2):\n                if j &gt; i + 1 and nums[j] == nums[j-1]: continue  # 保证不在遍历之前的值\n                k = j + 1\n                m = len(nums) - 1\n                while k &lt; m:\n                    if nums[i] + nums[j] + nums[k] + nums[m] &lt; target:\n                        k += 1\n                        while nums[k] == nums[k-1] and k &lt; m: k += 1\n                    elif nums[i] + nums[j] + nums[k] + nums[m] &gt; target:\n                        m -= 1\n                        while nums[m] == nums[m+1] and k &lt; m: m -= 1\n                    else:\n                        result.append([nums[i], nums[j], nums[k], nums[m]])\n                        k += 1\n                        m -= 1\n                        while nums[k] == nums[k-1] and nums[m] == nums[m+1] and k &lt; m: k += 1\n        return result\n\n\nclass Solution {\npublic:\n    vector&lt;vector&lt;int&gt;&gt; fourSum(vector&lt;int&gt;& nums, int target) {\n        vector&lt;vector&lt;int&gt;&gt; result;\n        sort(nums.begin(), nums.end());\n        auto last = nums.end();\n        for (auto i = nums.begin(); i &lt; last - 3; i++){\n            if (i &gt; nums.begin() && *i == *(i-1)) continue;\n            if ((long) *i + *(i + 1) + *(i + 2) + *(i + 3) &gt; target) break;  // 最小的三个数都没target大的话这个数组就不会有了\n            if ((long) *i + *(last - 1) + *(last - 2) + *(last - 3) &lt; target) continue;  // 目前跌迭代值和最大的三个数都没有target大的话此次迭代就不会有了\n            for (auto j = i + 1; j &lt; last - 2; j++){\n                if (j &gt; i + 1 && *j == *(j-1)) continue;\n                if ((long) *i + *j + *(j + 1) + *(j + 2) &gt; target) break;\n                if ((long) *i + *j + *(last - 1) + *(last - 2) &lt; target) continue;\n                auto k = j + 1;\n                auto m = nums.end() - 1;\n                while (k &lt; m){\n                    if ((long) *i + *j + *k + *m &lt; target){\n                        k++;\n                        while (*k == *(k - 1) && k &lt; m) k++;\n                    }\n                    else if ((long) *i + *j + *k + *m &gt; target){\n                        m--;\n                        while (*m == *(m + 1) && k &lt; m) m--;\n                    }\n                    else{\n                        result.push_back({*i, *j, *k, *m});\n                        k++;\n                        m--;\n                        while (*k == *(k - 1) && *m == *(m + 1) && k &lt; m) k++;\n                    }\n                }\n            }\n        }\n        return result;\n    }\n};\n\n\n\n\n\n2 移除元素\n\n\n\n\n\n\n\n分析\n\n\n\n只要是原地就可以想想双指针，思路类似Day1的\n\n\n\n伪代码PythonC++\n\n\n简单，😎\n\n\nclass Solution:\n    def removeElement(self, nums: List[int], val: int) -&gt; int:\n        index = 0\n        for i in range(len(nums)):\n            if nums[i] != val:\n                nums[index] = nums[i]\n                index += 1\n        return index\n\n\nclass Solution {\npublic:\n    int removeElement(vector&lt;int&gt;& nums, int val) {\n        int index = 0;\n        auto last = nums.end();\n        for (auto i = nums.begin(); i &lt; last; i++){\n            if (*i != val) nums[index++] = *i;\n        }\n        return index;\n    }\n};\n\n\n\n\n\n3 下一个排列\n\n\n\n\n\n\n\n分析\n\n\n\n别分析了，这个算法和开根号的算法类似，暂时不好解释，故了解算法即可：\n\n从右往左找到第一个非降序的元素a\n从右往左找到第一个比元素a大的数b\n将a，b进行交换位置，而后反转数a原来位置后元素的位置\n\n\n\n\nPythonC++\n\n\nclass Solution:\n    def nextPermutation(self, nums: List[int]) -&gt; None:\n        \"\"\"\n        Do not return anything, modify nums in-place instead.\n        \"\"\"\n        a = []; b=[]\n        for i in range(len(nums)-2, -1, -1):\n            # 找a\n            if nums[i] &lt; nums[i + 1]:\n                a = a + [i, nums[i]]\n                break\n        if a:      \n            for i in range(len(nums)-1, a[0], -1):\n                # 找b\n                if nums[i] &gt; a[1]:\n                    b = b + [i, nums[i]]\n                    break\n            nums[a[0]] = b[1]\n            nums[b[0]] = a[1]\n            nums[a[0]+1:] = sorted(nums[a[0] + 1:])\n        else:\n            nums.sort()\n\n\nclass Solution {\npublic:\n   void nextPermutation(vector&lt;int&gt;& nums) {\n        nextPermutation(nums.begin(), nums.end());\n   }\n\n    // 模板函数BidIt用于占位如vector，list等可双向跌打的\n    template&lt;typename BidiIt&gt;\n    bool nextPermutation(BidiIt first, BidiIt last) {\n        const auto rfirst = reverse_iterator&lt;BidiIt&gt;(last);  // 创建反向迭代器\n        const auto rlast = reverse_iterator&lt;BidiIt&gt;(first);\n\n        auto pivot = next(rfirst);\n        while (pivot != rlast && *pivot &gt;= *prev(pivot)) ++pivot;\n        if (pivot == rlast) {\n            reverse(rfirst, rlast);\n            return false;\n        }  // 当数组本身以全部逆序，则下一个排列只需逆序即可\n\n        auto change = find_if(rfirst, pivot, [&pivot](const auto& elem) {\n            return elem &gt; *pivot;});  // lambda函数,find_if函数用于指定搜索范围和搜索条件\n        swap(*change, *pivot);\n        reverse(rfirst, pivot);\n        return true;\n    }\n\n};\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[小项目]降雨预测/index.html",
    "href": "posts/[小项目]降雨预测/index.html",
    "title": "降雨二元预测",
    "section": "",
    "text": "1 分析\n\n不知到该城市是位于南半球还是北半球，且无法确定是否为同一个城市。\n降雨在四季拥有不同的机制，应当依据不同的季节分别训练四个模型。（LS-PLM)\n是否有不同年份降雨机制上的差异？\n\n\n\n2 数据可视化\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[Cpp]特1/index.html",
    "href": "posts/[Cpp]特1/index.html",
    "title": "C++模板参数、字符串的处理",
    "section": "",
    "text": "1）string s(n, \"0\")：该语句的本质上是创建一个string类型的变量并且设置长度为n，默认值都为”0”。类似的还有std::vector；std::deque；std::list；std:array&lt;T,N&gt;（必须在创建后立马进行初始化）。\n2）T用作类型参数的占位符，表示一个可以代表任何类型的通用类型。\n1template &lt;typename Sequence&gt;\n2Sequence kth_permutation(const Sequence& seq, int k) {\n    const int n = seq.size(); \n    Sequence S(seq);\n    Sequence result;\n    int base = factorial(n - 1);\n    --k;\n    for (int i = n - 1; i &gt; 0; k %= base, base /= i, --i) {\n3        auto a = next(S.begin(), k / base);\n        result.push_back(*a);\n4        S.erase(a);\n    }\n    result.push_back(S[0]);\n    return result;\n}\n\n1\n\n表示接下来的函数或类是一个模板，在此处的Sequence表示任何迭代器（str::string、std::vector等）\n\n2\n\nconst Sequence& 表示常量引用即不能通过该引用修改变量的值\n\n3\n\n产生一个迭代器\n\n4\n\n删除指定位置上的元素\n\n\n3）bool used[9]：创建一个长度为9的布尔数组；used 在大多数表达式中等价于 &used[0]，即指向 used 数组第一个元素的指针。\n4）fill(used, used+9, false)：为used首个元素和第9个元素赋值为false。\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[组会]1/index.html#ls-plm模型",
    "href": "posts/[组会]1/index.html#ls-plm模型",
    "title": "近期汇报",
    "section": "1.1 LS-PLM模型",
    "text": "1.1 LS-PLM模型\n\n\n\nLS-PLM模型示意图\n\n\nLS-PLM的模型采用了分而治之的策略，其有以下优点：\n\n可获取非线性关系\n可扩展性，适应大规模数据\n稀疏性\n\n\n\n[1] GAI K, ZHU X, LI H, 等. Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction[A/OL]. arXiv, 2017[2025-03-05]. http://arxiv.org/abs/1704.05194. DOI:10.48550/arXiv.1704.05194."
  },
  {
    "objectID": "posts/[组会]1/index.html",
    "href": "posts/[组会]1/index.html",
    "title": "近期汇报",
    "section": "",
    "text": "LS-PLM模型示意图\n\n\nLS-PLM的模型采用了分而治之的策略，其有以下优点：\n\n可获取非线性关系\n可扩展性，适应大规模数据\n稀疏性\n\n\n\n[1] GAI K, ZHU X, LI H, 等. Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction[A/OL]. arXiv, 2017[2025-03-05]. http://arxiv.org/abs/1704.05194. DOI:10.48550/arXiv.1704.05194.\n\n\n\n已知有如下数据集 \\(\\{x_t,y_t\\}|_{t=1}^n,y_t\\in {0,1},x_t\\in\\mathbb{R}^d\\)，则LS-PLM模型的一般形式为：\n\\[\np(y=1|x) = g\\left(\\sum_{j=1}^{m} \\sigma(u_j^T x) \\eta(w_j^T x)\\right)\n\\]\n记模型参数为 \\(\\Theta=\\{u_1,\\ldots,u_m,w_1,\\ldots,w_m\\}\\in R^{d\\times 2m}\\)，当聚类函数选取Softmax函数时，拟合使用LR模型，则有如下模型：\n\\[\np(y = 1|x) = \\sum_{i=1}^{m} \\frac{\\exp(u_i^T x)}{\\sum_{j=1}^{m} \\exp(u_j^T x)} \\cdot \\frac{1}{1 + \\exp(-w_i^T x)}\n\\]\n损失函数采用交叉熵损失函数:\n\\[\n\\text{loss}(\\Theta) = -\\sum_{t=1}^{n} \\left[ y_t \\log \\left( p(y_t=1|x_t, \\Theta) \\right) + (1 - y_t) \\log \\left( p(y_t=0|x_t, \\Theta) \\right) \\right]\n\\]\n为了模型的泛化能力和参数的稀疏性（便于实际部署），故有如下优化问题：\n\\[\n\\arg\\min_{\\Theta} f(\\Theta) = \\text{loss}(\\Theta) + \\lambda \\|\\Theta\\|_{2,1} + \\beta \\|\\Theta\\|_1\n\\tag{1}\\]\n\\(L_{2,1}\\) 范数定义如下：\n\\[\n\\|A\\|_{2,1} = \\sum_{i=1}^m \\sqrt{\\sum_{j=1}^n A_{ij}^2}\n\\]\n\n\\(L_{2,1}\\) 范数诱导了行的稀疏性，让不重要的特征前参数为0\n\\(L_1\\) 范数进一步诱导那些靠近0的参数变为0\n\n\n\n\n\n由下图易得 \\(L_1,L_{2,1}\\) 范数均非光滑\n\n\\(\\sqrt{x^2+y^2}\\)\\(|x|+|y|\\)\n\n\n\n\n                                                \n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n由于正则项的非光滑和Softmax函数捕捉非线性关系的能力导致目标函数非光滑且非凸，因此常用的传统办法如梯度下降算法、EM算法便难以使用了。\n这篇论文对于解决 Equation 1 方法是用使得方向导数最小的方向替代梯度来进行优化，先声明如下公式：\n\n右偏导\n\n\\[\n\\partial^+_{ij} f(\\Theta) = \\lim_{\\alpha \\to 0^+} \\frac{f(\\Theta + \\alpha e_{ij}) - f(\\Theta)}{\\alpha}\n\\]\n\n方向导数\n\n\\[\nf'(\\Theta; d) = \\lim_{\\alpha \\to 0^+} \\frac{f(\\Theta + \\alpha d) - f(\\Theta)}{\\alpha}\n\\tag{2}\\]\n如果方向 \\(d\\) s.t. \\(f'(\\Theta;d)&lt;0\\)，那么 \\(d\\) 被认为为下降方向。\n\n正交投影函数\n\n\\[\n\\pi_{ij}(\\Theta; \\Omega) = \\begin{cases}\n\\Theta_{ij} & , \\quad \\text{sign}(\\Theta_{ij}) = \\text{sign}(\\Omega_{ij}) \\\\\n0 & , \\quad \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n\nTheorem 1 (Equation 2 方向导数处处存在) Equation 1 的方向导数为：\n\\[\nf'(\\Theta;d)=\\nabla \\text{loss}(\\Theta)^Td+\\sum_{||\\Theta_{i\\cdot}||_{2,1}\\neq 0}\\lambda\\frac{\\Theta'_{i\\cdot}d_{i\\cdot}}{||\\Theta_{i\\cdot}||_{2,1}}+\\sum_{||\\Theta_{i\\cdot}||_{2,1}= 0}\\lambda||d_{i\\cdot}||_{2,1}+\\sum_{||\\Theta_{ij}||\\neq 0}\\beta\\text{sign}(\\Theta_{ij})d_{ij}+\\sum_{||\\Theta_{ij}||= 0}\\beta|d_{ij}|\n\\]\n\n\n\n\n\n\n\n\n方向导数的证明\n\n\n\n当得到方向导数后便可寻找d s.t. \\(f'(\\Theta,d)\\) 最小作为下降方向，也即是有如下优化问题：（约束条件是为了限制d的长度）\n\\[\n\\min_{d}f'(\\Theta;d)\\qquad \\text{s.t.} ||d||^2\\leq C\n\\]\n上式是一个不等式约束问题，为了求解可利用拉格朗日函数转换为它的对偶问题：\n\\[\n\\begin{aligned}\n&\\max_{\\mu}\\min_{d}L(d,\\mu)\\\\\n\\text{s.t.}& \\mu \\ge 0\\\\\n&L(d,\\mu)=f'(\\Theta;d)+\\mu(||d||^2-C)\n\\end{aligned}\n\\]\n易得拉格朗日函数 \\(L(d,\\mu)\\) 是 \\(d\\) 的凸函数，因此原始问题的解与对偶问题的解是等价的。\n令拉格朗日函数的偏导为0，则有：\n\\[\ns=-\\nabla \\text{loss}(\\Theta)_{ij}-\\lambda \\frac{\\Theta_{ij}}{||\\Theta_{i\\cdot}||_{2,1}}\n\\]\n\n当 \\(\\Theta_{ij}\\neq 0\\)\n\n\\[\n2\\mu d_{ij}=s-\\beta\\text{sign}(\\Theta_{ij})\n\\]\n\n\n当 \\(\\Theta_{ij}=0\\) 且 \\(||\\Theta_{i\\cdot}||_{2,1}\\neq 0\\)\n\n\\[\n2\\mu d_{ij}=\\max\\{|s| - \\beta, 0\\} \\text{sign}(s)\n\\]\n\n\n\n第二种情况下\\(d_{ij}\\)的计算\n\n\n\n\n当 \\(||\\Theta_{i\\cdot}||_{2,1}= 0\\)\n\n\\[\n2\\mu d_{ij}=\\frac{\\max\\{\\|v\\|_{2,1} - \\lambda, 0\\}}{\\|v\\|_{2,1}} v\n\\]\n\\[\nv = \\max\\{|-\\nabla loss(\\Theta)_{ij}| - \\beta, 0\\} \\text{sign}(-\\nabla loss(\\Theta)_{ij})\n\\]\n\n\n\n\\(d_{ij}\\)的第3种情况证明"
  },
  {
    "objectID": "posts/[组会]1/index.html#模型表达式",
    "href": "posts/[组会]1/index.html#模型表达式",
    "title": "近期汇报",
    "section": "1.1 模型表达式",
    "text": "1.1 模型表达式\n已知有如下数据集 \\(\\{x_t,y_t\\}|_{t=1}^n,y_t\\in {0,1},x_t\\in\\mathbb{R}^d\\)，则LS-PLM模型的一般形式为：\n\\[\np(y=1|x) = g\\left(\\sum_{j=1}^{m} \\sigma(u_j^T x) \\eta(w_j^T x)\\right)\n\\]\n记模型参数为 \\(\\Theta=\\{u_1,\\ldots,u_m,w_1,\\ldots,w_m\\}\\in R^{d\\times 2m}\\)，当聚类函数选取Softmax函数时，拟合使用LR模型，则有如下模型：\n\\[\np(y = 1|x) = \\sum_{i=1}^{m} \\frac{\\exp(u_i^T x)}{\\sum_{j=1}^{m} \\exp(u_j^T x)} \\cdot \\frac{1}{1 + \\exp(-w_i^T x)}\n\\qquad(1)\\]\n损失函数采用交叉熵损失函数:\n\\[\n\\text{loss}(\\Theta) = -\\sum_{t=1}^{n} \\left[ y_t \\log \\left( p(y_t=1|x_t, \\Theta) \\right) + (1 - y_t) \\log \\left( p(y_t=0|x_t, \\Theta) \\right) \\right]\n\\]\n为了模型的泛化能力和参数的稀疏性（便于实际部署），故有如下优化问题：\n\\[\n\\arg\\min_{\\Theta} f(\\Theta) = \\text{loss}(\\Theta) + \\lambda \\|\\Theta\\|_{2,1} + \\beta \\|\\Theta\\|_1\n\\qquad(2)\\]\n\\(L_{2,1}\\) 范数定义如下：\n\\[\n\\|A\\|_{2,1} = \\sum_{i=1}^m \\sqrt{\\sum_{j=1}^n A_{ij}^2}\n\\]\n\n\\(L_{2,1}\\) 范数诱导了行的稀疏性，让不重要的特征前参数为0\n\\(L_1\\) 范数进一步诱导那些靠近0的参数变为0"
  },
  {
    "objectID": "posts/[组会]1/index.html#目标函数的优化",
    "href": "posts/[组会]1/index.html#目标函数的优化",
    "title": "近期汇报",
    "section": "1.2 目标函数的优化",
    "text": "1.2 目标函数的优化\n由于正则项的非光滑和Softmax函数捕捉非线性关系的能力导致目标函数非光滑且非凸，因此常用的传统办法如梯度下降算法、EM算法便难以使用了。\n这篇论文对于解决 Equation 2 方法是用使得方向导数最小的方向替代梯度来进行优化，先声明如下公式：\n\n右偏导\n\n\\[\n\\partial^+_{ij} f(\\Theta) = \\lim_{\\alpha \\to 0^+} \\frac{f(\\Theta + \\alpha e_{ij}) - f(\\Theta)}{\\alpha}\n\\]\n\n方向导数\n\n\\[\nf'(\\Theta; d) = \\lim_{\\alpha \\to 0^+} \\frac{f(\\Theta + \\alpha d) - f(\\Theta)}{\\alpha}\n\\qquad(3)\\]\n如果方向 \\(d\\) s.t. \\(f'(\\Theta;d)&lt;0\\)，那么 \\(d\\) 被认为为下降方向。\n\n正交投影函数\n\n\\[\n\\pi_{ij}(\\Theta; \\Omega) = \\begin{cases}\n\\Theta_{ij} & , \\quad \\text{sign}(\\Theta_{ij}) = \\text{sign}(\\Omega_{ij}) \\\\\n0 & , \\quad \\text{otherwise}\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/[组会]1/index.html#工业实现",
    "href": "posts/[组会]1/index.html#工业实现",
    "title": "近期汇报",
    "section": "1.3 工业实现",
    "text": "1.3 工业实现\n\n\n\n\n\n1）图A)物理拓扑结构的优点\n\n上述图A)中的物理拓扑结构可以最大化CPU计算能力的利用率：传统的一个计算节点只会由服务节点或者工作节点，不会同时存在服务节点和工作节点，当服务节点不进行计算密集型任务时就会动态分配CPU资源给工作节点，而不像传统计算节点那样闲置CPU资源。\n最大化内存利用率：和上面类似，当两种节点同属在一个计算节点中便可共享内存，提高内存利用率。\n\n2）图B)数据并行和模型并行的方法\n\n将所有的数据集分成n份，每个工作节点仅存储一份。\n服务器节点存储模型的参数，但各服务器存储的参数互斥即一个参数只能存储在一个服务节点中。\n计算步骤：\n\n首先各工作节点通过自己的部分数据集训练出一个局部模型，当需要模型目前参数值时便与服务节点通信，此时工作节点会计算各自的损失和下降方向 \\(p\\)，而后各工作节点会将这些信息Push给服务节点。\n服务节点经过汇总上述信息更新参数，并将更新后的参数值广播到各工作节点。\n\n\n\n\n\n\n\ngraph LR\nA([工作节点局部计算]) --&gt; B[服务器节点下降方向、损失聚合]\nB --&gt; C([服务器节点参数更新])\nC --&gt; D(服务器节点向工作节点进行参数同步)"
  },
  {
    "objectID": "posts/[组会]1/index.html#共同特征技巧",
    "href": "posts/[组会]1/index.html#共同特征技巧",
    "title": "近期汇报",
    "section": "1.4 共同特征技巧",
    "text": "1.4 共同特征技巧\n\nEquation 1 中计算最复杂的部分是 \\(\\mu_i^Tx\\) 和 \\(w_i^Tx\\)，而共同技巧则将特征向量进行拆分为共同特征和非共同特征（类似因子分析中的将特征分为公共因子和特殊因子）：\n\\[\n\\begin{aligned}\nμ _ {i} x =& μ _ {i,c} x _ {c} + μ _ {i,nc} x _ {nc}\\\\\nw _ {i} x =& w _ {i,c} x _ {c} + w _ {i,nc} x _ {nc}\\\\\n\\end{aligned}\n\\]\n则在工业实现中运用共同特征技巧需要满足如下方面：\n\n确保拥有相同公共特征的样本处于同一个工作节点内\n公共特征仅存储一次\n对相同特征前的系数仅更新一次方向"
  },
  {
    "objectID": "posts/[组会]1/index.html#效果",
    "href": "posts/[组会]1/index.html#效果",
    "title": "近期汇报",
    "section": "1.5 效果",
    "text": "1.5 效果\n\nLS-PLM 模型在不同分片数中的效果正则项的效果共同特征技巧的效果"
  },
  {
    "objectID": "posts/[组会]1/index.html#哈希表",
    "href": "posts/[组会]1/index.html#哈希表",
    "title": "近期汇报",
    "section": "2.1 哈希表",
    "text": "2.1 哈希表\n主要用于查找，在理想情况下，查找的时间复杂度是 \\(O(1)\\) 的。\n2.1.1 哈希函数的实现方法\n在理想状态下，哈希函数会将数据均匀散列分布在哈希表中。主要有直接定址法、除留余数法等。\n\n直接定址法除留余数法\n\n\n当数据的范围比较集中是，优先考虑使用定址法。其主要实现方法类似一种键值对，键为数据的值，值为数据出现的次数。\n该方法的缺陷是当数据比较分散且范围较大时，就造成空间浪费。\n\n\n\n\n\n\n\n其核心思想是将数据对某个数（一般是表长）进行求余运算，将运算结果作为索引值，即表示该数据存放的下标位置。假设某哈希表的大小为M，则哈希函数为 \\(\\text{hash}=x \\% M\\)\n此时应当注意：\n\n为了尽量减少哈希冲突出现的次数，建议M取不太接近2的整数次幂的一个素数。\n除留余数法中，为了方便数据转化为索引值，要求哈希表的数据元素支持转换为整数。"
  },
  {
    "objectID": "posts/[组会]1/index.html#康托编码",
    "href": "posts/[组会]1/index.html#康托编码",
    "title": "近期汇报",
    "section": "2.2 康托编码",
    "text": "2.2 康托编码\n康托展开式全排列与自然数的双射，常用于空间压缩，本质上是计算当前排列在所有由小到大全排列中的顺序，因此可逆。\n2.2.1 排列 \\(\\to\\) 自然数\n求34152在全排列中的位置\n\n第一位小于3的所有数有：\\(2\\times 4!\\)\n第2位小于4的所有数：\\(2\\times 3!\\)(3由于被第一位占了所以是 \\(2\\times\\))\n第三位小于1的所有数：无\n第4位小于5的所有数：\\(1\\times 1!\\)\n第5位小于2的所有数：\\(0\\)\n\n则其编号为 \\(2 \\times 4! + 2 \\times 3! + 0 \\times 2! + 1 \\times 1! + 0 \\times 0! = 61\\)\n2.2.2 自然数 \\(\\to\\) 排列\n编号X=61\n\n第1位：\\(\\frac{61}{4} = 2 \\cdots \\cdots 13\\)，说明比第1位小的有2个，ans[0]=3\n第2位：\\(\\frac{13}{3} = 2 \\cdots \\cdots 1\\)，说明比第2位小的有2个，a[1]=4\n第3位：\\(\\frac{1}{2} = 0 \\cdots \\cdots 1\\)，说明比第3位小的有0个，ans[2]=1\n第4位：\\(\\frac{1}{1} = 1 \\cdots \\cdots 0\\)，说明比第4位小的有1个，ans[3]=5\n第5位：剩下2，ans[4]=2"
  },
  {
    "objectID": "posts/[hot100]Day10/index.html",
    "href": "posts/[hot100]Day10/index.html",
    "title": "Day10",
    "section": "",
    "text": "1 爬楼梯\n\n\n\n\n\n\n\n分析\n\n\n\n假设 \\(f(n)\\) 表示为爬n阶楼梯的不同方法，为了爬到第n阶台阶的方法数有：\n\\[\nf(n) = f(n-1) + f(n-2)\n\\]\n上式是一个斐波那契数列，数列的通项为：\n\\[\na_n=\\frac 1 {\\sqrt{5}} [(\\frac{(1+\\sqrt 5)^n}{2})-\\frac{(1-\\sqrt 5)^n}{2}]\n\\]\n\n\n\nPythonC++\n\n\nclass Solution:\n    def climbStairs(self, n: int) -&gt; int:\n        prev = 2\n        pprev = 1\n        if n == 1:\n            return 1\n        if n == 2:\n            return 2\n        for _ in range(2, n):\n            now = prev + pprev\n            pprev = prev\n            prev = now\n        return now\n\n\nclass Solution {\npublic:\n    int climbStairs(int n) {\n        int prev = 2, pprev = 1;\n        if (n == 1) return 1;\n        if (n == 2) return 2;\n        int now;\n        for (int i = 2; i &lt; n; i++) {\n            now = prev + pprev;\n            pprev = prev;\n            prev = now;\n        }\n        return now;\n    }\n};\n\n\n\n\n\n2 格雷码\n\n\n\n\n\n\n\n分析\n\n\n\n数字n的格雷码为：\n\\[\n\\text{Graycode}(n)=n\\oplus\\lfloor n/2\\rfloor\n\\]\n\n\n\nPython\n\n\nclass Solution:\n    def grayCode(self, n: int) -&gt; List[int]:\n        result_list = []\n        for i in range(2**n):\n            item = i ^ int(i / 2)\n            result_list.append(item)\n        return result_list\nclass Solution {\npublic:\n    vector&lt;int&gt; grayCode(int n) {\n        vector&lt;int&gt; result;\n1        for (int i = 0; i &lt; pow(2, n); i++) result.push_back(i ^ (i &gt;&gt; 1));\n        return result;\n    }\n};\n\n1\n\nC++中计算幂函数的方式为pow() 函数\n\n\n\n\n\n\n\n3 矩阵置零\n\n\n\n\n\n\n\n分析\n\n\n\n就首先按行搜索，记录存在0元素的列，而后将列均赋值为0，easy🤟\n\n\n\nPythonC++\n\n\nclass Solution:\n    def setZeroes(self, matrix: List[List[int]]) -&gt; None:\n        \"\"\"\n        Do not return anything, modify matrix in-place instead.\n        \"\"\"\n        m = len(matrix); n = len(matrix[0])\n        zero_col = []\n        for i in range(m):\n            zero_indicator = False\n            for fast in range(n):\n                if matrix[i][fast] == 0:\n                    zero_indicator = True\n                    zero_col.append(fast)\n            if zero_indicator:\n                for slow in range(n):\n                    matrix[i][slow] = 0\n        for j in zero_col:\n            for i in range(m):\n                matrix[i][j] = 0\n\n\nclass Solution {\npublic:\n    void setZeroes(vector&lt;vector&lt;int&gt;&gt;& matrix) {\n        int m = matrix.size(), n = matrix[0].size();\n        vector&lt;int&gt; zero_col;\n        for (int i = 0; i &lt; m; i++) {\n            bool zero_indicator = false;\n            for (int j = 0; j &lt; n; j++) {\n                if (matrix[i][j] == 0) {\n                    zero_indicator = true;\n                    zero_col.push_back(j);\n                }\n            }\n            if (zero_indicator) {\n                for (int j = 0; j &lt; n; j++) matrix[i][j] = 0;\n            }\n        }\n\n        for (auto j = zero_col.begin(); j &lt; zero_col.end(); j++) {\n            for (int i = 0; i &lt; m; i++) matrix[i][*j] = 0;\n        } \n    }\n};\n\n\n\n\n\n4 加油站\n\n\n\n\n\n\n\n分析\n\n\n\n\n若从某站出发不足以前往下一站便比必不可能作为起始站\n起始站只能是从某站开始剩余油量始终为非负的\n可以只遍历一次，一个变量用于检测从某站开始油量的情况，另一个变量用于检测目前整体油量是否有亏欠\n\n\n\n\nPythonC++\n\n\nclass Solution:\n    def canCompleteCircuit(self, gas: List[int], cost: List[int]) -&gt; int:\n        total = 0; j = -1\n        sum = 0\n        for i in range(len(cost)):\n            sum += gas[i] - cost[i]\n            total += gas[i] - cost[i]\n            if sum &lt; 0:\n                j = i\n                sum = 0\n        return j + 1 if total &gt;= 0 else -1\n\n\nclass Solution {\npublic:\n    int canCompleteCircuit(vector&lt;int&gt;& gas, vector&lt;int&gt;& cost) {\n        int total = 0;\n        int j = -1;\n        for (int i = 0, sum = 0; i &lt; gas.size(); ++i) {\n            sum += gas[i] - cost[i];\n            total += gas[i] - cost[i];\n            if (sum &lt; 0) {\n                j = i;\n                sum = 0;\n            }\n        }\n        return total &gt;= 0 ? j + 1 : -1;\n    }\n};\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[hot100]Day9/index.html",
    "href": "posts/[hot100]Day9/index.html",
    "title": "Day9",
    "section": "",
    "text": "恶补\n\n\n\n前一阵子在忙组会，耽误了😭\n\n\n\n1 加一\n\n\nPythonC++\n\n\nclass Solution:\n    def plusOne(self, digits: List[int]) -&gt; List[int]:\n        plus = 1\n        for i in range(len(digits)-1, -1, -1):\n            if digits[i] + plus &gt;= 10:\n                digits[i] = 0\n                plus = 1\n            else:\n                digits[i] += plus\n                break\n            if i == 0 and plus == 1:\n                digits.insert(0, plus)\n        return digits\n\n\nclass Solution {\npublic:\n    vector&lt;int&gt; plusOne(vector&lt;int&gt;& digits) {\n        auto last = digits.end();\n        for (auto i = last-1; i &gt;= digits.begin(); i--) {\n            if ((*i + 1) == 10) {\n                *i = 0;\n            } else {\n                *i = *i + 1;\n                break;\n            }\n            if (i == digits.begin()) digits.insert(digits.begin(), 1);\n        }\n        return digits;\n    }\n};\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/[hot100]Day11/index.html",
    "href": "posts/[hot100]Day11/index.html",
    "title": "Day11",
    "section": "",
    "text": "1 分发糖果\n\n\n\n\n\n\n\n分析\n\n\n\n左右各扫一遍，只要比前一个（后一个）大就得在基础值上加一同时保证不小于当前递增矩阵的糖果树。\n\n\n\nPythonC++\n\n\nclass Solution:\n    def candy(self, ratings: List[int]) -&gt; int:\n        n = len(ratings)\n        base = 1\n        increment = [0] * n\n        for i in range(1, n):\n            if ratings[i] &gt; ratings[i-1]:\n                increment[i] = max(base, increment[i])\n                base += 1\n            else: base = 1\n        base = 1\n        for i in range(n-2, -1, -1):\n            if ratings[i] &gt; ratings[i+1]:\n                increment[i] = max(base, increment[i])\n                base += 1\n            else: base = 1\n        return sum(increment) + n\n\n\nclass Solution {\npublic:\n    int candy(vector&lt;int&gt;& ratings) {\n        const int n = ratings.size();\n        vector&lt;int&gt; increment(n);\n\n        for (int i = 1, inc = 1; i &lt; n; i++) {\n            if (ratings[i] &gt; ratings[i-1]) increment[i] = max(inc++, increment[i]);\n            else inc = 1;\n        }\n\n        for (int i = n - 2, inc = 1; i&gt;=0; i--) {\n            if (ratings[i] &gt; ratings[i + 1]) increment[i] = max(inc++, increment[i]);\n            else inc = 1;\n        }\n1        return accumulate(&increment[0], &increment[0] + n, n);\n    }\n};\n\n1\n\naccumulate(begin, end, init)：begin为数值开始的地址，end为结束，init为累加前的初始值\n\n\n\n\n\n\n\n\n\n Back to top"
  }
]